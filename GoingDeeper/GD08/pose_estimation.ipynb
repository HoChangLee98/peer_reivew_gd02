{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0836449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'mine')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cdd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb188c",
   "metadata": {},
   "source": [
    "0 - 오른쪽 발목\n",
    "1 - 오른쪽 무릎\n",
    "2 - 오른쪽 엉덩이\n",
    "3 - 왼쪽 엉덩이\n",
    "4 - 왼쪽 무릎\n",
    "5 - 왼쪽 발목\n",
    "6 - 골반\n",
    "7 - 가슴(흉부)\n",
    "8 - 목\n",
    "9 - 머리 위\n",
    "10 - 오른쪽 손목\n",
    "11 - 오른쪽 팔꿈치\n",
    "12 - 오른쪽 어깨\n",
    "13 - 왼쪽 어깨\n",
    "14 - 왼쪽 팔꿈치\n",
    "15 - 왼쪽 손목"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209fc58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json annotation parsing\n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58fa9f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27bc6b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de9e6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 얼마나 많은 TFRecord를 만들지 결정할 함수 : shard\n",
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa336f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09e93fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 병렬처리 위한 ray\n",
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb243bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53740215",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "180a73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data label로 만들기\n",
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db684017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image crop : 정사각형, 이 때 이미지 바깥으로 crop box가 나가면 안됨\n",
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f56cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f0426c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e8816",
   "metadata": {},
   "source": [
    "# Hourglass model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e91a1738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34e6c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9602f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11ac6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0145af7",
   "metadata": {},
   "source": [
    "# GPU가 여러개인 환경에서 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c02d75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = tf.constant(0.0, dtype=tf.float32) # float32 타입으로 초기화\n",
    "        labels = tf.cast(labels, dtype=tf.float32) # labels를 float32로 변환\n",
    "        for output in outputs:\n",
    "            output = tf.cast(output, dtype=tf.float32) # output을 float32로 변환\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += tf.cast(batch_loss, tf.float32)\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += tf.cast(batch_loss, tf.float32)\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdfd62e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "# 데이터셋 만드는 함수\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2060df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 연결 및 학습\n",
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        #model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "        model = SimpleBaseline(IMAGE_SHAPE)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad80430",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74567177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.5146277 epoch total loss 2.5146277\n",
      "Trained batch 2 batch loss 2.45807815 epoch total loss 2.48635292\n",
      "Trained batch 3 batch loss 2.55746269 epoch total loss 2.51005626\n",
      "Trained batch 4 batch loss 2.37066984 epoch total loss 2.47520971\n",
      "Trained batch 5 batch loss 2.38536167 epoch total loss 2.45724\n",
      "Trained batch 6 batch loss 2.33650112 epoch total loss 2.43711686\n",
      "Trained batch 7 batch loss 2.26773787 epoch total loss 2.41292\n",
      "Trained batch 8 batch loss 2.09957528 epoch total loss 2.37375188\n",
      "Trained batch 9 batch loss 2.00856137 epoch total loss 2.33317518\n",
      "Trained batch 10 batch loss 2.1967082 epoch total loss 2.31952858\n",
      "Trained batch 11 batch loss 2.12115669 epoch total loss 2.30149484\n",
      "Trained batch 12 batch loss 2.12126493 epoch total loss 2.28647566\n",
      "Trained batch 13 batch loss 2.16053081 epoch total loss 2.27678752\n",
      "Trained batch 14 batch loss 2.09075952 epoch total loss 2.26349974\n",
      "Trained batch 15 batch loss 2.13072705 epoch total loss 2.25464845\n",
      "Trained batch 16 batch loss 2.11028433 epoch total loss 2.2456255\n",
      "Trained batch 17 batch loss 2.07889318 epoch total loss 2.23581767\n",
      "Trained batch 18 batch loss 2.10215664 epoch total loss 2.22839212\n",
      "Trained batch 19 batch loss 2.09178495 epoch total loss 2.22120214\n",
      "Trained batch 20 batch loss 2.00825357 epoch total loss 2.21055484\n",
      "Trained batch 21 batch loss 1.96085548 epoch total loss 2.19866443\n",
      "Trained batch 22 batch loss 1.90278578 epoch total loss 2.18521523\n",
      "Trained batch 23 batch loss 1.96932483 epoch total loss 2.17582893\n",
      "Trained batch 24 batch loss 1.95655441 epoch total loss 2.1666925\n",
      "Trained batch 25 batch loss 1.98414731 epoch total loss 2.15939045\n",
      "Trained batch 26 batch loss 1.94998956 epoch total loss 2.15133667\n",
      "Trained batch 27 batch loss 1.91210413 epoch total loss 2.14247632\n",
      "Trained batch 28 batch loss 1.90200114 epoch total loss 2.13388777\n",
      "Trained batch 29 batch loss 1.80456 epoch total loss 2.12253165\n",
      "Trained batch 30 batch loss 1.79622018 epoch total loss 2.11165476\n",
      "Trained batch 31 batch loss 1.8031826 epoch total loss 2.10170412\n",
      "Trained batch 32 batch loss 1.74792647 epoch total loss 2.09064841\n",
      "Trained batch 33 batch loss 1.73693168 epoch total loss 2.07992959\n",
      "Trained batch 34 batch loss 1.77028906 epoch total loss 2.07082248\n",
      "Trained batch 35 batch loss 1.79102504 epoch total loss 2.0628283\n",
      "Trained batch 36 batch loss 1.76419115 epoch total loss 2.05453277\n",
      "Trained batch 37 batch loss 1.77637887 epoch total loss 2.04701519\n",
      "Trained batch 38 batch loss 1.74888158 epoch total loss 2.03916955\n",
      "Trained batch 39 batch loss 1.7040906 epoch total loss 2.0305779\n",
      "Trained batch 40 batch loss 1.71522355 epoch total loss 2.02269411\n",
      "Trained batch 41 batch loss 1.73771799 epoch total loss 2.01574326\n",
      "Trained batch 42 batch loss 1.88571143 epoch total loss 2.01264739\n",
      "Trained batch 43 batch loss 1.70749784 epoch total loss 2.00555086\n",
      "Trained batch 44 batch loss 1.74900126 epoch total loss 1.9997201\n",
      "Trained batch 45 batch loss 1.72001052 epoch total loss 1.99350429\n",
      "Trained batch 46 batch loss 1.50974393 epoch total loss 1.98298776\n",
      "Trained batch 47 batch loss 1.53546429 epoch total loss 1.97346592\n",
      "Trained batch 48 batch loss 1.5242095 epoch total loss 1.96410644\n",
      "Trained batch 49 batch loss 1.65388918 epoch total loss 1.95777547\n",
      "Trained batch 50 batch loss 1.66449976 epoch total loss 1.9519099\n",
      "Trained batch 51 batch loss 1.81147361 epoch total loss 1.94915617\n",
      "Trained batch 52 batch loss 1.76246703 epoch total loss 1.94556606\n",
      "Trained batch 53 batch loss 1.84289467 epoch total loss 1.94362891\n",
      "Trained batch 54 batch loss 1.80490065 epoch total loss 1.94105983\n",
      "Trained batch 55 batch loss 1.75244355 epoch total loss 1.93763041\n",
      "Trained batch 56 batch loss 1.81773388 epoch total loss 1.93548942\n",
      "Trained batch 57 batch loss 1.79533505 epoch total loss 1.93303049\n",
      "Trained batch 58 batch loss 1.84866142 epoch total loss 1.93157589\n",
      "Trained batch 59 batch loss 1.80308151 epoch total loss 1.92939806\n",
      "Trained batch 60 batch loss 1.8447361 epoch total loss 1.92798698\n",
      "Trained batch 61 batch loss 1.74491787 epoch total loss 1.92498589\n",
      "Trained batch 62 batch loss 1.74804771 epoch total loss 1.92213202\n",
      "Trained batch 63 batch loss 1.67641783 epoch total loss 1.91823173\n",
      "Trained batch 64 batch loss 1.59774709 epoch total loss 1.91322422\n",
      "Trained batch 65 batch loss 1.72505343 epoch total loss 1.91032922\n",
      "Trained batch 66 batch loss 1.65957713 epoch total loss 1.90653\n",
      "Trained batch 67 batch loss 1.62729692 epoch total loss 1.90236235\n",
      "Trained batch 68 batch loss 1.72071612 epoch total loss 1.89969099\n",
      "Trained batch 69 batch loss 1.72939372 epoch total loss 1.897223\n",
      "Trained batch 70 batch loss 1.77889407 epoch total loss 1.89553261\n",
      "Trained batch 71 batch loss 1.75997019 epoch total loss 1.89362323\n",
      "Trained batch 72 batch loss 1.78410482 epoch total loss 1.89210212\n",
      "Trained batch 73 batch loss 1.60241628 epoch total loss 1.88813388\n",
      "Trained batch 74 batch loss 1.65719783 epoch total loss 1.8850131\n",
      "Trained batch 75 batch loss 1.68204641 epoch total loss 1.88230693\n",
      "Trained batch 76 batch loss 1.70775211 epoch total loss 1.88001013\n",
      "Trained batch 77 batch loss 1.73057663 epoch total loss 1.8780694\n",
      "Trained batch 78 batch loss 1.54473805 epoch total loss 1.87379587\n",
      "Trained batch 79 batch loss 1.62117136 epoch total loss 1.87059808\n",
      "Trained batch 80 batch loss 1.60656965 epoch total loss 1.86729777\n",
      "Trained batch 81 batch loss 1.71367157 epoch total loss 1.86540115\n",
      "Trained batch 82 batch loss 1.7500596 epoch total loss 1.86399448\n",
      "Trained batch 83 batch loss 1.81603 epoch total loss 1.86341655\n",
      "Trained batch 84 batch loss 1.75061917 epoch total loss 1.86207378\n",
      "Trained batch 85 batch loss 1.66858447 epoch total loss 1.85979736\n",
      "Trained batch 86 batch loss 1.56684053 epoch total loss 1.85639083\n",
      "Trained batch 87 batch loss 1.64388204 epoch total loss 1.85394812\n",
      "Trained batch 88 batch loss 1.50127029 epoch total loss 1.84994042\n",
      "Trained batch 89 batch loss 1.6262331 epoch total loss 1.84742689\n",
      "Trained batch 90 batch loss 1.7045753 epoch total loss 1.84583962\n",
      "Trained batch 91 batch loss 1.72620714 epoch total loss 1.84452498\n",
      "Trained batch 92 batch loss 1.75064862 epoch total loss 1.84350467\n",
      "Trained batch 93 batch loss 1.78454852 epoch total loss 1.84287071\n",
      "Trained batch 94 batch loss 1.65863144 epoch total loss 1.84091067\n",
      "Trained batch 95 batch loss 1.59345198 epoch total loss 1.83830583\n",
      "Trained batch 96 batch loss 1.62565625 epoch total loss 1.83609068\n",
      "Trained batch 97 batch loss 1.63136685 epoch total loss 1.83398008\n",
      "Trained batch 98 batch loss 1.71578419 epoch total loss 1.83277416\n",
      "Trained batch 99 batch loss 1.76526046 epoch total loss 1.83209217\n",
      "Trained batch 100 batch loss 1.74586248 epoch total loss 1.83122981\n",
      "Trained batch 101 batch loss 1.72079265 epoch total loss 1.83013642\n",
      "Trained batch 102 batch loss 1.73027933 epoch total loss 1.82915747\n",
      "Trained batch 103 batch loss 1.7416991 epoch total loss 1.82830834\n",
      "Trained batch 104 batch loss 1.71902204 epoch total loss 1.82725763\n",
      "Trained batch 105 batch loss 1.73264503 epoch total loss 1.82635653\n",
      "Trained batch 106 batch loss 1.72319674 epoch total loss 1.82538331\n",
      "Trained batch 107 batch loss 1.66671598 epoch total loss 1.82390046\n",
      "Trained batch 108 batch loss 1.61872637 epoch total loss 1.82200074\n",
      "Trained batch 109 batch loss 1.71092653 epoch total loss 1.82098162\n",
      "Trained batch 110 batch loss 1.72878242 epoch total loss 1.82014346\n",
      "Trained batch 111 batch loss 1.73725104 epoch total loss 1.81939662\n",
      "Trained batch 112 batch loss 1.71046185 epoch total loss 1.81842399\n",
      "Trained batch 113 batch loss 1.73146081 epoch total loss 1.81765437\n",
      "Trained batch 114 batch loss 1.71235847 epoch total loss 1.81673074\n",
      "Trained batch 115 batch loss 1.7363317 epoch total loss 1.81603158\n",
      "Trained batch 116 batch loss 1.7543869 epoch total loss 1.81550014\n",
      "Trained batch 117 batch loss 1.77037358 epoch total loss 1.8151145\n",
      "Trained batch 118 batch loss 1.71280754 epoch total loss 1.81424749\n",
      "Trained batch 119 batch loss 1.71875107 epoch total loss 1.81344497\n",
      "Trained batch 120 batch loss 1.71860301 epoch total loss 1.81265461\n",
      "Trained batch 121 batch loss 1.6922965 epoch total loss 1.81165993\n",
      "Trained batch 122 batch loss 1.70984137 epoch total loss 1.81082523\n",
      "Trained batch 123 batch loss 1.73904538 epoch total loss 1.8102417\n",
      "Trained batch 124 batch loss 1.68555427 epoch total loss 1.80923605\n",
      "Trained batch 125 batch loss 1.66204107 epoch total loss 1.80805862\n",
      "Trained batch 126 batch loss 1.75237429 epoch total loss 1.80761671\n",
      "Trained batch 127 batch loss 1.76416481 epoch total loss 1.80727458\n",
      "Trained batch 128 batch loss 1.65900135 epoch total loss 1.8061161\n",
      "Trained batch 129 batch loss 1.58340585 epoch total loss 1.80438972\n",
      "Trained batch 130 batch loss 1.73635805 epoch total loss 1.80386639\n",
      "Trained batch 131 batch loss 1.71745408 epoch total loss 1.80320668\n",
      "Trained batch 132 batch loss 1.74616647 epoch total loss 1.80277455\n",
      "Trained batch 133 batch loss 1.68644238 epoch total loss 1.80189991\n",
      "Trained batch 134 batch loss 1.74646103 epoch total loss 1.80148625\n",
      "Trained batch 135 batch loss 1.67730188 epoch total loss 1.80056643\n",
      "Trained batch 136 batch loss 1.64914501 epoch total loss 1.7994529\n",
      "Trained batch 137 batch loss 1.6969173 epoch total loss 1.7987045\n",
      "Trained batch 138 batch loss 1.73703349 epoch total loss 1.79825759\n",
      "Trained batch 139 batch loss 1.72927332 epoch total loss 1.79776132\n",
      "Trained batch 140 batch loss 1.67542613 epoch total loss 1.79688752\n",
      "Trained batch 141 batch loss 1.71648407 epoch total loss 1.79631722\n",
      "Trained batch 142 batch loss 1.68151414 epoch total loss 1.79550874\n",
      "Trained batch 143 batch loss 1.68948889 epoch total loss 1.79476738\n",
      "Trained batch 144 batch loss 1.72546577 epoch total loss 1.79428613\n",
      "Trained batch 145 batch loss 1.68381321 epoch total loss 1.79352415\n",
      "Trained batch 146 batch loss 1.65981138 epoch total loss 1.79260838\n",
      "Trained batch 147 batch loss 1.61157262 epoch total loss 1.79137683\n",
      "Trained batch 148 batch loss 1.65951419 epoch total loss 1.79048586\n",
      "Trained batch 149 batch loss 1.59613466 epoch total loss 1.78918147\n",
      "Trained batch 150 batch loss 1.59049273 epoch total loss 1.78785682\n",
      "Trained batch 151 batch loss 1.48163188 epoch total loss 1.78582883\n",
      "Trained batch 152 batch loss 1.50754166 epoch total loss 1.78399801\n",
      "Trained batch 153 batch loss 1.63518751 epoch total loss 1.78302538\n",
      "Trained batch 154 batch loss 1.46975672 epoch total loss 1.7809912\n",
      "Trained batch 155 batch loss 1.5542922 epoch total loss 1.77952862\n",
      "Trained batch 156 batch loss 1.46081829 epoch total loss 1.77748561\n",
      "Trained batch 157 batch loss 1.56644857 epoch total loss 1.77614129\n",
      "Trained batch 158 batch loss 1.67870092 epoch total loss 1.77552462\n",
      "Trained batch 159 batch loss 1.75466275 epoch total loss 1.77539349\n",
      "Trained batch 160 batch loss 1.78450131 epoch total loss 1.77545047\n",
      "Trained batch 161 batch loss 1.71745992 epoch total loss 1.77509034\n",
      "Trained batch 162 batch loss 1.73402548 epoch total loss 1.77483702\n",
      "Trained batch 163 batch loss 1.731125 epoch total loss 1.77456892\n",
      "Trained batch 164 batch loss 1.65896213 epoch total loss 1.77386403\n",
      "Trained batch 165 batch loss 1.58495116 epoch total loss 1.77271914\n",
      "Trained batch 166 batch loss 1.66370666 epoch total loss 1.77206242\n",
      "Trained batch 167 batch loss 1.65203989 epoch total loss 1.77134371\n",
      "Trained batch 168 batch loss 1.68712962 epoch total loss 1.77084243\n",
      "Trained batch 169 batch loss 1.70098412 epoch total loss 1.77042913\n",
      "Trained batch 170 batch loss 1.66772556 epoch total loss 1.76982498\n",
      "Trained batch 171 batch loss 1.67202187 epoch total loss 1.76925302\n",
      "Trained batch 172 batch loss 1.59558582 epoch total loss 1.76824331\n",
      "Trained batch 173 batch loss 1.68770051 epoch total loss 1.7677778\n",
      "Trained batch 174 batch loss 1.70279455 epoch total loss 1.76740432\n",
      "Trained batch 175 batch loss 1.67614508 epoch total loss 1.7668829\n",
      "Trained batch 176 batch loss 1.62310684 epoch total loss 1.76606596\n",
      "Trained batch 177 batch loss 1.62290537 epoch total loss 1.76525712\n",
      "Trained batch 178 batch loss 1.62246466 epoch total loss 1.76445484\n",
      "Trained batch 179 batch loss 1.46706414 epoch total loss 1.76279354\n",
      "Trained batch 180 batch loss 1.57793343 epoch total loss 1.76176655\n",
      "Trained batch 181 batch loss 1.60385251 epoch total loss 1.76089406\n",
      "Trained batch 182 batch loss 1.55030894 epoch total loss 1.75973713\n",
      "Trained batch 183 batch loss 1.63516271 epoch total loss 1.75905633\n",
      "Trained batch 184 batch loss 1.58804929 epoch total loss 1.75812697\n",
      "Trained batch 185 batch loss 1.59923875 epoch total loss 1.75726819\n",
      "Trained batch 186 batch loss 1.61099195 epoch total loss 1.75648165\n",
      "Trained batch 187 batch loss 1.67365646 epoch total loss 1.75603878\n",
      "Trained batch 188 batch loss 1.6705 epoch total loss 1.75558376\n",
      "Trained batch 189 batch loss 1.57710695 epoch total loss 1.75463951\n",
      "Trained batch 190 batch loss 1.51995206 epoch total loss 1.75340426\n",
      "Trained batch 191 batch loss 1.65223086 epoch total loss 1.75287461\n",
      "Trained batch 192 batch loss 1.67926013 epoch total loss 1.75249112\n",
      "Trained batch 193 batch loss 1.64590085 epoch total loss 1.75193894\n",
      "Trained batch 194 batch loss 1.67419362 epoch total loss 1.75153816\n",
      "Trained batch 195 batch loss 1.71509075 epoch total loss 1.75135124\n",
      "Trained batch 196 batch loss 1.61209369 epoch total loss 1.75064075\n",
      "Trained batch 197 batch loss 1.66203606 epoch total loss 1.75019097\n",
      "Trained batch 198 batch loss 1.58657396 epoch total loss 1.74936473\n",
      "Trained batch 199 batch loss 1.63600636 epoch total loss 1.74879515\n",
      "Trained batch 200 batch loss 1.69100881 epoch total loss 1.74850619\n",
      "Trained batch 201 batch loss 1.72206879 epoch total loss 1.7483747\n",
      "Trained batch 202 batch loss 1.77347326 epoch total loss 1.74849892\n",
      "Trained batch 203 batch loss 1.47695947 epoch total loss 1.74716127\n",
      "Trained batch 204 batch loss 1.5114758 epoch total loss 1.74600589\n",
      "Trained batch 205 batch loss 1.62553763 epoch total loss 1.74541831\n",
      "Trained batch 206 batch loss 1.62501538 epoch total loss 1.74483395\n",
      "Trained batch 207 batch loss 1.65075326 epoch total loss 1.7443794\n",
      "Trained batch 208 batch loss 1.59294033 epoch total loss 1.74365127\n",
      "Trained batch 209 batch loss 1.64077353 epoch total loss 1.74315906\n",
      "Trained batch 210 batch loss 1.65931964 epoch total loss 1.74276\n",
      "Trained batch 211 batch loss 1.66281772 epoch total loss 1.74238098\n",
      "Trained batch 212 batch loss 1.65627348 epoch total loss 1.74197483\n",
      "Trained batch 213 batch loss 1.66577899 epoch total loss 1.74161708\n",
      "Trained batch 214 batch loss 1.68506658 epoch total loss 1.7413528\n",
      "Trained batch 215 batch loss 1.65503561 epoch total loss 1.7409513\n",
      "Trained batch 216 batch loss 1.64753091 epoch total loss 1.74051881\n",
      "Trained batch 217 batch loss 1.64640379 epoch total loss 1.74008501\n",
      "Trained batch 218 batch loss 1.60238791 epoch total loss 1.73945343\n",
      "Trained batch 219 batch loss 1.58890784 epoch total loss 1.73876595\n",
      "Trained batch 220 batch loss 1.61154091 epoch total loss 1.73818767\n",
      "Trained batch 221 batch loss 1.59379339 epoch total loss 1.73753417\n",
      "Trained batch 222 batch loss 1.64951146 epoch total loss 1.73713768\n",
      "Trained batch 223 batch loss 1.62668037 epoch total loss 1.73664236\n",
      "Trained batch 224 batch loss 1.64889801 epoch total loss 1.73625064\n",
      "Trained batch 225 batch loss 1.64847314 epoch total loss 1.73586047\n",
      "Trained batch 226 batch loss 1.59238446 epoch total loss 1.73522556\n",
      "Trained batch 227 batch loss 1.64386582 epoch total loss 1.73482311\n",
      "Trained batch 228 batch loss 1.60605693 epoch total loss 1.73425829\n",
      "Trained batch 229 batch loss 1.58109856 epoch total loss 1.73358941\n",
      "Trained batch 230 batch loss 1.62230718 epoch total loss 1.73310566\n",
      "Trained batch 231 batch loss 1.68560696 epoch total loss 1.7329\n",
      "Trained batch 232 batch loss 1.68705249 epoch total loss 1.73270237\n",
      "Trained batch 233 batch loss 1.69916391 epoch total loss 1.73255837\n",
      "Trained batch 234 batch loss 1.80915761 epoch total loss 1.7328856\n",
      "Trained batch 235 batch loss 1.82775629 epoch total loss 1.73328936\n",
      "Trained batch 236 batch loss 1.7644732 epoch total loss 1.73342144\n",
      "Trained batch 237 batch loss 1.72971547 epoch total loss 1.73340583\n",
      "Trained batch 238 batch loss 1.6401062 epoch total loss 1.73301375\n",
      "Trained batch 239 batch loss 1.60425556 epoch total loss 1.73247504\n",
      "Trained batch 240 batch loss 1.64667118 epoch total loss 1.73211741\n",
      "Trained batch 241 batch loss 1.67685783 epoch total loss 1.73188818\n",
      "Trained batch 242 batch loss 1.64225304 epoch total loss 1.73151767\n",
      "Trained batch 243 batch loss 1.62957513 epoch total loss 1.73109818\n",
      "Trained batch 244 batch loss 1.66382861 epoch total loss 1.73082244\n",
      "Trained batch 245 batch loss 1.60251951 epoch total loss 1.73029876\n",
      "Trained batch 246 batch loss 1.61370397 epoch total loss 1.72982478\n",
      "Trained batch 247 batch loss 1.69342208 epoch total loss 1.72967744\n",
      "Trained batch 248 batch loss 1.62308216 epoch total loss 1.72924757\n",
      "Trained batch 249 batch loss 1.68847489 epoch total loss 1.72908378\n",
      "Trained batch 250 batch loss 1.54567862 epoch total loss 1.72835016\n",
      "Trained batch 251 batch loss 1.58206582 epoch total loss 1.72776735\n",
      "Trained batch 252 batch loss 1.60406637 epoch total loss 1.72727656\n",
      "Trained batch 253 batch loss 1.6831243 epoch total loss 1.72710204\n",
      "Trained batch 254 batch loss 1.6326654 epoch total loss 1.72673023\n",
      "Trained batch 255 batch loss 1.68780863 epoch total loss 1.72657752\n",
      "Trained batch 256 batch loss 1.68219471 epoch total loss 1.72640419\n",
      "Trained batch 257 batch loss 1.62442195 epoch total loss 1.72600734\n",
      "Trained batch 258 batch loss 1.64280987 epoch total loss 1.725685\n",
      "Trained batch 259 batch loss 1.66461468 epoch total loss 1.72544909\n",
      "Trained batch 260 batch loss 1.61354804 epoch total loss 1.72501874\n",
      "Trained batch 261 batch loss 1.66712046 epoch total loss 1.72479689\n",
      "Trained batch 262 batch loss 1.63626158 epoch total loss 1.72445905\n",
      "Trained batch 263 batch loss 1.59426558 epoch total loss 1.72396398\n",
      "Trained batch 264 batch loss 1.68824792 epoch total loss 1.72382879\n",
      "Trained batch 265 batch loss 1.67617989 epoch total loss 1.72364891\n",
      "Trained batch 266 batch loss 1.63863933 epoch total loss 1.72332931\n",
      "Trained batch 267 batch loss 1.63241601 epoch total loss 1.72298884\n",
      "Trained batch 268 batch loss 1.57704043 epoch total loss 1.72244418\n",
      "Trained batch 269 batch loss 1.63154888 epoch total loss 1.72210634\n",
      "Trained batch 270 batch loss 1.63561952 epoch total loss 1.72178602\n",
      "Trained batch 271 batch loss 1.67068624 epoch total loss 1.72159743\n",
      "Trained batch 272 batch loss 1.69920278 epoch total loss 1.72151506\n",
      "Trained batch 273 batch loss 1.72948921 epoch total loss 1.72154427\n",
      "Trained batch 274 batch loss 1.70931828 epoch total loss 1.72149968\n",
      "Trained batch 275 batch loss 1.673751 epoch total loss 1.72132599\n",
      "Trained batch 276 batch loss 1.71508706 epoch total loss 1.72130346\n",
      "Trained batch 277 batch loss 1.65982008 epoch total loss 1.7210815\n",
      "Trained batch 278 batch loss 1.64728618 epoch total loss 1.72081602\n",
      "Trained batch 279 batch loss 1.58796096 epoch total loss 1.72033978\n",
      "Trained batch 280 batch loss 1.70767915 epoch total loss 1.72029448\n",
      "Trained batch 281 batch loss 1.58949351 epoch total loss 1.71982908\n",
      "Trained batch 282 batch loss 1.60226929 epoch total loss 1.71941221\n",
      "Trained batch 283 batch loss 1.67000687 epoch total loss 1.71923769\n",
      "Trained batch 284 batch loss 1.48759782 epoch total loss 1.71842206\n",
      "Trained batch 285 batch loss 1.59196472 epoch total loss 1.71797836\n",
      "Trained batch 286 batch loss 1.63028 epoch total loss 1.71767175\n",
      "Trained batch 287 batch loss 1.52199674 epoch total loss 1.71699\n",
      "Trained batch 288 batch loss 1.5194993 epoch total loss 1.7163043\n",
      "Trained batch 289 batch loss 1.51210439 epoch total loss 1.71559775\n",
      "Trained batch 290 batch loss 1.67019236 epoch total loss 1.71544111\n",
      "Trained batch 291 batch loss 1.64899719 epoch total loss 1.71521282\n",
      "Trained batch 292 batch loss 1.60787272 epoch total loss 1.71484518\n",
      "Trained batch 293 batch loss 1.62553239 epoch total loss 1.71454036\n",
      "Trained batch 294 batch loss 1.62936306 epoch total loss 1.71425068\n",
      "Trained batch 295 batch loss 1.59911942 epoch total loss 1.71386039\n",
      "Trained batch 296 batch loss 1.532426 epoch total loss 1.71324742\n",
      "Trained batch 297 batch loss 1.53743482 epoch total loss 1.71265554\n",
      "Trained batch 298 batch loss 1.57889152 epoch total loss 1.7122066\n",
      "Trained batch 299 batch loss 1.59547567 epoch total loss 1.71181631\n",
      "Trained batch 300 batch loss 1.59011245 epoch total loss 1.71141052\n",
      "Trained batch 301 batch loss 1.66683125 epoch total loss 1.71126235\n",
      "Trained batch 302 batch loss 1.60358691 epoch total loss 1.71090579\n",
      "Trained batch 303 batch loss 1.59396493 epoch total loss 1.71051991\n",
      "Trained batch 304 batch loss 1.52819383 epoch total loss 1.70992017\n",
      "Trained batch 305 batch loss 1.63508916 epoch total loss 1.70967472\n",
      "Trained batch 306 batch loss 1.49858809 epoch total loss 1.70898497\n",
      "Trained batch 307 batch loss 1.40994644 epoch total loss 1.70801103\n",
      "Trained batch 308 batch loss 1.48207605 epoch total loss 1.70727742\n",
      "Trained batch 309 batch loss 1.48729026 epoch total loss 1.7065655\n",
      "Trained batch 310 batch loss 1.49957252 epoch total loss 1.70589781\n",
      "Trained batch 311 batch loss 1.50701725 epoch total loss 1.70525825\n",
      "Trained batch 312 batch loss 1.56142104 epoch total loss 1.70479715\n",
      "Trained batch 313 batch loss 1.59272957 epoch total loss 1.70443916\n",
      "Trained batch 314 batch loss 1.53422832 epoch total loss 1.70389712\n",
      "Trained batch 315 batch loss 1.62152016 epoch total loss 1.70363557\n",
      "Trained batch 316 batch loss 1.57956195 epoch total loss 1.70324302\n",
      "Trained batch 317 batch loss 1.65724051 epoch total loss 1.70309782\n",
      "Trained batch 318 batch loss 1.54248405 epoch total loss 1.70259273\n",
      "Trained batch 319 batch loss 1.32213783 epoch total loss 1.70140016\n",
      "Trained batch 320 batch loss 1.12618327 epoch total loss 1.69960248\n",
      "Trained batch 321 batch loss 1.40821946 epoch total loss 1.69869471\n",
      "Trained batch 322 batch loss 1.65797949 epoch total loss 1.69856822\n",
      "Trained batch 323 batch loss 1.75275433 epoch total loss 1.69873595\n",
      "Trained batch 324 batch loss 1.70391178 epoch total loss 1.69875193\n",
      "Trained batch 325 batch loss 1.68638742 epoch total loss 1.6987139\n",
      "Trained batch 326 batch loss 1.65537333 epoch total loss 1.6985811\n",
      "Trained batch 327 batch loss 1.66229045 epoch total loss 1.69847012\n",
      "Trained batch 328 batch loss 1.55767107 epoch total loss 1.69804084\n",
      "Trained batch 329 batch loss 1.59451962 epoch total loss 1.69772625\n",
      "Trained batch 330 batch loss 1.60749304 epoch total loss 1.69745278\n",
      "Trained batch 331 batch loss 1.56845474 epoch total loss 1.69706321\n",
      "Trained batch 332 batch loss 1.60428989 epoch total loss 1.69678378\n",
      "Trained batch 333 batch loss 1.53715444 epoch total loss 1.69630444\n",
      "Trained batch 334 batch loss 1.60861135 epoch total loss 1.69604182\n",
      "Trained batch 335 batch loss 1.61567366 epoch total loss 1.69580185\n",
      "Trained batch 336 batch loss 1.53248751 epoch total loss 1.69531572\n",
      "Trained batch 337 batch loss 1.63932431 epoch total loss 1.69514966\n",
      "Trained batch 338 batch loss 1.6326232 epoch total loss 1.69496477\n",
      "Trained batch 339 batch loss 1.68756938 epoch total loss 1.69494283\n",
      "Trained batch 340 batch loss 1.67981899 epoch total loss 1.69489837\n",
      "Trained batch 341 batch loss 1.66322267 epoch total loss 1.69480538\n",
      "Trained batch 342 batch loss 1.48109686 epoch total loss 1.69418049\n",
      "Trained batch 343 batch loss 1.46685755 epoch total loss 1.6935178\n",
      "Trained batch 344 batch loss 1.56394625 epoch total loss 1.6931411\n",
      "Trained batch 345 batch loss 1.63502932 epoch total loss 1.69297266\n",
      "Trained batch 346 batch loss 1.58625925 epoch total loss 1.69266415\n",
      "Trained batch 347 batch loss 1.66204607 epoch total loss 1.69257593\n",
      "Trained batch 348 batch loss 1.61083031 epoch total loss 1.69234109\n",
      "Trained batch 349 batch loss 1.52296114 epoch total loss 1.69185567\n",
      "Trained batch 350 batch loss 1.6772759 epoch total loss 1.69181395\n",
      "Trained batch 351 batch loss 1.81026113 epoch total loss 1.69215131\n",
      "Trained batch 352 batch loss 1.715837 epoch total loss 1.69221866\n",
      "Trained batch 353 batch loss 1.7465353 epoch total loss 1.69237244\n",
      "Trained batch 354 batch loss 1.66358542 epoch total loss 1.69229114\n",
      "Trained batch 355 batch loss 1.632864 epoch total loss 1.69212377\n",
      "Trained batch 356 batch loss 1.47926211 epoch total loss 1.6915257\n",
      "Trained batch 357 batch loss 1.58770418 epoch total loss 1.69123495\n",
      "Trained batch 358 batch loss 1.59020531 epoch total loss 1.69095278\n",
      "Trained batch 359 batch loss 1.69457209 epoch total loss 1.69096291\n",
      "Trained batch 360 batch loss 1.49879551 epoch total loss 1.69042897\n",
      "Trained batch 361 batch loss 1.57974946 epoch total loss 1.69012249\n",
      "Trained batch 362 batch loss 1.50643897 epoch total loss 1.68961501\n",
      "Trained batch 363 batch loss 1.63098109 epoch total loss 1.68945348\n",
      "Trained batch 364 batch loss 1.47368503 epoch total loss 1.68886077\n",
      "Trained batch 365 batch loss 1.50207853 epoch total loss 1.68834901\n",
      "Trained batch 366 batch loss 1.56983757 epoch total loss 1.68802512\n",
      "Trained batch 367 batch loss 1.57174218 epoch total loss 1.68770826\n",
      "Trained batch 368 batch loss 1.6151967 epoch total loss 1.68751109\n",
      "Trained batch 369 batch loss 1.67690969 epoch total loss 1.68748236\n",
      "Trained batch 370 batch loss 1.65238976 epoch total loss 1.68738747\n",
      "Trained batch 371 batch loss 1.61063683 epoch total loss 1.68718064\n",
      "Trained batch 372 batch loss 1.67517185 epoch total loss 1.68714833\n",
      "Trained batch 373 batch loss 1.69126761 epoch total loss 1.68715954\n",
      "Trained batch 374 batch loss 1.64015341 epoch total loss 1.68703377\n",
      "Trained batch 375 batch loss 1.57420218 epoch total loss 1.68673289\n",
      "Trained batch 376 batch loss 1.58898103 epoch total loss 1.68647301\n",
      "Trained batch 377 batch loss 1.59740663 epoch total loss 1.68623674\n",
      "Trained batch 378 batch loss 1.57634783 epoch total loss 1.68594599\n",
      "Trained batch 379 batch loss 1.61156726 epoch total loss 1.68574977\n",
      "Trained batch 380 batch loss 1.60261297 epoch total loss 1.68553102\n",
      "Trained batch 381 batch loss 1.52539086 epoch total loss 1.68511069\n",
      "Trained batch 382 batch loss 1.57298684 epoch total loss 1.68481719\n",
      "Trained batch 383 batch loss 1.54071188 epoch total loss 1.68444097\n",
      "Trained batch 384 batch loss 1.57164216 epoch total loss 1.68414724\n",
      "Trained batch 385 batch loss 1.51347721 epoch total loss 1.6837039\n",
      "Trained batch 386 batch loss 1.5513804 epoch total loss 1.68336117\n",
      "Trained batch 387 batch loss 1.56566954 epoch total loss 1.68305707\n",
      "Trained batch 388 batch loss 1.43670237 epoch total loss 1.68242216\n",
      "Trained batch 389 batch loss 1.44358015 epoch total loss 1.68180823\n",
      "Trained batch 390 batch loss 1.46495867 epoch total loss 1.68125224\n",
      "Trained batch 391 batch loss 1.47335696 epoch total loss 1.68072045\n",
      "Trained batch 392 batch loss 1.40853548 epoch total loss 1.68002605\n",
      "Trained batch 393 batch loss 1.58896077 epoch total loss 1.67979431\n",
      "Trained batch 394 batch loss 1.59424305 epoch total loss 1.67957723\n",
      "Trained batch 395 batch loss 1.58419979 epoch total loss 1.67933583\n",
      "Trained batch 396 batch loss 1.6002655 epoch total loss 1.67913616\n",
      "Trained batch 397 batch loss 1.66674125 epoch total loss 1.67910492\n",
      "Trained batch 398 batch loss 1.55726826 epoch total loss 1.67879879\n",
      "Trained batch 399 batch loss 1.65771723 epoch total loss 1.67874599\n",
      "Trained batch 400 batch loss 1.62701166 epoch total loss 1.67861664\n",
      "Trained batch 401 batch loss 1.63336647 epoch total loss 1.67850375\n",
      "Trained batch 402 batch loss 1.61411953 epoch total loss 1.67834365\n",
      "Trained batch 403 batch loss 1.5711782 epoch total loss 1.6780777\n",
      "Trained batch 404 batch loss 1.70952272 epoch total loss 1.67815554\n",
      "Trained batch 405 batch loss 1.61738467 epoch total loss 1.67800546\n",
      "Trained batch 406 batch loss 1.59700108 epoch total loss 1.6778059\n",
      "Trained batch 407 batch loss 1.53817427 epoch total loss 1.67746282\n",
      "Trained batch 408 batch loss 1.59148955 epoch total loss 1.67725205\n",
      "Trained batch 409 batch loss 1.57438982 epoch total loss 1.67700064\n",
      "Trained batch 410 batch loss 1.56298757 epoch total loss 1.67672253\n",
      "Trained batch 411 batch loss 1.58662736 epoch total loss 1.6765033\n",
      "Trained batch 412 batch loss 1.70868862 epoch total loss 1.67658138\n",
      "Trained batch 413 batch loss 1.53210282 epoch total loss 1.6762315\n",
      "Trained batch 414 batch loss 1.5702517 epoch total loss 1.67597556\n",
      "Trained batch 415 batch loss 1.4576602 epoch total loss 1.67544949\n",
      "Trained batch 416 batch loss 1.48065352 epoch total loss 1.67498124\n",
      "Trained batch 417 batch loss 1.58804202 epoch total loss 1.67477262\n",
      "Trained batch 418 batch loss 1.61787891 epoch total loss 1.67463648\n",
      "Trained batch 419 batch loss 1.57616925 epoch total loss 1.67440152\n",
      "Trained batch 420 batch loss 1.55118179 epoch total loss 1.67410815\n",
      "Trained batch 421 batch loss 1.57953656 epoch total loss 1.67388356\n",
      "Trained batch 422 batch loss 1.53503799 epoch total loss 1.67355442\n",
      "Trained batch 423 batch loss 1.55307055 epoch total loss 1.67326975\n",
      "Trained batch 424 batch loss 1.51365662 epoch total loss 1.67289329\n",
      "Trained batch 425 batch loss 1.58025885 epoch total loss 1.67267537\n",
      "Trained batch 426 batch loss 1.43570793 epoch total loss 1.67211914\n",
      "Trained batch 427 batch loss 1.58323538 epoch total loss 1.671911\n",
      "Trained batch 428 batch loss 1.52557695 epoch total loss 1.67156911\n",
      "Trained batch 429 batch loss 1.59449792 epoch total loss 1.67138946\n",
      "Trained batch 430 batch loss 1.58314812 epoch total loss 1.67118418\n",
      "Trained batch 431 batch loss 1.61544394 epoch total loss 1.67105472\n",
      "Trained batch 432 batch loss 1.55546212 epoch total loss 1.67078722\n",
      "Trained batch 433 batch loss 1.50564945 epoch total loss 1.67040598\n",
      "Trained batch 434 batch loss 1.6920017 epoch total loss 1.67045569\n",
      "Trained batch 435 batch loss 1.62559903 epoch total loss 1.67035258\n",
      "Trained batch 436 batch loss 1.66024852 epoch total loss 1.67032957\n",
      "Trained batch 437 batch loss 1.52822673 epoch total loss 1.67000425\n",
      "Trained batch 438 batch loss 1.64492297 epoch total loss 1.66994691\n",
      "Trained batch 439 batch loss 1.58354735 epoch total loss 1.66975021\n",
      "Trained batch 440 batch loss 1.5511682 epoch total loss 1.66948056\n",
      "Trained batch 441 batch loss 1.6385591 epoch total loss 1.66941047\n",
      "Trained batch 442 batch loss 1.53572083 epoch total loss 1.66910803\n",
      "Trained batch 443 batch loss 1.59826303 epoch total loss 1.66894805\n",
      "Trained batch 444 batch loss 1.62180924 epoch total loss 1.66884196\n",
      "Trained batch 445 batch loss 1.52789617 epoch total loss 1.66852522\n",
      "Trained batch 446 batch loss 1.59990168 epoch total loss 1.66837132\n",
      "Trained batch 447 batch loss 1.63878131 epoch total loss 1.66830516\n",
      "Trained batch 448 batch loss 1.58592939 epoch total loss 1.66812134\n",
      "Trained batch 449 batch loss 1.66698885 epoch total loss 1.66811883\n",
      "Trained batch 450 batch loss 1.64241982 epoch total loss 1.66806161\n",
      "Trained batch 451 batch loss 1.58788908 epoch total loss 1.66788387\n",
      "Trained batch 452 batch loss 1.53315306 epoch total loss 1.66758585\n",
      "Trained batch 453 batch loss 1.50141811 epoch total loss 1.66721892\n",
      "Trained batch 454 batch loss 1.42991734 epoch total loss 1.66669631\n",
      "Trained batch 455 batch loss 1.56944621 epoch total loss 1.66648257\n",
      "Trained batch 456 batch loss 1.53544104 epoch total loss 1.66619527\n",
      "Trained batch 457 batch loss 1.54044843 epoch total loss 1.66592014\n",
      "Trained batch 458 batch loss 1.52224147 epoch total loss 1.66560638\n",
      "Trained batch 459 batch loss 1.48075759 epoch total loss 1.66520369\n",
      "Trained batch 460 batch loss 1.59728086 epoch total loss 1.66505599\n",
      "Trained batch 461 batch loss 1.58344436 epoch total loss 1.66487896\n",
      "Trained batch 462 batch loss 1.60918176 epoch total loss 1.66475844\n",
      "Trained batch 463 batch loss 1.56858742 epoch total loss 1.66455078\n",
      "Trained batch 464 batch loss 1.60347879 epoch total loss 1.66441905\n",
      "Trained batch 465 batch loss 1.61095726 epoch total loss 1.66430414\n",
      "Trained batch 466 batch loss 1.61193919 epoch total loss 1.66419172\n",
      "Trained batch 467 batch loss 1.51421547 epoch total loss 1.66387069\n",
      "Trained batch 468 batch loss 1.4279784 epoch total loss 1.66336656\n",
      "Trained batch 469 batch loss 1.40340114 epoch total loss 1.66281223\n",
      "Trained batch 470 batch loss 1.51678133 epoch total loss 1.66250157\n",
      "Trained batch 471 batch loss 1.48846638 epoch total loss 1.66213202\n",
      "Trained batch 472 batch loss 1.64778018 epoch total loss 1.66210163\n",
      "Trained batch 473 batch loss 1.54555881 epoch total loss 1.66185522\n",
      "Trained batch 474 batch loss 1.56054854 epoch total loss 1.66164148\n",
      "Trained batch 475 batch loss 1.41204596 epoch total loss 1.661116\n",
      "Trained batch 476 batch loss 1.4886117 epoch total loss 1.66075349\n",
      "Trained batch 477 batch loss 1.55261731 epoch total loss 1.66052675\n",
      "Trained batch 478 batch loss 1.58342147 epoch total loss 1.66036558\n",
      "Trained batch 479 batch loss 1.55959535 epoch total loss 1.66015506\n",
      "Trained batch 480 batch loss 1.52467501 epoch total loss 1.65987277\n",
      "Trained batch 481 batch loss 1.58434486 epoch total loss 1.65971577\n",
      "Trained batch 482 batch loss 1.6126827 epoch total loss 1.65961826\n",
      "Trained batch 483 batch loss 1.53161383 epoch total loss 1.65935314\n",
      "Trained batch 484 batch loss 1.59201336 epoch total loss 1.65921414\n",
      "Trained batch 485 batch loss 1.49731541 epoch total loss 1.65888035\n",
      "Trained batch 486 batch loss 1.52334595 epoch total loss 1.65860152\n",
      "Trained batch 487 batch loss 1.4153564 epoch total loss 1.65810204\n",
      "Trained batch 488 batch loss 1.61674583 epoch total loss 1.65801728\n",
      "Trained batch 489 batch loss 1.53350306 epoch total loss 1.65776265\n",
      "Trained batch 490 batch loss 1.61402869 epoch total loss 1.65767336\n",
      "Trained batch 491 batch loss 1.67021322 epoch total loss 1.65769899\n",
      "Trained batch 492 batch loss 1.71446013 epoch total loss 1.65781438\n",
      "Trained batch 493 batch loss 1.69971132 epoch total loss 1.65789926\n",
      "Trained batch 494 batch loss 1.6466682 epoch total loss 1.65787661\n",
      "Trained batch 495 batch loss 1.49224865 epoch total loss 1.65754199\n",
      "Trained batch 496 batch loss 1.35793924 epoch total loss 1.65693784\n",
      "Trained batch 497 batch loss 1.26346445 epoch total loss 1.65614617\n",
      "Trained batch 498 batch loss 1.3455832 epoch total loss 1.65552258\n",
      "Trained batch 499 batch loss 1.26456749 epoch total loss 1.65473914\n",
      "Trained batch 500 batch loss 1.30448163 epoch total loss 1.65403867\n",
      "Trained batch 501 batch loss 1.41420949 epoch total loss 1.65355992\n",
      "Trained batch 502 batch loss 1.45098543 epoch total loss 1.6531564\n",
      "Trained batch 503 batch loss 1.39216363 epoch total loss 1.65263748\n",
      "Trained batch 504 batch loss 1.56290579 epoch total loss 1.6524595\n",
      "Trained batch 505 batch loss 1.5834372 epoch total loss 1.65232289\n",
      "Trained batch 506 batch loss 1.46084225 epoch total loss 1.6519444\n",
      "Trained batch 507 batch loss 1.61537766 epoch total loss 1.65187216\n",
      "Trained batch 508 batch loss 1.54833066 epoch total loss 1.65166843\n",
      "Trained batch 509 batch loss 1.56675553 epoch total loss 1.65150166\n",
      "Trained batch 510 batch loss 1.56722379 epoch total loss 1.65133631\n",
      "Trained batch 511 batch loss 1.47488761 epoch total loss 1.65099108\n",
      "Trained batch 512 batch loss 1.52413213 epoch total loss 1.65074325\n",
      "Trained batch 513 batch loss 1.50697422 epoch total loss 1.65046299\n",
      "Trained batch 514 batch loss 1.49976325 epoch total loss 1.65016973\n",
      "Trained batch 515 batch loss 1.58853161 epoch total loss 1.65005016\n",
      "Trained batch 516 batch loss 1.6900878 epoch total loss 1.65012765\n",
      "Trained batch 517 batch loss 1.53743446 epoch total loss 1.64990962\n",
      "Trained batch 518 batch loss 1.5463537 epoch total loss 1.6497097\n",
      "Trained batch 519 batch loss 1.41361189 epoch total loss 1.6492548\n",
      "Trained batch 520 batch loss 1.42688191 epoch total loss 1.6488272\n",
      "Trained batch 521 batch loss 1.36831117 epoch total loss 1.64828873\n",
      "Trained batch 522 batch loss 1.59515703 epoch total loss 1.64818692\n",
      "Trained batch 523 batch loss 1.66321135 epoch total loss 1.64821565\n",
      "Trained batch 524 batch loss 1.65694392 epoch total loss 1.64823222\n",
      "Trained batch 525 batch loss 1.63197458 epoch total loss 1.64820123\n",
      "Trained batch 526 batch loss 1.6576283 epoch total loss 1.64821923\n",
      "Trained batch 527 batch loss 1.62120843 epoch total loss 1.64816797\n",
      "Trained batch 528 batch loss 1.62179399 epoch total loss 1.64811802\n",
      "Trained batch 529 batch loss 1.4836359 epoch total loss 1.64780712\n",
      "Trained batch 530 batch loss 1.56275964 epoch total loss 1.64764655\n",
      "Trained batch 531 batch loss 1.63415635 epoch total loss 1.64762115\n",
      "Trained batch 532 batch loss 1.55980635 epoch total loss 1.64745617\n",
      "Trained batch 533 batch loss 1.54270065 epoch total loss 1.64725959\n",
      "Trained batch 534 batch loss 1.59301877 epoch total loss 1.64715803\n",
      "Trained batch 535 batch loss 1.58440816 epoch total loss 1.64704072\n",
      "Trained batch 536 batch loss 1.56321502 epoch total loss 1.64688444\n",
      "Trained batch 537 batch loss 1.58337593 epoch total loss 1.64676619\n",
      "Trained batch 538 batch loss 1.73812842 epoch total loss 1.64693594\n",
      "Trained batch 539 batch loss 1.64837933 epoch total loss 1.64693856\n",
      "Trained batch 540 batch loss 1.66249478 epoch total loss 1.64696729\n",
      "Trained batch 541 batch loss 1.63127851 epoch total loss 1.64693832\n",
      "Trained batch 542 batch loss 1.64029086 epoch total loss 1.64692616\n",
      "Trained batch 543 batch loss 1.63261521 epoch total loss 1.64689982\n",
      "Trained batch 544 batch loss 1.53868639 epoch total loss 1.64670086\n",
      "Trained batch 545 batch loss 1.57793283 epoch total loss 1.64657474\n",
      "Trained batch 546 batch loss 1.44629884 epoch total loss 1.64620793\n",
      "Trained batch 547 batch loss 1.61317992 epoch total loss 1.64614749\n",
      "Trained batch 548 batch loss 1.53093636 epoch total loss 1.64593732\n",
      "Trained batch 549 batch loss 1.53691018 epoch total loss 1.64573872\n",
      "Trained batch 550 batch loss 1.54820681 epoch total loss 1.64556146\n",
      "Trained batch 551 batch loss 1.49759603 epoch total loss 1.64529288\n",
      "Trained batch 552 batch loss 1.57177138 epoch total loss 1.64515972\n",
      "Trained batch 553 batch loss 1.55893958 epoch total loss 1.64500391\n",
      "Trained batch 554 batch loss 1.59593678 epoch total loss 1.64491534\n",
      "Trained batch 555 batch loss 1.54589415 epoch total loss 1.64473689\n",
      "Trained batch 556 batch loss 1.57221389 epoch total loss 1.64460647\n",
      "Trained batch 557 batch loss 1.62440908 epoch total loss 1.64457011\n",
      "Trained batch 558 batch loss 1.60315847 epoch total loss 1.64449596\n",
      "Trained batch 559 batch loss 1.59550822 epoch total loss 1.64440835\n",
      "Trained batch 560 batch loss 1.53990817 epoch total loss 1.64422166\n",
      "Trained batch 561 batch loss 1.55898333 epoch total loss 1.64406979\n",
      "Trained batch 562 batch loss 1.4924016 epoch total loss 1.6437999\n",
      "Trained batch 563 batch loss 1.39009166 epoch total loss 1.64334929\n",
      "Trained batch 564 batch loss 1.51931274 epoch total loss 1.64312923\n",
      "Trained batch 565 batch loss 1.38595927 epoch total loss 1.64267421\n",
      "Trained batch 566 batch loss 1.54214823 epoch total loss 1.64249659\n",
      "Trained batch 567 batch loss 1.5250268 epoch total loss 1.6422894\n",
      "Trained batch 568 batch loss 1.50654376 epoch total loss 1.64205039\n",
      "Trained batch 569 batch loss 1.53533506 epoch total loss 1.64186287\n",
      "Trained batch 570 batch loss 1.53385091 epoch total loss 1.64167345\n",
      "Trained batch 571 batch loss 1.57944667 epoch total loss 1.64156449\n",
      "Trained batch 572 batch loss 1.595191 epoch total loss 1.64148343\n",
      "Trained batch 573 batch loss 1.6174283 epoch total loss 1.64144146\n",
      "Trained batch 574 batch loss 1.5507133 epoch total loss 1.64128339\n",
      "Trained batch 575 batch loss 1.56659055 epoch total loss 1.64115345\n",
      "Trained batch 576 batch loss 1.54608035 epoch total loss 1.64098847\n",
      "Trained batch 577 batch loss 1.50950527 epoch total loss 1.64076066\n",
      "Trained batch 578 batch loss 1.48177552 epoch total loss 1.64048553\n",
      "Trained batch 579 batch loss 1.60858524 epoch total loss 1.64043045\n",
      "Trained batch 580 batch loss 1.64624214 epoch total loss 1.64044046\n",
      "Trained batch 581 batch loss 1.60013175 epoch total loss 1.64037108\n",
      "Trained batch 582 batch loss 1.47396314 epoch total loss 1.6400851\n",
      "Trained batch 583 batch loss 1.43543136 epoch total loss 1.63973403\n",
      "Trained batch 584 batch loss 1.54135394 epoch total loss 1.63956571\n",
      "Trained batch 585 batch loss 1.5435164 epoch total loss 1.63940144\n",
      "Trained batch 586 batch loss 1.55412078 epoch total loss 1.639256\n",
      "Trained batch 587 batch loss 1.50715184 epoch total loss 1.63903093\n",
      "Trained batch 588 batch loss 1.50836074 epoch total loss 1.63880873\n",
      "Trained batch 589 batch loss 1.54303467 epoch total loss 1.63864613\n",
      "Trained batch 590 batch loss 1.56732166 epoch total loss 1.63852513\n",
      "Trained batch 591 batch loss 1.59860146 epoch total loss 1.63845754\n",
      "Trained batch 592 batch loss 1.51225495 epoch total loss 1.63824439\n",
      "Trained batch 593 batch loss 1.47405243 epoch total loss 1.63796759\n",
      "Trained batch 594 batch loss 1.41805482 epoch total loss 1.63759732\n",
      "Trained batch 595 batch loss 1.45778084 epoch total loss 1.63729501\n",
      "Trained batch 596 batch loss 1.48637104 epoch total loss 1.63704181\n",
      "Trained batch 597 batch loss 1.48785567 epoch total loss 1.63679194\n",
      "Trained batch 598 batch loss 1.57460904 epoch total loss 1.63668787\n",
      "Trained batch 599 batch loss 1.61948884 epoch total loss 1.63665926\n",
      "Trained batch 600 batch loss 1.55923653 epoch total loss 1.63653028\n",
      "Trained batch 601 batch loss 1.55767095 epoch total loss 1.63639903\n",
      "Trained batch 602 batch loss 1.52588892 epoch total loss 1.63621545\n",
      "Trained batch 603 batch loss 1.49854708 epoch total loss 1.63598716\n",
      "Trained batch 604 batch loss 1.53588116 epoch total loss 1.63582146\n",
      "Trained batch 605 batch loss 1.54177523 epoch total loss 1.63566589\n",
      "Trained batch 606 batch loss 1.5037837 epoch total loss 1.63544834\n",
      "Trained batch 607 batch loss 1.61733091 epoch total loss 1.63541842\n",
      "Trained batch 608 batch loss 1.6402812 epoch total loss 1.6354264\n",
      "Trained batch 609 batch loss 1.58807111 epoch total loss 1.63534856\n",
      "Trained batch 610 batch loss 1.62579286 epoch total loss 1.63533294\n",
      "Trained batch 611 batch loss 1.5403285 epoch total loss 1.63517749\n",
      "Trained batch 612 batch loss 1.58747745 epoch total loss 1.63509953\n",
      "Trained batch 613 batch loss 1.61832106 epoch total loss 1.63507223\n",
      "Trained batch 614 batch loss 1.57556868 epoch total loss 1.63497531\n",
      "Trained batch 615 batch loss 1.55013561 epoch total loss 1.63483727\n",
      "Trained batch 616 batch loss 1.48428023 epoch total loss 1.63459277\n",
      "Trained batch 617 batch loss 1.424891 epoch total loss 1.63425291\n",
      "Trained batch 618 batch loss 1.28526342 epoch total loss 1.63368821\n",
      "Trained batch 619 batch loss 1.48997426 epoch total loss 1.63345611\n",
      "Trained batch 620 batch loss 1.34374189 epoch total loss 1.63298881\n",
      "Trained batch 621 batch loss 1.27604604 epoch total loss 1.6324141\n",
      "Trained batch 622 batch loss 1.29741752 epoch total loss 1.63187551\n",
      "Trained batch 623 batch loss 1.21413386 epoch total loss 1.63120496\n",
      "Trained batch 624 batch loss 1.41477013 epoch total loss 1.63085806\n",
      "Trained batch 625 batch loss 1.63551474 epoch total loss 1.63086557\n",
      "Trained batch 626 batch loss 1.55527389 epoch total loss 1.63074481\n",
      "Trained batch 627 batch loss 1.50140691 epoch total loss 1.63053858\n",
      "Trained batch 628 batch loss 1.52105892 epoch total loss 1.63036418\n",
      "Trained batch 629 batch loss 1.62197042 epoch total loss 1.63035095\n",
      "Trained batch 630 batch loss 1.67792118 epoch total loss 1.63042653\n",
      "Trained batch 631 batch loss 1.42599428 epoch total loss 1.63010263\n",
      "Trained batch 632 batch loss 1.54056203 epoch total loss 1.62996089\n",
      "Trained batch 633 batch loss 1.33511961 epoch total loss 1.62949502\n",
      "Trained batch 634 batch loss 1.49058735 epoch total loss 1.62927592\n",
      "Trained batch 635 batch loss 1.55331969 epoch total loss 1.62915635\n",
      "Trained batch 636 batch loss 1.43852031 epoch total loss 1.62885654\n",
      "Trained batch 637 batch loss 1.32805943 epoch total loss 1.62838423\n",
      "Trained batch 638 batch loss 1.31277657 epoch total loss 1.62788951\n",
      "Trained batch 639 batch loss 1.45262349 epoch total loss 1.62761521\n",
      "Trained batch 640 batch loss 1.4627707 epoch total loss 1.62735772\n",
      "Trained batch 641 batch loss 1.26809192 epoch total loss 1.6267972\n",
      "Trained batch 642 batch loss 1.48072934 epoch total loss 1.62656963\n",
      "Trained batch 643 batch loss 1.43259442 epoch total loss 1.62626803\n",
      "Trained batch 644 batch loss 1.49245024 epoch total loss 1.62606013\n",
      "Trained batch 645 batch loss 1.4598887 epoch total loss 1.6258024\n",
      "Trained batch 646 batch loss 1.40642214 epoch total loss 1.62546277\n",
      "Trained batch 647 batch loss 1.44986296 epoch total loss 1.62519133\n",
      "Trained batch 648 batch loss 1.53377354 epoch total loss 1.62505031\n",
      "Trained batch 649 batch loss 1.58980083 epoch total loss 1.62499607\n",
      "Trained batch 650 batch loss 1.58633244 epoch total loss 1.62493658\n",
      "Trained batch 651 batch loss 1.51496959 epoch total loss 1.62476766\n",
      "Trained batch 652 batch loss 1.58118153 epoch total loss 1.62470078\n",
      "Trained batch 653 batch loss 1.56714523 epoch total loss 1.62461269\n",
      "Trained batch 654 batch loss 1.56690347 epoch total loss 1.62452435\n",
      "Trained batch 655 batch loss 1.44842219 epoch total loss 1.62425542\n",
      "Trained batch 656 batch loss 1.53222382 epoch total loss 1.62411523\n",
      "Trained batch 657 batch loss 1.50691402 epoch total loss 1.62393689\n",
      "Trained batch 658 batch loss 1.49178064 epoch total loss 1.62373602\n",
      "Trained batch 659 batch loss 1.52789927 epoch total loss 1.62359071\n",
      "Trained batch 660 batch loss 1.4890573 epoch total loss 1.62338686\n",
      "Trained batch 661 batch loss 1.53907037 epoch total loss 1.62325931\n",
      "Trained batch 662 batch loss 1.57486045 epoch total loss 1.62318611\n",
      "Trained batch 663 batch loss 1.5563997 epoch total loss 1.62308538\n",
      "Trained batch 664 batch loss 1.621176 epoch total loss 1.62308252\n",
      "Trained batch 665 batch loss 1.55575538 epoch total loss 1.62298131\n",
      "Trained batch 666 batch loss 1.5862608 epoch total loss 1.62292624\n",
      "Trained batch 667 batch loss 1.52880514 epoch total loss 1.62278521\n",
      "Trained batch 668 batch loss 1.46637821 epoch total loss 1.62255108\n",
      "Trained batch 669 batch loss 1.51057196 epoch total loss 1.62238383\n",
      "Trained batch 670 batch loss 1.43598294 epoch total loss 1.62210572\n",
      "Trained batch 671 batch loss 1.55548453 epoch total loss 1.62200642\n",
      "Trained batch 672 batch loss 1.51808679 epoch total loss 1.6218518\n",
      "Trained batch 673 batch loss 1.36036539 epoch total loss 1.62146318\n",
      "Trained batch 674 batch loss 1.39168119 epoch total loss 1.62112236\n",
      "Trained batch 675 batch loss 1.47749972 epoch total loss 1.62090969\n",
      "Trained batch 676 batch loss 1.48963737 epoch total loss 1.6207155\n",
      "Trained batch 677 batch loss 1.50712323 epoch total loss 1.62054753\n",
      "Trained batch 678 batch loss 1.60191941 epoch total loss 1.62052011\n",
      "Trained batch 679 batch loss 1.42362165 epoch total loss 1.62023008\n",
      "Trained batch 680 batch loss 1.43972659 epoch total loss 1.6199646\n",
      "Trained batch 681 batch loss 1.47621095 epoch total loss 1.61975348\n",
      "Trained batch 682 batch loss 1.52919269 epoch total loss 1.61962068\n",
      "Trained batch 683 batch loss 1.5346936 epoch total loss 1.61949635\n",
      "Trained batch 684 batch loss 1.44766068 epoch total loss 1.61924505\n",
      "Trained batch 685 batch loss 1.41960967 epoch total loss 1.61895347\n",
      "Trained batch 686 batch loss 1.47718346 epoch total loss 1.61874688\n",
      "Trained batch 687 batch loss 1.55960047 epoch total loss 1.61866069\n",
      "Trained batch 688 batch loss 1.58551991 epoch total loss 1.61861265\n",
      "Trained batch 689 batch loss 1.57236218 epoch total loss 1.61854553\n",
      "Trained batch 690 batch loss 1.40574956 epoch total loss 1.61823714\n",
      "Trained batch 691 batch loss 1.46673083 epoch total loss 1.61801779\n",
      "Trained batch 692 batch loss 1.45796657 epoch total loss 1.61778653\n",
      "Trained batch 693 batch loss 1.47175658 epoch total loss 1.61757588\n",
      "Trained batch 694 batch loss 1.47916532 epoch total loss 1.61737645\n",
      "Trained batch 695 batch loss 1.4859556 epoch total loss 1.61718738\n",
      "Trained batch 696 batch loss 1.49765825 epoch total loss 1.6170156\n",
      "Trained batch 697 batch loss 1.48649359 epoch total loss 1.61682832\n",
      "Trained batch 698 batch loss 1.52222276 epoch total loss 1.61669278\n",
      "Trained batch 699 batch loss 1.51159978 epoch total loss 1.61654234\n",
      "Trained batch 700 batch loss 1.53258598 epoch total loss 1.61642241\n",
      "Trained batch 701 batch loss 1.52486181 epoch total loss 1.61629188\n",
      "Trained batch 702 batch loss 1.56854105 epoch total loss 1.61622381\n",
      "Trained batch 703 batch loss 1.55051589 epoch total loss 1.61613035\n",
      "Trained batch 704 batch loss 1.58204556 epoch total loss 1.61608195\n",
      "Trained batch 705 batch loss 1.58111215 epoch total loss 1.61603224\n",
      "Trained batch 706 batch loss 1.59892976 epoch total loss 1.61600792\n",
      "Trained batch 707 batch loss 1.59549403 epoch total loss 1.61597884\n",
      "Trained batch 708 batch loss 1.61197782 epoch total loss 1.61597323\n",
      "Trained batch 709 batch loss 1.62707484 epoch total loss 1.61598885\n",
      "Trained batch 710 batch loss 1.51883519 epoch total loss 1.615852\n",
      "Trained batch 711 batch loss 1.59001207 epoch total loss 1.61581552\n",
      "Trained batch 712 batch loss 1.6044929 epoch total loss 1.61579967\n",
      "Trained batch 713 batch loss 1.68257213 epoch total loss 1.61589336\n",
      "Trained batch 714 batch loss 1.60322154 epoch total loss 1.61587572\n",
      "Trained batch 715 batch loss 1.55640793 epoch total loss 1.61579251\n",
      "Trained batch 716 batch loss 1.41278362 epoch total loss 1.61550903\n",
      "Trained batch 717 batch loss 1.27464378 epoch total loss 1.61503363\n",
      "Trained batch 718 batch loss 1.2541101 epoch total loss 1.61453104\n",
      "Trained batch 719 batch loss 1.47924066 epoch total loss 1.61434293\n",
      "Trained batch 720 batch loss 1.52968919 epoch total loss 1.61422527\n",
      "Trained batch 721 batch loss 1.75969887 epoch total loss 1.61442697\n",
      "Trained batch 722 batch loss 1.46316481 epoch total loss 1.6142174\n",
      "Trained batch 723 batch loss 1.51089215 epoch total loss 1.61407447\n",
      "Trained batch 724 batch loss 1.46329117 epoch total loss 1.61386609\n",
      "Trained batch 725 batch loss 1.61573482 epoch total loss 1.61386871\n",
      "Trained batch 726 batch loss 1.64553082 epoch total loss 1.61391222\n",
      "Trained batch 727 batch loss 1.55090928 epoch total loss 1.61382556\n",
      "Trained batch 728 batch loss 1.58902228 epoch total loss 1.61379147\n",
      "Trained batch 729 batch loss 1.60759127 epoch total loss 1.61378288\n",
      "Trained batch 730 batch loss 1.61160398 epoch total loss 1.6137799\n",
      "Trained batch 731 batch loss 1.52557743 epoch total loss 1.61365926\n",
      "Trained batch 732 batch loss 1.50386238 epoch total loss 1.61350942\n",
      "Trained batch 733 batch loss 1.50051451 epoch total loss 1.61335516\n",
      "Trained batch 734 batch loss 1.46451664 epoch total loss 1.61315238\n",
      "Trained batch 735 batch loss 1.53267801 epoch total loss 1.61304295\n",
      "Trained batch 736 batch loss 1.50104225 epoch total loss 1.61289084\n",
      "Trained batch 737 batch loss 1.379264 epoch total loss 1.61257386\n",
      "Trained batch 738 batch loss 1.49232233 epoch total loss 1.6124109\n",
      "Trained batch 739 batch loss 1.49598324 epoch total loss 1.61225331\n",
      "Trained batch 740 batch loss 1.49077141 epoch total loss 1.61208904\n",
      "Trained batch 741 batch loss 1.35403275 epoch total loss 1.61174083\n",
      "Trained batch 742 batch loss 1.34819567 epoch total loss 1.61138558\n",
      "Trained batch 743 batch loss 1.43582726 epoch total loss 1.61114919\n",
      "Trained batch 744 batch loss 1.40729761 epoch total loss 1.61087525\n",
      "Trained batch 745 batch loss 1.57776356 epoch total loss 1.61083078\n",
      "Trained batch 746 batch loss 1.54306078 epoch total loss 1.61074007\n",
      "Trained batch 747 batch loss 1.45808852 epoch total loss 1.61053574\n",
      "Trained batch 748 batch loss 1.47290921 epoch total loss 1.61035168\n",
      "Trained batch 749 batch loss 1.54472065 epoch total loss 1.61026406\n",
      "Trained batch 750 batch loss 1.6087836 epoch total loss 1.61026204\n",
      "Trained batch 751 batch loss 1.54282665 epoch total loss 1.61017227\n",
      "Trained batch 752 batch loss 1.48645711 epoch total loss 1.61000776\n",
      "Trained batch 753 batch loss 1.54334807 epoch total loss 1.60991919\n",
      "Trained batch 754 batch loss 1.43011749 epoch total loss 1.60968077\n",
      "Trained batch 755 batch loss 1.44808686 epoch total loss 1.60946679\n",
      "Trained batch 756 batch loss 1.45584714 epoch total loss 1.60926354\n",
      "Trained batch 757 batch loss 1.47935498 epoch total loss 1.609092\n",
      "Trained batch 758 batch loss 1.41819561 epoch total loss 1.60884023\n",
      "Trained batch 759 batch loss 1.41968405 epoch total loss 1.60859096\n",
      "Trained batch 760 batch loss 1.45419705 epoch total loss 1.60838783\n",
      "Trained batch 761 batch loss 1.59058332 epoch total loss 1.60836446\n",
      "Trained batch 762 batch loss 1.48542225 epoch total loss 1.60820317\n",
      "Trained batch 763 batch loss 1.5099721 epoch total loss 1.60807443\n",
      "Trained batch 764 batch loss 1.52449751 epoch total loss 1.60796511\n",
      "Trained batch 765 batch loss 1.45153046 epoch total loss 1.60776067\n",
      "Trained batch 766 batch loss 1.40665674 epoch total loss 1.60749805\n",
      "Trained batch 767 batch loss 1.41910946 epoch total loss 1.60725236\n",
      "Trained batch 768 batch loss 1.52051592 epoch total loss 1.60713947\n",
      "Trained batch 769 batch loss 1.51519501 epoch total loss 1.60701978\n",
      "Trained batch 770 batch loss 1.46524596 epoch total loss 1.6068356\n",
      "Trained batch 771 batch loss 1.41929603 epoch total loss 1.60659242\n",
      "Trained batch 772 batch loss 1.44281292 epoch total loss 1.60638034\n",
      "Trained batch 773 batch loss 1.52594471 epoch total loss 1.60627639\n",
      "Trained batch 774 batch loss 1.55304754 epoch total loss 1.60620761\n",
      "Trained batch 775 batch loss 1.68022537 epoch total loss 1.6063031\n",
      "Trained batch 776 batch loss 1.55187321 epoch total loss 1.606233\n",
      "Trained batch 777 batch loss 1.56296921 epoch total loss 1.60617733\n",
      "Trained batch 778 batch loss 1.48846722 epoch total loss 1.60602605\n",
      "Trained batch 779 batch loss 1.62015533 epoch total loss 1.60604417\n",
      "Trained batch 780 batch loss 1.49039865 epoch total loss 1.60589588\n",
      "Trained batch 781 batch loss 1.50299621 epoch total loss 1.60576415\n",
      "Trained batch 782 batch loss 1.48730981 epoch total loss 1.60561264\n",
      "Trained batch 783 batch loss 1.55909634 epoch total loss 1.60555327\n",
      "Trained batch 784 batch loss 1.54380155 epoch total loss 1.60547447\n",
      "Trained batch 785 batch loss 1.43753028 epoch total loss 1.60526049\n",
      "Trained batch 786 batch loss 1.40336645 epoch total loss 1.6050036\n",
      "Trained batch 787 batch loss 1.32917 epoch total loss 1.60465324\n",
      "Trained batch 788 batch loss 1.52714133 epoch total loss 1.60455477\n",
      "Trained batch 789 batch loss 1.58395326 epoch total loss 1.60452867\n",
      "Trained batch 790 batch loss 1.55974722 epoch total loss 1.60447192\n",
      "Trained batch 791 batch loss 1.56764901 epoch total loss 1.60442531\n",
      "Trained batch 792 batch loss 1.64224 epoch total loss 1.60447311\n",
      "Trained batch 793 batch loss 1.56684613 epoch total loss 1.60442567\n",
      "Trained batch 794 batch loss 1.60918367 epoch total loss 1.60443163\n",
      "Trained batch 795 batch loss 1.50928116 epoch total loss 1.60431194\n",
      "Trained batch 796 batch loss 1.52725077 epoch total loss 1.60421503\n",
      "Trained batch 797 batch loss 1.51000643 epoch total loss 1.60409689\n",
      "Trained batch 798 batch loss 1.54119086 epoch total loss 1.60401797\n",
      "Trained batch 799 batch loss 1.537745 epoch total loss 1.603935\n",
      "Trained batch 800 batch loss 1.59709215 epoch total loss 1.60392642\n",
      "Trained batch 801 batch loss 1.4851104 epoch total loss 1.603778\n",
      "Trained batch 802 batch loss 1.54897189 epoch total loss 1.6037097\n",
      "Trained batch 803 batch loss 1.58262885 epoch total loss 1.60368347\n",
      "Trained batch 804 batch loss 1.50945377 epoch total loss 1.60356617\n",
      "Trained batch 805 batch loss 1.377226 epoch total loss 1.60328496\n",
      "Trained batch 806 batch loss 1.22260678 epoch total loss 1.60281277\n",
      "Trained batch 807 batch loss 1.44988132 epoch total loss 1.60262322\n",
      "Trained batch 808 batch loss 1.48961067 epoch total loss 1.60248327\n",
      "Trained batch 809 batch loss 1.51765776 epoch total loss 1.60237849\n",
      "Trained batch 810 batch loss 1.53528512 epoch total loss 1.60229564\n",
      "Trained batch 811 batch loss 1.51468325 epoch total loss 1.60218763\n",
      "Trained batch 812 batch loss 1.48116755 epoch total loss 1.60203862\n",
      "Trained batch 813 batch loss 1.49755383 epoch total loss 1.60191011\n",
      "Trained batch 814 batch loss 1.41084552 epoch total loss 1.60167539\n",
      "Trained batch 815 batch loss 1.40275478 epoch total loss 1.60143125\n",
      "Trained batch 816 batch loss 1.46834314 epoch total loss 1.60126829\n",
      "Trained batch 817 batch loss 1.48478496 epoch total loss 1.6011256\n",
      "Trained batch 818 batch loss 1.48699784 epoch total loss 1.600986\n",
      "Trained batch 819 batch loss 1.46518314 epoch total loss 1.60082018\n",
      "Trained batch 820 batch loss 1.54205084 epoch total loss 1.60074854\n",
      "Trained batch 821 batch loss 1.49228549 epoch total loss 1.60061646\n",
      "Trained batch 822 batch loss 1.50254011 epoch total loss 1.60049713\n",
      "Trained batch 823 batch loss 1.4617399 epoch total loss 1.60032856\n",
      "Trained batch 824 batch loss 1.5057919 epoch total loss 1.60021377\n",
      "Trained batch 825 batch loss 1.42121673 epoch total loss 1.59999692\n",
      "Trained batch 826 batch loss 1.42734241 epoch total loss 1.59978795\n",
      "Trained batch 827 batch loss 1.54415417 epoch total loss 1.59972072\n",
      "Trained batch 828 batch loss 1.51023853 epoch total loss 1.59961259\n",
      "Trained batch 829 batch loss 1.41686666 epoch total loss 1.59939218\n",
      "Trained batch 830 batch loss 1.53308284 epoch total loss 1.59931231\n",
      "Trained batch 831 batch loss 1.53663719 epoch total loss 1.59923685\n",
      "Trained batch 832 batch loss 1.50087488 epoch total loss 1.59911859\n",
      "Trained batch 833 batch loss 1.5991025 epoch total loss 1.59911859\n",
      "Trained batch 834 batch loss 1.46118665 epoch total loss 1.59895325\n",
      "Trained batch 835 batch loss 1.47144032 epoch total loss 1.59880054\n",
      "Trained batch 836 batch loss 1.38322282 epoch total loss 1.59854257\n",
      "Trained batch 837 batch loss 1.3968178 epoch total loss 1.59830165\n",
      "Trained batch 838 batch loss 1.38744831 epoch total loss 1.59805\n",
      "Trained batch 839 batch loss 1.42195773 epoch total loss 1.59784019\n",
      "Trained batch 840 batch loss 1.47644734 epoch total loss 1.59769559\n",
      "Trained batch 841 batch loss 1.4489224 epoch total loss 1.5975188\n",
      "Trained batch 842 batch loss 1.40753448 epoch total loss 1.59729326\n",
      "Trained batch 843 batch loss 1.41845655 epoch total loss 1.59708107\n",
      "Trained batch 844 batch loss 1.52753627 epoch total loss 1.59699869\n",
      "Trained batch 845 batch loss 1.35902178 epoch total loss 1.59671712\n",
      "Trained batch 846 batch loss 1.40357018 epoch total loss 1.59648883\n",
      "Trained batch 847 batch loss 1.28571105 epoch total loss 1.59612191\n",
      "Trained batch 848 batch loss 1.45140636 epoch total loss 1.59595132\n",
      "Trained batch 849 batch loss 1.48084307 epoch total loss 1.59581566\n",
      "Trained batch 850 batch loss 1.4580996 epoch total loss 1.59565377\n",
      "Trained batch 851 batch loss 1.48171902 epoch total loss 1.59551978\n",
      "Trained batch 852 batch loss 1.54096651 epoch total loss 1.59545588\n",
      "Trained batch 853 batch loss 1.47198963 epoch total loss 1.59531116\n",
      "Trained batch 854 batch loss 1.59052384 epoch total loss 1.59530556\n",
      "Trained batch 855 batch loss 1.40726876 epoch total loss 1.59508562\n",
      "Trained batch 856 batch loss 1.27968323 epoch total loss 1.59471714\n",
      "Trained batch 857 batch loss 1.31471419 epoch total loss 1.59439039\n",
      "Trained batch 858 batch loss 1.43488455 epoch total loss 1.59420455\n",
      "Trained batch 859 batch loss 1.40826321 epoch total loss 1.59398806\n",
      "Trained batch 860 batch loss 1.38499308 epoch total loss 1.59374499\n",
      "Trained batch 861 batch loss 1.49027729 epoch total loss 1.59362483\n",
      "Trained batch 862 batch loss 1.46194935 epoch total loss 1.593472\n",
      "Trained batch 863 batch loss 1.56352305 epoch total loss 1.59343731\n",
      "Trained batch 864 batch loss 1.48104835 epoch total loss 1.59330726\n",
      "Trained batch 865 batch loss 1.56959271 epoch total loss 1.59327984\n",
      "Trained batch 866 batch loss 1.69072032 epoch total loss 1.59339225\n",
      "Trained batch 867 batch loss 1.67399991 epoch total loss 1.59348512\n",
      "Trained batch 868 batch loss 1.45746303 epoch total loss 1.59332848\n",
      "Trained batch 869 batch loss 1.54761612 epoch total loss 1.5932759\n",
      "Trained batch 870 batch loss 1.6224525 epoch total loss 1.5933094\n",
      "Trained batch 871 batch loss 1.46604562 epoch total loss 1.59316337\n",
      "Trained batch 872 batch loss 1.5205183 epoch total loss 1.59308\n",
      "Trained batch 873 batch loss 1.53301561 epoch total loss 1.59301114\n",
      "Trained batch 874 batch loss 1.54123449 epoch total loss 1.59295189\n",
      "Trained batch 875 batch loss 1.5126164 epoch total loss 1.5928601\n",
      "Trained batch 876 batch loss 1.49990344 epoch total loss 1.59275389\n",
      "Trained batch 877 batch loss 1.5617075 epoch total loss 1.5927186\n",
      "Trained batch 878 batch loss 1.42538977 epoch total loss 1.5925281\n",
      "Trained batch 879 batch loss 1.46950829 epoch total loss 1.59238803\n",
      "Trained batch 880 batch loss 1.47738528 epoch total loss 1.59225738\n",
      "Trained batch 881 batch loss 1.46080875 epoch total loss 1.59210825\n",
      "Trained batch 882 batch loss 1.52454507 epoch total loss 1.5920316\n",
      "Trained batch 883 batch loss 1.52763724 epoch total loss 1.59195864\n",
      "Trained batch 884 batch loss 1.48009586 epoch total loss 1.59183204\n",
      "Trained batch 885 batch loss 1.47822058 epoch total loss 1.59170377\n",
      "Trained batch 886 batch loss 1.54245 epoch total loss 1.59164822\n",
      "Trained batch 887 batch loss 1.47258687 epoch total loss 1.59151387\n",
      "Trained batch 888 batch loss 1.63757873 epoch total loss 1.59156585\n",
      "Trained batch 889 batch loss 1.3969053 epoch total loss 1.59134674\n",
      "Trained batch 890 batch loss 1.41361308 epoch total loss 1.59114707\n",
      "Trained batch 891 batch loss 1.45314848 epoch total loss 1.59099209\n",
      "Trained batch 892 batch loss 1.55553198 epoch total loss 1.5909524\n",
      "Trained batch 893 batch loss 1.50391734 epoch total loss 1.59085488\n",
      "Trained batch 894 batch loss 1.5284493 epoch total loss 1.59078503\n",
      "Trained batch 895 batch loss 1.46500015 epoch total loss 1.59064448\n",
      "Trained batch 896 batch loss 1.35655737 epoch total loss 1.59038329\n",
      "Trained batch 897 batch loss 1.53833866 epoch total loss 1.59032524\n",
      "Trained batch 898 batch loss 1.40037584 epoch total loss 1.59011376\n",
      "Trained batch 899 batch loss 1.36823583 epoch total loss 1.589867\n",
      "Trained batch 900 batch loss 1.41988337 epoch total loss 1.58967817\n",
      "Trained batch 901 batch loss 1.41443169 epoch total loss 1.58948362\n",
      "Trained batch 902 batch loss 1.44103813 epoch total loss 1.58931911\n",
      "Trained batch 903 batch loss 1.25024748 epoch total loss 1.5889436\n",
      "Trained batch 904 batch loss 1.43776989 epoch total loss 1.58877635\n",
      "Trained batch 905 batch loss 1.48627567 epoch total loss 1.5886631\n",
      "Trained batch 906 batch loss 1.36687553 epoch total loss 1.58841825\n",
      "Trained batch 907 batch loss 1.52976584 epoch total loss 1.58835363\n",
      "Trained batch 908 batch loss 1.48907793 epoch total loss 1.58824432\n",
      "Trained batch 909 batch loss 1.53335011 epoch total loss 1.58818388\n",
      "Trained batch 910 batch loss 1.49480224 epoch total loss 1.58808124\n",
      "Trained batch 911 batch loss 1.46952629 epoch total loss 1.58795106\n",
      "Trained batch 912 batch loss 1.45328 epoch total loss 1.58780336\n",
      "Trained batch 913 batch loss 1.43760824 epoch total loss 1.58763885\n",
      "Trained batch 914 batch loss 1.5942657 epoch total loss 1.58764613\n",
      "Trained batch 915 batch loss 1.40857124 epoch total loss 1.58745039\n",
      "Trained batch 916 batch loss 1.44431388 epoch total loss 1.5872941\n",
      "Trained batch 917 batch loss 1.4551456 epoch total loss 1.5871501\n",
      "Trained batch 918 batch loss 1.4441247 epoch total loss 1.58699429\n",
      "Trained batch 919 batch loss 1.43649137 epoch total loss 1.5868305\n",
      "Trained batch 920 batch loss 1.51903749 epoch total loss 1.58675683\n",
      "Trained batch 921 batch loss 1.42193282 epoch total loss 1.58657777\n",
      "Trained batch 922 batch loss 1.42099988 epoch total loss 1.58639824\n",
      "Trained batch 923 batch loss 1.49951744 epoch total loss 1.58630407\n",
      "Trained batch 924 batch loss 1.48342049 epoch total loss 1.58619273\n",
      "Trained batch 925 batch loss 1.509 epoch total loss 1.58610928\n",
      "Trained batch 926 batch loss 1.48315465 epoch total loss 1.58599818\n",
      "Trained batch 927 batch loss 1.45230114 epoch total loss 1.58585393\n",
      "Trained batch 928 batch loss 1.3678391 epoch total loss 1.58561885\n",
      "Trained batch 929 batch loss 1.40358329 epoch total loss 1.58542299\n",
      "Trained batch 930 batch loss 1.38956094 epoch total loss 1.58521235\n",
      "Trained batch 931 batch loss 1.4512347 epoch total loss 1.58506846\n",
      "Trained batch 932 batch loss 1.59548569 epoch total loss 1.58507955\n",
      "Trained batch 933 batch loss 1.49971831 epoch total loss 1.58498812\n",
      "Trained batch 934 batch loss 1.50301933 epoch total loss 1.58490038\n",
      "Trained batch 935 batch loss 1.47347 epoch total loss 1.58478129\n",
      "Trained batch 936 batch loss 1.43019915 epoch total loss 1.58461607\n",
      "Trained batch 937 batch loss 1.42354548 epoch total loss 1.58444428\n",
      "Trained batch 938 batch loss 1.44525647 epoch total loss 1.58429587\n",
      "Trained batch 939 batch loss 1.5126363 epoch total loss 1.58421969\n",
      "Trained batch 940 batch loss 1.36656594 epoch total loss 1.58398819\n",
      "Trained batch 941 batch loss 1.27809513 epoch total loss 1.58366311\n",
      "Trained batch 942 batch loss 1.23010802 epoch total loss 1.58328772\n",
      "Trained batch 943 batch loss 1.21764076 epoch total loss 1.58289993\n",
      "Trained batch 944 batch loss 1.50771296 epoch total loss 1.5828203\n",
      "Trained batch 945 batch loss 1.6006453 epoch total loss 1.58283913\n",
      "Trained batch 946 batch loss 1.47569847 epoch total loss 1.58272588\n",
      "Trained batch 947 batch loss 1.56682193 epoch total loss 1.58270907\n",
      "Trained batch 948 batch loss 1.53863478 epoch total loss 1.58266246\n",
      "Trained batch 949 batch loss 1.44289374 epoch total loss 1.58251512\n",
      "Trained batch 950 batch loss 1.31045556 epoch total loss 1.58222878\n",
      "Trained batch 951 batch loss 1.35883176 epoch total loss 1.58199394\n",
      "Trained batch 952 batch loss 1.45435286 epoch total loss 1.58185983\n",
      "Trained batch 953 batch loss 1.42422938 epoch total loss 1.58169436\n",
      "Trained batch 954 batch loss 1.4052304 epoch total loss 1.58150947\n",
      "Trained batch 955 batch loss 1.58107615 epoch total loss 1.58150899\n",
      "Trained batch 956 batch loss 1.5063194 epoch total loss 1.58143032\n",
      "Trained batch 957 batch loss 1.57709932 epoch total loss 1.58142591\n",
      "Trained batch 958 batch loss 1.61673284 epoch total loss 1.58146274\n",
      "Trained batch 959 batch loss 1.58622456 epoch total loss 1.58146763\n",
      "Trained batch 960 batch loss 1.62689471 epoch total loss 1.58151495\n",
      "Trained batch 961 batch loss 1.55431557 epoch total loss 1.5814867\n",
      "Trained batch 962 batch loss 1.47368276 epoch total loss 1.58137453\n",
      "Trained batch 963 batch loss 1.52284038 epoch total loss 1.58131373\n",
      "Trained batch 964 batch loss 1.57766807 epoch total loss 1.58130991\n",
      "Trained batch 965 batch loss 1.57582629 epoch total loss 1.58130431\n",
      "Trained batch 966 batch loss 1.5758996 epoch total loss 1.58129871\n",
      "Trained batch 967 batch loss 1.53416467 epoch total loss 1.58125\n",
      "Trained batch 968 batch loss 1.51237059 epoch total loss 1.58117878\n",
      "Trained batch 969 batch loss 1.51625586 epoch total loss 1.58111179\n",
      "Trained batch 970 batch loss 1.54209709 epoch total loss 1.5810715\n",
      "Trained batch 971 batch loss 1.61064434 epoch total loss 1.58110201\n",
      "Trained batch 972 batch loss 1.56348789 epoch total loss 1.58108377\n",
      "Trained batch 973 batch loss 1.58591115 epoch total loss 1.58108878\n",
      "Trained batch 974 batch loss 1.46402073 epoch total loss 1.58096862\n",
      "Trained batch 975 batch loss 1.3221221 epoch total loss 1.58070314\n",
      "Trained batch 976 batch loss 1.39776671 epoch total loss 1.58051574\n",
      "Trained batch 977 batch loss 1.45771396 epoch total loss 1.5803901\n",
      "Trained batch 978 batch loss 1.37004852 epoch total loss 1.58017492\n",
      "Trained batch 979 batch loss 1.42911363 epoch total loss 1.58002067\n",
      "Trained batch 980 batch loss 1.48748422 epoch total loss 1.57992613\n",
      "Trained batch 981 batch loss 1.46306908 epoch total loss 1.57980692\n",
      "Trained batch 982 batch loss 1.71284366 epoch total loss 1.57994246\n",
      "Trained batch 983 batch loss 1.65438592 epoch total loss 1.58001828\n",
      "Trained batch 984 batch loss 1.65107632 epoch total loss 1.58009052\n",
      "Trained batch 985 batch loss 1.60560381 epoch total loss 1.58011639\n",
      "Trained batch 986 batch loss 1.52938306 epoch total loss 1.58006501\n",
      "Trained batch 987 batch loss 1.55595493 epoch total loss 1.58004057\n",
      "Trained batch 988 batch loss 1.57320023 epoch total loss 1.58003366\n",
      "Trained batch 989 batch loss 1.45783317 epoch total loss 1.57991016\n",
      "Trained batch 990 batch loss 1.43961716 epoch total loss 1.57976842\n",
      "Trained batch 991 batch loss 1.53757358 epoch total loss 1.57972586\n",
      "Trained batch 992 batch loss 1.55468941 epoch total loss 1.57970059\n",
      "Trained batch 993 batch loss 1.59889269 epoch total loss 1.5797199\n",
      "Trained batch 994 batch loss 1.52866602 epoch total loss 1.57966852\n",
      "Trained batch 995 batch loss 1.49438572 epoch total loss 1.57958281\n",
      "Trained batch 996 batch loss 1.41716862 epoch total loss 1.57941973\n",
      "Trained batch 997 batch loss 1.41672111 epoch total loss 1.57925653\n",
      "Trained batch 998 batch loss 1.52178121 epoch total loss 1.57919896\n",
      "Trained batch 999 batch loss 1.71120286 epoch total loss 1.57933104\n",
      "Trained batch 1000 batch loss 1.64616036 epoch total loss 1.5793978\n",
      "Trained batch 1001 batch loss 1.5769912 epoch total loss 1.57939541\n",
      "Trained batch 1002 batch loss 1.39586663 epoch total loss 1.57921231\n",
      "Trained batch 1003 batch loss 1.46263266 epoch total loss 1.57909608\n",
      "Trained batch 1004 batch loss 1.42237031 epoch total loss 1.57894\n",
      "Trained batch 1005 batch loss 1.47258496 epoch total loss 1.57883406\n",
      "Trained batch 1006 batch loss 1.45816326 epoch total loss 1.57871413\n",
      "Trained batch 1007 batch loss 1.50399637 epoch total loss 1.57864\n",
      "Trained batch 1008 batch loss 1.52794683 epoch total loss 1.57858968\n",
      "Trained batch 1009 batch loss 1.49516511 epoch total loss 1.57850695\n",
      "Trained batch 1010 batch loss 1.54633117 epoch total loss 1.57847512\n",
      "Trained batch 1011 batch loss 1.46043825 epoch total loss 1.57835841\n",
      "Trained batch 1012 batch loss 1.45723748 epoch total loss 1.57823873\n",
      "Trained batch 1013 batch loss 1.52925813 epoch total loss 1.57819045\n",
      "Trained batch 1014 batch loss 1.47606063 epoch total loss 1.57808971\n",
      "Trained batch 1015 batch loss 1.4132067 epoch total loss 1.57792723\n",
      "Trained batch 1016 batch loss 1.43406618 epoch total loss 1.57778573\n",
      "Trained batch 1017 batch loss 1.4000349 epoch total loss 1.57761097\n",
      "Trained batch 1018 batch loss 1.45612717 epoch total loss 1.57749164\n",
      "Trained batch 1019 batch loss 1.49113536 epoch total loss 1.57740688\n",
      "Trained batch 1020 batch loss 1.50994229 epoch total loss 1.5773406\n",
      "Trained batch 1021 batch loss 1.40895462 epoch total loss 1.57717574\n",
      "Trained batch 1022 batch loss 1.49192142 epoch total loss 1.57709229\n",
      "Trained batch 1023 batch loss 1.50595856 epoch total loss 1.57702279\n",
      "Trained batch 1024 batch loss 1.55869722 epoch total loss 1.57700491\n",
      "Trained batch 1025 batch loss 1.30905616 epoch total loss 1.57674348\n",
      "Trained batch 1026 batch loss 1.33075762 epoch total loss 1.57650387\n",
      "Trained batch 1027 batch loss 1.29986227 epoch total loss 1.57623434\n",
      "Trained batch 1028 batch loss 1.3833096 epoch total loss 1.57604671\n",
      "Trained batch 1029 batch loss 1.52048242 epoch total loss 1.5759927\n",
      "Trained batch 1030 batch loss 1.45004487 epoch total loss 1.57587051\n",
      "Trained batch 1031 batch loss 1.41329074 epoch total loss 1.5757128\n",
      "Trained batch 1032 batch loss 1.44099391 epoch total loss 1.57558239\n",
      "Trained batch 1033 batch loss 1.44393075 epoch total loss 1.57545495\n",
      "Trained batch 1034 batch loss 1.41129756 epoch total loss 1.57529616\n",
      "Trained batch 1035 batch loss 1.51208472 epoch total loss 1.57523501\n",
      "Trained batch 1036 batch loss 1.48588276 epoch total loss 1.57514882\n",
      "Trained batch 1037 batch loss 1.4564507 epoch total loss 1.57503426\n",
      "Trained batch 1038 batch loss 1.42960715 epoch total loss 1.57489419\n",
      "Trained batch 1039 batch loss 1.33835721 epoch total loss 1.5746665\n",
      "Trained batch 1040 batch loss 1.36682665 epoch total loss 1.57446671\n",
      "Trained batch 1041 batch loss 1.4362427 epoch total loss 1.57433391\n",
      "Trained batch 1042 batch loss 1.45255816 epoch total loss 1.57421696\n",
      "Trained batch 1043 batch loss 1.37129 epoch total loss 1.57402253\n",
      "Trained batch 1044 batch loss 1.48125958 epoch total loss 1.5739336\n",
      "Trained batch 1045 batch loss 1.33267045 epoch total loss 1.57370269\n",
      "Trained batch 1046 batch loss 1.36756396 epoch total loss 1.57350564\n",
      "Trained batch 1047 batch loss 1.51576388 epoch total loss 1.57345045\n",
      "Trained batch 1048 batch loss 1.38016653 epoch total loss 1.57326591\n",
      "Trained batch 1049 batch loss 1.42844546 epoch total loss 1.57312787\n",
      "Trained batch 1050 batch loss 1.34925103 epoch total loss 1.57291472\n",
      "Trained batch 1051 batch loss 1.39293516 epoch total loss 1.57274342\n",
      "Trained batch 1052 batch loss 1.29180729 epoch total loss 1.57247639\n",
      "Trained batch 1053 batch loss 1.25979495 epoch total loss 1.57217932\n",
      "Trained batch 1054 batch loss 1.36649418 epoch total loss 1.57198417\n",
      "Trained batch 1055 batch loss 1.45992792 epoch total loss 1.57187796\n",
      "Trained batch 1056 batch loss 1.37587285 epoch total loss 1.57169235\n",
      "Trained batch 1057 batch loss 1.37298834 epoch total loss 1.57150447\n",
      "Trained batch 1058 batch loss 1.46092153 epoch total loss 1.57139993\n",
      "Trained batch 1059 batch loss 1.56811333 epoch total loss 1.57139683\n",
      "Trained batch 1060 batch loss 1.54293036 epoch total loss 1.57137\n",
      "Trained batch 1061 batch loss 1.54756594 epoch total loss 1.57134759\n",
      "Trained batch 1062 batch loss 1.50371766 epoch total loss 1.57128394\n",
      "Trained batch 1063 batch loss 1.45694757 epoch total loss 1.57117629\n",
      "Trained batch 1064 batch loss 1.50448871 epoch total loss 1.57111359\n",
      "Trained batch 1065 batch loss 1.45972645 epoch total loss 1.57100904\n",
      "Trained batch 1066 batch loss 1.48407269 epoch total loss 1.5709275\n",
      "Trained batch 1067 batch loss 1.47804785 epoch total loss 1.57084048\n",
      "Trained batch 1068 batch loss 1.5079478 epoch total loss 1.57078159\n",
      "Trained batch 1069 batch loss 1.55507505 epoch total loss 1.57076693\n",
      "Trained batch 1070 batch loss 1.46275711 epoch total loss 1.57066596\n",
      "Trained batch 1071 batch loss 1.4229027 epoch total loss 1.57052791\n",
      "Trained batch 1072 batch loss 1.43340075 epoch total loss 1.5704\n",
      "Trained batch 1073 batch loss 1.42015648 epoch total loss 1.57025993\n",
      "Trained batch 1074 batch loss 1.49108028 epoch total loss 1.57018626\n",
      "Trained batch 1075 batch loss 1.50508118 epoch total loss 1.5701257\n",
      "Trained batch 1076 batch loss 1.49376392 epoch total loss 1.57005477\n",
      "Trained batch 1077 batch loss 1.51664388 epoch total loss 1.57000506\n",
      "Trained batch 1078 batch loss 1.4792192 epoch total loss 1.5699209\n",
      "Trained batch 1079 batch loss 1.56568956 epoch total loss 1.56991696\n",
      "Trained batch 1080 batch loss 1.2723968 epoch total loss 1.56964147\n",
      "Trained batch 1081 batch loss 1.4248755 epoch total loss 1.5695076\n",
      "Trained batch 1082 batch loss 1.51269114 epoch total loss 1.56945503\n",
      "Trained batch 1083 batch loss 1.5150615 epoch total loss 1.56940484\n",
      "Trained batch 1084 batch loss 1.54867482 epoch total loss 1.56938577\n",
      "Trained batch 1085 batch loss 1.51854348 epoch total loss 1.56933892\n",
      "Trained batch 1086 batch loss 1.43879247 epoch total loss 1.56921875\n",
      "Trained batch 1087 batch loss 1.5339824 epoch total loss 1.56918621\n",
      "Trained batch 1088 batch loss 1.32054973 epoch total loss 1.56895769\n",
      "Trained batch 1089 batch loss 1.36812925 epoch total loss 1.56877339\n",
      "Trained batch 1090 batch loss 1.40186 epoch total loss 1.5686202\n",
      "Trained batch 1091 batch loss 1.40534306 epoch total loss 1.5684706\n",
      "Trained batch 1092 batch loss 1.5077635 epoch total loss 1.56841505\n",
      "Trained batch 1093 batch loss 1.48549724 epoch total loss 1.56833911\n",
      "Trained batch 1094 batch loss 1.52509117 epoch total loss 1.56829965\n",
      "Trained batch 1095 batch loss 1.44408441 epoch total loss 1.56818628\n",
      "Trained batch 1096 batch loss 1.43835878 epoch total loss 1.56806779\n",
      "Trained batch 1097 batch loss 1.42983556 epoch total loss 1.56794178\n",
      "Trained batch 1098 batch loss 1.46872413 epoch total loss 1.56785142\n",
      "Trained batch 1099 batch loss 1.3705 epoch total loss 1.56767178\n",
      "Trained batch 1100 batch loss 1.33977878 epoch total loss 1.56746459\n",
      "Trained batch 1101 batch loss 1.52954149 epoch total loss 1.56743014\n",
      "Trained batch 1102 batch loss 1.42529643 epoch total loss 1.56730115\n",
      "Trained batch 1103 batch loss 1.46840096 epoch total loss 1.56721151\n",
      "Trained batch 1104 batch loss 1.4530108 epoch total loss 1.56710804\n",
      "Trained batch 1105 batch loss 1.43574893 epoch total loss 1.56698918\n",
      "Trained batch 1106 batch loss 1.34865713 epoch total loss 1.56679177\n",
      "Trained batch 1107 batch loss 1.39961886 epoch total loss 1.56664085\n",
      "Trained batch 1108 batch loss 1.43628073 epoch total loss 1.56652319\n",
      "Trained batch 1109 batch loss 1.3814044 epoch total loss 1.56635618\n",
      "Trained batch 1110 batch loss 1.5001595 epoch total loss 1.56629646\n",
      "Trained batch 1111 batch loss 1.51926816 epoch total loss 1.56625414\n",
      "Trained batch 1112 batch loss 1.54806554 epoch total loss 1.56623781\n",
      "Trained batch 1113 batch loss 1.48494947 epoch total loss 1.56616485\n",
      "Trained batch 1114 batch loss 1.41382301 epoch total loss 1.56602812\n",
      "Trained batch 1115 batch loss 1.5071063 epoch total loss 1.56597519\n",
      "Trained batch 1116 batch loss 1.57402217 epoch total loss 1.56598234\n",
      "Trained batch 1117 batch loss 1.52370894 epoch total loss 1.56594455\n",
      "Trained batch 1118 batch loss 1.50879693 epoch total loss 1.56589341\n",
      "Trained batch 1119 batch loss 1.4301157 epoch total loss 1.56577218\n",
      "Trained batch 1120 batch loss 1.42789054 epoch total loss 1.56564903\n",
      "Trained batch 1121 batch loss 1.36319518 epoch total loss 1.56546831\n",
      "Trained batch 1122 batch loss 1.44150901 epoch total loss 1.56535792\n",
      "Trained batch 1123 batch loss 1.48282087 epoch total loss 1.56528437\n",
      "Trained batch 1124 batch loss 1.47614479 epoch total loss 1.5652051\n",
      "Trained batch 1125 batch loss 1.47819865 epoch total loss 1.56512773\n",
      "Trained batch 1126 batch loss 1.36126304 epoch total loss 1.56494665\n",
      "Trained batch 1127 batch loss 1.41000724 epoch total loss 1.5648092\n",
      "Trained batch 1128 batch loss 1.46024084 epoch total loss 1.56471646\n",
      "Trained batch 1129 batch loss 1.41934669 epoch total loss 1.56458759\n",
      "Trained batch 1130 batch loss 1.47893322 epoch total loss 1.56451178\n",
      "Trained batch 1131 batch loss 1.50168884 epoch total loss 1.56445622\n",
      "Trained batch 1132 batch loss 1.44475138 epoch total loss 1.56435049\n",
      "Trained batch 1133 batch loss 1.46598744 epoch total loss 1.56426358\n",
      "Trained batch 1134 batch loss 1.48263156 epoch total loss 1.5641917\n",
      "Trained batch 1135 batch loss 1.53787112 epoch total loss 1.56416845\n",
      "Trained batch 1136 batch loss 1.43983006 epoch total loss 1.56405902\n",
      "Trained batch 1137 batch loss 1.45723665 epoch total loss 1.56396508\n",
      "Trained batch 1138 batch loss 1.52501559 epoch total loss 1.56393087\n",
      "Trained batch 1139 batch loss 1.57251203 epoch total loss 1.56393838\n",
      "Trained batch 1140 batch loss 1.3958075 epoch total loss 1.5637908\n",
      "Trained batch 1141 batch loss 1.46980548 epoch total loss 1.56370854\n",
      "Trained batch 1142 batch loss 1.4859159 epoch total loss 1.56364048\n",
      "Trained batch 1143 batch loss 1.5520705 epoch total loss 1.56363034\n",
      "Trained batch 1144 batch loss 1.54015398 epoch total loss 1.56360984\n",
      "Trained batch 1145 batch loss 1.56763947 epoch total loss 1.5636133\n",
      "Trained batch 1146 batch loss 1.46717727 epoch total loss 1.56352913\n",
      "Trained batch 1147 batch loss 1.40198112 epoch total loss 1.56338835\n",
      "Trained batch 1148 batch loss 1.49202359 epoch total loss 1.56332624\n",
      "Trained batch 1149 batch loss 1.55673122 epoch total loss 1.56332052\n",
      "Trained batch 1150 batch loss 1.47559607 epoch total loss 1.56324422\n",
      "Trained batch 1151 batch loss 1.60398018 epoch total loss 1.56327963\n",
      "Trained batch 1152 batch loss 1.59126151 epoch total loss 1.56330395\n",
      "Trained batch 1153 batch loss 1.62317526 epoch total loss 1.56335592\n",
      "Trained batch 1154 batch loss 1.61934781 epoch total loss 1.56340444\n",
      "Trained batch 1155 batch loss 1.55550075 epoch total loss 1.56339765\n",
      "Trained batch 1156 batch loss 1.44312179 epoch total loss 1.56329358\n",
      "Trained batch 1157 batch loss 1.43918765 epoch total loss 1.56318629\n",
      "Trained batch 1158 batch loss 1.42469525 epoch total loss 1.56306672\n",
      "Trained batch 1159 batch loss 1.52033782 epoch total loss 1.56302989\n",
      "Trained batch 1160 batch loss 1.51739359 epoch total loss 1.56299055\n",
      "Trained batch 1161 batch loss 1.4371767 epoch total loss 1.56288207\n",
      "Trained batch 1162 batch loss 1.38324046 epoch total loss 1.56272757\n",
      "Trained batch 1163 batch loss 1.38886571 epoch total loss 1.56257808\n",
      "Trained batch 1164 batch loss 1.48366106 epoch total loss 1.56251025\n",
      "Trained batch 1165 batch loss 1.45458376 epoch total loss 1.56241763\n",
      "Trained batch 1166 batch loss 1.68115103 epoch total loss 1.56251943\n",
      "Trained batch 1167 batch loss 1.69646978 epoch total loss 1.56263423\n",
      "Trained batch 1168 batch loss 1.52987242 epoch total loss 1.56260622\n",
      "Trained batch 1169 batch loss 1.58655822 epoch total loss 1.56262672\n",
      "Trained batch 1170 batch loss 1.60392261 epoch total loss 1.56266189\n",
      "Trained batch 1171 batch loss 1.57880509 epoch total loss 1.56267571\n",
      "Trained batch 1172 batch loss 1.24484789 epoch total loss 1.56240463\n",
      "Trained batch 1173 batch loss 1.34196925 epoch total loss 1.56221664\n",
      "Trained batch 1174 batch loss 1.50564897 epoch total loss 1.56216836\n",
      "Trained batch 1175 batch loss 1.505651 epoch total loss 1.56212032\n",
      "Trained batch 1176 batch loss 1.49066508 epoch total loss 1.56205952\n",
      "Trained batch 1177 batch loss 1.55381095 epoch total loss 1.56205261\n",
      "Trained batch 1178 batch loss 1.5604049 epoch total loss 1.56205118\n",
      "Trained batch 1179 batch loss 1.53243876 epoch total loss 1.56202614\n",
      "Trained batch 1180 batch loss 1.49179327 epoch total loss 1.56196666\n",
      "Trained batch 1181 batch loss 1.60126352 epoch total loss 1.56199992\n",
      "Trained batch 1182 batch loss 1.57648253 epoch total loss 1.5620122\n",
      "Trained batch 1183 batch loss 1.50076187 epoch total loss 1.56196046\n",
      "Trained batch 1184 batch loss 1.44007015 epoch total loss 1.56185746\n",
      "Trained batch 1185 batch loss 1.42266321 epoch total loss 1.56173992\n",
      "Trained batch 1186 batch loss 1.34710741 epoch total loss 1.56155908\n",
      "Trained batch 1187 batch loss 1.41824508 epoch total loss 1.56143832\n",
      "Trained batch 1188 batch loss 1.43893552 epoch total loss 1.56133521\n",
      "Trained batch 1189 batch loss 1.45833278 epoch total loss 1.56124854\n",
      "Trained batch 1190 batch loss 1.44325566 epoch total loss 1.56114948\n",
      "Trained batch 1191 batch loss 1.63292921 epoch total loss 1.56120968\n",
      "Trained batch 1192 batch loss 1.46513867 epoch total loss 1.56112909\n",
      "Trained batch 1193 batch loss 1.65428913 epoch total loss 1.56120718\n",
      "Trained batch 1194 batch loss 1.71332693 epoch total loss 1.56133461\n",
      "Trained batch 1195 batch loss 1.52572513 epoch total loss 1.56130481\n",
      "Trained batch 1196 batch loss 1.42128301 epoch total loss 1.56118774\n",
      "Trained batch 1197 batch loss 1.32890916 epoch total loss 1.56099367\n",
      "Trained batch 1198 batch loss 1.37563682 epoch total loss 1.56083894\n",
      "Trained batch 1199 batch loss 1.46907735 epoch total loss 1.56076241\n",
      "Trained batch 1200 batch loss 1.38356781 epoch total loss 1.56061471\n",
      "Trained batch 1201 batch loss 1.44530296 epoch total loss 1.56051874\n",
      "Trained batch 1202 batch loss 1.41717923 epoch total loss 1.56039953\n",
      "Trained batch 1203 batch loss 1.56853986 epoch total loss 1.56040621\n",
      "Trained batch 1204 batch loss 1.52105546 epoch total loss 1.56037354\n",
      "Trained batch 1205 batch loss 1.58697236 epoch total loss 1.56039548\n",
      "Trained batch 1206 batch loss 1.41873956 epoch total loss 1.56027806\n",
      "Trained batch 1207 batch loss 1.41684902 epoch total loss 1.56015921\n",
      "Trained batch 1208 batch loss 1.40848875 epoch total loss 1.56003368\n",
      "Trained batch 1209 batch loss 1.4494319 epoch total loss 1.55994213\n",
      "Trained batch 1210 batch loss 1.50651634 epoch total loss 1.55989802\n",
      "Trained batch 1211 batch loss 1.36966705 epoch total loss 1.5597409\n",
      "Trained batch 1212 batch loss 1.40106142 epoch total loss 1.55960989\n",
      "Trained batch 1213 batch loss 1.31548345 epoch total loss 1.55940855\n",
      "Trained batch 1214 batch loss 1.38451946 epoch total loss 1.55926454\n",
      "Trained batch 1215 batch loss 1.41258812 epoch total loss 1.55914378\n",
      "Trained batch 1216 batch loss 1.35520732 epoch total loss 1.55897617\n",
      "Trained batch 1217 batch loss 1.3261615 epoch total loss 1.55878484\n",
      "Trained batch 1218 batch loss 1.42385721 epoch total loss 1.55867398\n",
      "Trained batch 1219 batch loss 1.50294578 epoch total loss 1.55862832\n",
      "Trained batch 1220 batch loss 1.34363174 epoch total loss 1.55845201\n",
      "Trained batch 1221 batch loss 1.31601167 epoch total loss 1.55825353\n",
      "Trained batch 1222 batch loss 1.36157858 epoch total loss 1.55809259\n",
      "Trained batch 1223 batch loss 1.42352653 epoch total loss 1.55798256\n",
      "Trained batch 1224 batch loss 1.43075633 epoch total loss 1.55787873\n",
      "Trained batch 1225 batch loss 1.48433089 epoch total loss 1.55781865\n",
      "Trained batch 1226 batch loss 1.38861191 epoch total loss 1.55768073\n",
      "Trained batch 1227 batch loss 1.40169501 epoch total loss 1.55755365\n",
      "Trained batch 1228 batch loss 1.4485147 epoch total loss 1.55746484\n",
      "Trained batch 1229 batch loss 1.41400361 epoch total loss 1.55734813\n",
      "Trained batch 1230 batch loss 1.48009682 epoch total loss 1.55728531\n",
      "Trained batch 1231 batch loss 1.48578894 epoch total loss 1.55722725\n",
      "Trained batch 1232 batch loss 1.39319956 epoch total loss 1.5570941\n",
      "Trained batch 1233 batch loss 1.37439871 epoch total loss 1.55694592\n",
      "Trained batch 1234 batch loss 1.40162396 epoch total loss 1.55682\n",
      "Trained batch 1235 batch loss 1.40618813 epoch total loss 1.55669808\n",
      "Trained batch 1236 batch loss 1.3137629 epoch total loss 1.55650151\n",
      "Trained batch 1237 batch loss 1.48808539 epoch total loss 1.55644608\n",
      "Trained batch 1238 batch loss 1.45250297 epoch total loss 1.55636215\n",
      "Trained batch 1239 batch loss 1.56732917 epoch total loss 1.55637109\n",
      "Trained batch 1240 batch loss 1.47023547 epoch total loss 1.55630159\n",
      "Trained batch 1241 batch loss 1.69861269 epoch total loss 1.55641627\n",
      "Trained batch 1242 batch loss 1.60722208 epoch total loss 1.55645716\n",
      "Trained batch 1243 batch loss 1.41907907 epoch total loss 1.55634665\n",
      "Trained batch 1244 batch loss 1.41631782 epoch total loss 1.556234\n",
      "Trained batch 1245 batch loss 1.32111239 epoch total loss 1.55604517\n",
      "Trained batch 1246 batch loss 1.22239745 epoch total loss 1.55577743\n",
      "Trained batch 1247 batch loss 1.32401252 epoch total loss 1.55559158\n",
      "Trained batch 1248 batch loss 1.42122972 epoch total loss 1.55548394\n",
      "Trained batch 1249 batch loss 1.20447683 epoch total loss 1.55520284\n",
      "Trained batch 1250 batch loss 1.24759984 epoch total loss 1.55495679\n",
      "Trained batch 1251 batch loss 1.15212488 epoch total loss 1.55463469\n",
      "Trained batch 1252 batch loss 1.30561233 epoch total loss 1.55443585\n",
      "Trained batch 1253 batch loss 1.36902547 epoch total loss 1.55428791\n",
      "Trained batch 1254 batch loss 1.48517501 epoch total loss 1.55423284\n",
      "Trained batch 1255 batch loss 1.48974 epoch total loss 1.55418146\n",
      "Trained batch 1256 batch loss 1.51022184 epoch total loss 1.55414641\n",
      "Trained batch 1257 batch loss 1.57038736 epoch total loss 1.5541594\n",
      "Trained batch 1258 batch loss 1.50422037 epoch total loss 1.55411971\n",
      "Trained batch 1259 batch loss 1.5120163 epoch total loss 1.55408621\n",
      "Trained batch 1260 batch loss 1.5068922 epoch total loss 1.55404878\n",
      "Trained batch 1261 batch loss 1.41143155 epoch total loss 1.55393565\n",
      "Trained batch 1262 batch loss 1.40527821 epoch total loss 1.55381787\n",
      "Trained batch 1263 batch loss 1.35119879 epoch total loss 1.55365741\n",
      "Trained batch 1264 batch loss 1.40978086 epoch total loss 1.55354357\n",
      "Trained batch 1265 batch loss 1.46224284 epoch total loss 1.55347145\n",
      "Trained batch 1266 batch loss 1.49424458 epoch total loss 1.55342472\n",
      "Trained batch 1267 batch loss 1.66191041 epoch total loss 1.55351031\n",
      "Trained batch 1268 batch loss 1.61263013 epoch total loss 1.55355692\n",
      "Trained batch 1269 batch loss 1.49923468 epoch total loss 1.55351412\n",
      "Trained batch 1270 batch loss 1.53480589 epoch total loss 1.55349934\n",
      "Trained batch 1271 batch loss 1.44646657 epoch total loss 1.55341518\n",
      "Trained batch 1272 batch loss 1.37989 epoch total loss 1.55327868\n",
      "Trained batch 1273 batch loss 1.3420366 epoch total loss 1.55311275\n",
      "Trained batch 1274 batch loss 1.53055525 epoch total loss 1.55309498\n",
      "Trained batch 1275 batch loss 1.55399811 epoch total loss 1.5530957\n",
      "Trained batch 1276 batch loss 1.53964972 epoch total loss 1.55308521\n",
      "Trained batch 1277 batch loss 1.41826117 epoch total loss 1.55297959\n",
      "Trained batch 1278 batch loss 1.48098409 epoch total loss 1.5529232\n",
      "Trained batch 1279 batch loss 1.48278713 epoch total loss 1.55286837\n",
      "Trained batch 1280 batch loss 1.47906303 epoch total loss 1.55281067\n",
      "Trained batch 1281 batch loss 1.47991323 epoch total loss 1.55275369\n",
      "Trained batch 1282 batch loss 1.38783622 epoch total loss 1.55262506\n",
      "Trained batch 1283 batch loss 1.40276718 epoch total loss 1.55250823\n",
      "Trained batch 1284 batch loss 1.45523369 epoch total loss 1.55243242\n",
      "Trained batch 1285 batch loss 1.43641877 epoch total loss 1.55234218\n",
      "Trained batch 1286 batch loss 1.4687593 epoch total loss 1.55227709\n",
      "Trained batch 1287 batch loss 1.40276122 epoch total loss 1.55216098\n",
      "Trained batch 1288 batch loss 1.53070331 epoch total loss 1.55214429\n",
      "Trained batch 1289 batch loss 1.57893121 epoch total loss 1.55216515\n",
      "Trained batch 1290 batch loss 1.49396229 epoch total loss 1.55212009\n",
      "Trained batch 1291 batch loss 1.41062856 epoch total loss 1.55201042\n",
      "Trained batch 1292 batch loss 1.50601399 epoch total loss 1.55197489\n",
      "Trained batch 1293 batch loss 1.36107981 epoch total loss 1.55182719\n",
      "Trained batch 1294 batch loss 1.40638971 epoch total loss 1.55171478\n",
      "Trained batch 1295 batch loss 1.4553529 epoch total loss 1.55164039\n",
      "Trained batch 1296 batch loss 1.46269059 epoch total loss 1.55157173\n",
      "Trained batch 1297 batch loss 1.39575505 epoch total loss 1.55145156\n",
      "Trained batch 1298 batch loss 1.44026804 epoch total loss 1.55136597\n",
      "Trained batch 1299 batch loss 1.4754231 epoch total loss 1.55130744\n",
      "Trained batch 1300 batch loss 1.4099406 epoch total loss 1.55119872\n",
      "Trained batch 1301 batch loss 1.41822 epoch total loss 1.55109656\n",
      "Trained batch 1302 batch loss 1.4459796 epoch total loss 1.55101573\n",
      "Trained batch 1303 batch loss 1.42395043 epoch total loss 1.55091822\n",
      "Trained batch 1304 batch loss 1.42831278 epoch total loss 1.55082417\n",
      "Trained batch 1305 batch loss 1.44056761 epoch total loss 1.55073977\n",
      "Trained batch 1306 batch loss 1.41950941 epoch total loss 1.55063927\n",
      "Trained batch 1307 batch loss 1.34431314 epoch total loss 1.55048144\n",
      "Trained batch 1308 batch loss 1.411484 epoch total loss 1.55037522\n",
      "Trained batch 1309 batch loss 1.4191432 epoch total loss 1.55027497\n",
      "Trained batch 1310 batch loss 1.35308647 epoch total loss 1.55012441\n",
      "Trained batch 1311 batch loss 1.36422408 epoch total loss 1.54998267\n",
      "Trained batch 1312 batch loss 1.32981074 epoch total loss 1.54981482\n",
      "Trained batch 1313 batch loss 1.43487048 epoch total loss 1.5497272\n",
      "Trained batch 1314 batch loss 1.32882249 epoch total loss 1.54955912\n",
      "Trained batch 1315 batch loss 1.374264 epoch total loss 1.54942584\n",
      "Trained batch 1316 batch loss 1.34526467 epoch total loss 1.54927063\n",
      "Trained batch 1317 batch loss 1.44877386 epoch total loss 1.54919434\n",
      "Trained batch 1318 batch loss 1.41292822 epoch total loss 1.54909098\n",
      "Trained batch 1319 batch loss 1.59324884 epoch total loss 1.54912448\n",
      "Trained batch 1320 batch loss 1.60099828 epoch total loss 1.5491637\n",
      "Trained batch 1321 batch loss 1.4891094 epoch total loss 1.54911828\n",
      "Trained batch 1322 batch loss 1.53254879 epoch total loss 1.54910576\n",
      "Trained batch 1323 batch loss 1.30548477 epoch total loss 1.5489217\n",
      "Trained batch 1324 batch loss 1.35637069 epoch total loss 1.54877627\n",
      "Trained batch 1325 batch loss 1.54279804 epoch total loss 1.54877174\n",
      "Trained batch 1326 batch loss 1.55429375 epoch total loss 1.54877579\n",
      "Trained batch 1327 batch loss 1.46133661 epoch total loss 1.54871\n",
      "Trained batch 1328 batch loss 1.524876 epoch total loss 1.54869211\n",
      "Trained batch 1329 batch loss 1.42733788 epoch total loss 1.54860067\n",
      "Trained batch 1330 batch loss 1.40728021 epoch total loss 1.54849446\n",
      "Trained batch 1331 batch loss 1.43296599 epoch total loss 1.54840755\n",
      "Trained batch 1332 batch loss 1.44645834 epoch total loss 1.54833102\n",
      "Trained batch 1333 batch loss 1.45034242 epoch total loss 1.54825759\n",
      "Trained batch 1334 batch loss 1.42835283 epoch total loss 1.54816782\n",
      "Trained batch 1335 batch loss 1.49672484 epoch total loss 1.54812932\n",
      "Trained batch 1336 batch loss 1.37849832 epoch total loss 1.54800236\n",
      "Trained batch 1337 batch loss 1.43253744 epoch total loss 1.54791605\n",
      "Trained batch 1338 batch loss 1.20431769 epoch total loss 1.54765928\n",
      "Trained batch 1339 batch loss 1.29754758 epoch total loss 1.54747248\n",
      "Trained batch 1340 batch loss 1.2599628 epoch total loss 1.54725802\n",
      "Trained batch 1341 batch loss 1.45412505 epoch total loss 1.54718852\n",
      "Trained batch 1342 batch loss 1.46148896 epoch total loss 1.54712462\n",
      "Trained batch 1343 batch loss 1.55323887 epoch total loss 1.54712915\n",
      "Trained batch 1344 batch loss 1.68202353 epoch total loss 1.54722953\n",
      "Trained batch 1345 batch loss 1.53920686 epoch total loss 1.54722369\n",
      "Trained batch 1346 batch loss 1.45222795 epoch total loss 1.54715312\n",
      "Trained batch 1347 batch loss 1.46132207 epoch total loss 1.54708946\n",
      "Trained batch 1348 batch loss 1.44436383 epoch total loss 1.54701316\n",
      "Trained batch 1349 batch loss 1.49886703 epoch total loss 1.5469774\n",
      "Trained batch 1350 batch loss 1.52137601 epoch total loss 1.54695857\n",
      "Trained batch 1351 batch loss 1.48636889 epoch total loss 1.54691362\n",
      "Trained batch 1352 batch loss 1.390728 epoch total loss 1.54679811\n",
      "Trained batch 1353 batch loss 1.52534783 epoch total loss 1.54678226\n",
      "Trained batch 1354 batch loss 1.47932482 epoch total loss 1.54673243\n",
      "Trained batch 1355 batch loss 1.572263 epoch total loss 1.54675126\n",
      "Trained batch 1356 batch loss 1.41179156 epoch total loss 1.54665172\n",
      "Trained batch 1357 batch loss 1.35040498 epoch total loss 1.54650712\n",
      "Trained batch 1358 batch loss 1.36805093 epoch total loss 1.54637575\n",
      "Trained batch 1359 batch loss 1.39254618 epoch total loss 1.54626262\n",
      "Trained batch 1360 batch loss 1.42680991 epoch total loss 1.54617476\n",
      "Trained batch 1361 batch loss 1.45364952 epoch total loss 1.5461067\n",
      "Trained batch 1362 batch loss 1.36430871 epoch total loss 1.54597318\n",
      "Trained batch 1363 batch loss 1.44045019 epoch total loss 1.5458957\n",
      "Trained batch 1364 batch loss 1.34230018 epoch total loss 1.54574645\n",
      "Trained batch 1365 batch loss 1.34690511 epoch total loss 1.54560077\n",
      "Trained batch 1366 batch loss 1.44834244 epoch total loss 1.5455296\n",
      "Trained batch 1367 batch loss 1.4325316 epoch total loss 1.54544699\n",
      "Trained batch 1368 batch loss 1.39831471 epoch total loss 1.54533935\n",
      "Trained batch 1369 batch loss 1.47065425 epoch total loss 1.54528475\n",
      "Trained batch 1370 batch loss 1.40238369 epoch total loss 1.54518044\n",
      "Trained batch 1371 batch loss 1.3010366 epoch total loss 1.54500234\n",
      "Trained batch 1372 batch loss 1.38976252 epoch total loss 1.54488909\n",
      "Trained batch 1373 batch loss 1.344769 epoch total loss 1.5447433\n",
      "Trained batch 1374 batch loss 1.45187211 epoch total loss 1.54467583\n",
      "Trained batch 1375 batch loss 1.41029418 epoch total loss 1.54457808\n",
      "Trained batch 1376 batch loss 1.34924293 epoch total loss 1.5444361\n",
      "Trained batch 1377 batch loss 1.41893554 epoch total loss 1.5443449\n",
      "Trained batch 1378 batch loss 1.42608571 epoch total loss 1.54425907\n",
      "Trained batch 1379 batch loss 1.47954857 epoch total loss 1.5442121\n",
      "Trained batch 1380 batch loss 1.33730602 epoch total loss 1.54406226\n",
      "Trained batch 1381 batch loss 1.37496567 epoch total loss 1.54393983\n",
      "Trained batch 1382 batch loss 1.3915143 epoch total loss 1.54382956\n",
      "Trained batch 1383 batch loss 1.41513956 epoch total loss 1.54373646\n",
      "Trained batch 1384 batch loss 1.51807046 epoch total loss 1.54371798\n",
      "Trained batch 1385 batch loss 1.40829921 epoch total loss 1.54362011\n",
      "Trained batch 1386 batch loss 1.39216352 epoch total loss 1.54351079\n",
      "Trained batch 1387 batch loss 1.35438263 epoch total loss 1.54337442\n",
      "Trained batch 1388 batch loss 1.46074879 epoch total loss 1.54331493\n",
      "Epoch 1 train loss 1.5433149337768555\n",
      "Validated batch 1 batch loss 1.35935104\n",
      "Validated batch 2 batch loss 1.44532204\n",
      "Validated batch 3 batch loss 1.31352663\n",
      "Validated batch 4 batch loss 1.32049239\n",
      "Validated batch 5 batch loss 1.34266\n",
      "Validated batch 6 batch loss 1.44122887\n",
      "Validated batch 7 batch loss 1.42325735\n",
      "Validated batch 8 batch loss 1.31682396\n",
      "Validated batch 9 batch loss 1.38149142\n",
      "Validated batch 10 batch loss 1.42490494\n",
      "Validated batch 11 batch loss 1.35056853\n",
      "Validated batch 12 batch loss 1.40177369\n",
      "Validated batch 13 batch loss 1.47166038\n",
      "Validated batch 14 batch loss 1.37706268\n",
      "Validated batch 15 batch loss 1.47794175\n",
      "Validated batch 16 batch loss 1.47916317\n",
      "Validated batch 17 batch loss 1.40856171\n",
      "Validated batch 18 batch loss 1.48443747\n",
      "Validated batch 19 batch loss 1.29253829\n",
      "Validated batch 20 batch loss 1.38134646\n",
      "Validated batch 21 batch loss 1.34754038\n",
      "Validated batch 22 batch loss 1.46496058\n",
      "Validated batch 23 batch loss 1.52770424\n",
      "Validated batch 24 batch loss 1.48232102\n",
      "Validated batch 25 batch loss 1.46167803\n",
      "Validated batch 26 batch loss 1.38486648\n",
      "Validated batch 27 batch loss 1.33439124\n",
      "Validated batch 28 batch loss 1.39128649\n",
      "Validated batch 29 batch loss 1.44346809\n",
      "Validated batch 30 batch loss 1.34266305\n",
      "Validated batch 31 batch loss 1.38515902\n",
      "Validated batch 32 batch loss 1.41222179\n",
      "Validated batch 33 batch loss 1.4366672\n",
      "Validated batch 34 batch loss 1.34450316\n",
      "Validated batch 35 batch loss 1.31807041\n",
      "Validated batch 36 batch loss 1.42430449\n",
      "Validated batch 37 batch loss 1.42770922\n",
      "Validated batch 38 batch loss 1.50342679\n",
      "Validated batch 39 batch loss 1.48254919\n",
      "Validated batch 40 batch loss 1.3700918\n",
      "Validated batch 41 batch loss 1.51916552\n",
      "Validated batch 42 batch loss 1.39881754\n",
      "Validated batch 43 batch loss 1.41991949\n",
      "Validated batch 44 batch loss 1.47472262\n",
      "Validated batch 45 batch loss 1.20986581\n",
      "Validated batch 46 batch loss 1.47102761\n",
      "Validated batch 47 batch loss 1.4583869\n",
      "Validated batch 48 batch loss 1.37490296\n",
      "Validated batch 49 batch loss 1.33916306\n",
      "Validated batch 50 batch loss 1.34817362\n",
      "Validated batch 51 batch loss 1.38195038\n",
      "Validated batch 52 batch loss 1.47612679\n",
      "Validated batch 53 batch loss 1.31347096\n",
      "Validated batch 54 batch loss 1.41309929\n",
      "Validated batch 55 batch loss 1.39947557\n",
      "Validated batch 56 batch loss 1.4050107\n",
      "Validated batch 57 batch loss 1.39416802\n",
      "Validated batch 58 batch loss 1.30057693\n",
      "Validated batch 59 batch loss 1.54191875\n",
      "Validated batch 60 batch loss 1.35810077\n",
      "Validated batch 61 batch loss 1.4453764\n",
      "Validated batch 62 batch loss 1.3738811\n",
      "Validated batch 63 batch loss 1.45064521\n",
      "Validated batch 64 batch loss 1.29061067\n",
      "Validated batch 65 batch loss 1.3793571\n",
      "Validated batch 66 batch loss 1.37235117\n",
      "Validated batch 67 batch loss 1.35573554\n",
      "Validated batch 68 batch loss 1.41005158\n",
      "Validated batch 69 batch loss 1.42270517\n",
      "Validated batch 70 batch loss 1.43326271\n",
      "Validated batch 71 batch loss 1.44139206\n",
      "Validated batch 72 batch loss 1.30211186\n",
      "Validated batch 73 batch loss 1.45181251\n",
      "Validated batch 74 batch loss 1.44365048\n",
      "Validated batch 75 batch loss 1.42308903\n",
      "Validated batch 76 batch loss 1.45464993\n",
      "Validated batch 77 batch loss 1.48503411\n",
      "Validated batch 78 batch loss 1.47176039\n",
      "Validated batch 79 batch loss 1.45855141\n",
      "Validated batch 80 batch loss 1.50673914\n",
      "Validated batch 81 batch loss 1.44692886\n",
      "Validated batch 82 batch loss 1.37577951\n",
      "Validated batch 83 batch loss 1.51492596\n",
      "Validated batch 84 batch loss 1.42970252\n",
      "Validated batch 85 batch loss 1.43676233\n",
      "Validated batch 86 batch loss 1.5473156\n",
      "Validated batch 87 batch loss 1.24339962\n",
      "Validated batch 88 batch loss 1.39854503\n",
      "Validated batch 89 batch loss 1.34484196\n",
      "Validated batch 90 batch loss 1.38355303\n",
      "Validated batch 91 batch loss 1.54482222\n",
      "Validated batch 92 batch loss 1.34862375\n",
      "Validated batch 93 batch loss 1.37543535\n",
      "Validated batch 94 batch loss 1.33722579\n",
      "Validated batch 95 batch loss 1.40197897\n",
      "Validated batch 96 batch loss 1.36337841\n",
      "Validated batch 97 batch loss 1.37556767\n",
      "Validated batch 98 batch loss 1.52236593\n",
      "Validated batch 99 batch loss 1.42908096\n",
      "Validated batch 100 batch loss 1.45368803\n",
      "Validated batch 101 batch loss 1.49171126\n",
      "Validated batch 102 batch loss 1.4622035\n",
      "Validated batch 103 batch loss 1.40716517\n",
      "Validated batch 104 batch loss 1.4828856\n",
      "Validated batch 105 batch loss 1.42644608\n",
      "Validated batch 106 batch loss 1.48886955\n",
      "Validated batch 107 batch loss 1.48664296\n",
      "Validated batch 108 batch loss 1.49322963\n",
      "Validated batch 109 batch loss 1.46166742\n",
      "Validated batch 110 batch loss 1.29434288\n",
      "Validated batch 111 batch loss 1.41353321\n",
      "Validated batch 112 batch loss 1.43226814\n",
      "Validated batch 113 batch loss 1.46432042\n",
      "Validated batch 114 batch loss 1.41161692\n",
      "Validated batch 115 batch loss 1.37039828\n",
      "Validated batch 116 batch loss 1.41675305\n",
      "Validated batch 117 batch loss 1.31687617\n",
      "Validated batch 118 batch loss 1.40834486\n",
      "Validated batch 119 batch loss 1.33176613\n",
      "Validated batch 120 batch loss 1.38564253\n",
      "Validated batch 121 batch loss 1.40994763\n",
      "Validated batch 122 batch loss 1.38945973\n",
      "Validated batch 123 batch loss 1.43267834\n",
      "Validated batch 124 batch loss 1.43366933\n",
      "Validated batch 125 batch loss 1.38514042\n",
      "Validated batch 126 batch loss 1.53594029\n",
      "Validated batch 127 batch loss 1.49881804\n",
      "Validated batch 128 batch loss 1.321576\n",
      "Validated batch 129 batch loss 1.39408088\n",
      "Validated batch 130 batch loss 1.43323994\n",
      "Validated batch 131 batch loss 1.42435837\n",
      "Validated batch 132 batch loss 1.50070119\n",
      "Validated batch 133 batch loss 1.4273535\n",
      "Validated batch 134 batch loss 1.42554522\n",
      "Validated batch 135 batch loss 1.43092644\n",
      "Validated batch 136 batch loss 1.36922216\n",
      "Validated batch 137 batch loss 1.44513535\n",
      "Validated batch 138 batch loss 1.41762924\n",
      "Validated batch 139 batch loss 1.36592865\n",
      "Validated batch 140 batch loss 1.45169711\n",
      "Validated batch 141 batch loss 1.46926022\n",
      "Validated batch 142 batch loss 1.39168811\n",
      "Validated batch 143 batch loss 1.44197857\n",
      "Validated batch 144 batch loss 1.55451989\n",
      "Validated batch 145 batch loss 1.29271674\n",
      "Validated batch 146 batch loss 1.47059584\n",
      "Validated batch 147 batch loss 1.39616346\n",
      "Validated batch 148 batch loss 1.41618109\n",
      "Validated batch 149 batch loss 1.4309814\n",
      "Validated batch 150 batch loss 1.38726974\n",
      "Validated batch 151 batch loss 1.19251871\n",
      "Validated batch 152 batch loss 1.41631305\n",
      "Validated batch 153 batch loss 1.41619194\n",
      "Validated batch 154 batch loss 1.42304802\n",
      "Validated batch 155 batch loss 1.43305874\n",
      "Validated batch 156 batch loss 1.35002685\n",
      "Validated batch 157 batch loss 1.45802474\n",
      "Validated batch 158 batch loss 1.4728806\n",
      "Validated batch 159 batch loss 1.42341042\n",
      "Validated batch 160 batch loss 1.40067339\n",
      "Validated batch 161 batch loss 1.34384537\n",
      "Validated batch 162 batch loss 1.48448539\n",
      "Validated batch 163 batch loss 1.41331518\n",
      "Validated batch 164 batch loss 1.44670963\n",
      "Validated batch 165 batch loss 1.4045372\n",
      "Validated batch 166 batch loss 1.34524941\n",
      "Validated batch 167 batch loss 1.43515837\n",
      "Validated batch 168 batch loss 1.42126203\n",
      "Validated batch 169 batch loss 1.34751534\n",
      "Validated batch 170 batch loss 1.32478678\n",
      "Validated batch 171 batch loss 1.41395319\n",
      "Validated batch 172 batch loss 1.40318024\n",
      "Validated batch 173 batch loss 1.44842124\n",
      "Validated batch 174 batch loss 1.40102\n",
      "Validated batch 175 batch loss 1.3367877\n",
      "Validated batch 176 batch loss 1.40478551\n",
      "Validated batch 177 batch loss 1.39254987\n",
      "Validated batch 178 batch loss 1.39928043\n",
      "Validated batch 179 batch loss 1.42040992\n",
      "Validated batch 180 batch loss 1.50223196\n",
      "Validated batch 181 batch loss 1.60398698\n",
      "Validated batch 182 batch loss 1.5733875\n",
      "Validated batch 183 batch loss 1.44855595\n",
      "Validated batch 184 batch loss 1.3241477\n",
      "Validated batch 185 batch loss 1.32109284\n",
      "Epoch 1 val loss 1.4127694368362427\n",
      "Model /aiffel/aiffel/mpii/mine/model-epoch-1-loss-1.4128.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.52332819 epoch total loss 1.52332819\n",
      "Trained batch 2 batch loss 1.41267121 epoch total loss 1.4679997\n",
      "Trained batch 3 batch loss 1.46545863 epoch total loss 1.4671526\n",
      "Trained batch 4 batch loss 1.39518738 epoch total loss 1.44916129\n",
      "Trained batch 5 batch loss 1.45778656 epoch total loss 1.45088637\n",
      "Trained batch 6 batch loss 1.54416347 epoch total loss 1.46643257\n",
      "Trained batch 7 batch loss 1.41778159 epoch total loss 1.45948243\n",
      "Trained batch 8 batch loss 1.42377949 epoch total loss 1.45501959\n",
      "Trained batch 9 batch loss 1.3871913 epoch total loss 1.44748306\n",
      "Trained batch 10 batch loss 1.43773365 epoch total loss 1.44650817\n",
      "Trained batch 11 batch loss 1.49046183 epoch total loss 1.45050395\n",
      "Trained batch 12 batch loss 1.54439807 epoch total loss 1.45832837\n",
      "Trained batch 13 batch loss 1.50328493 epoch total loss 1.46178651\n",
      "Trained batch 14 batch loss 1.51047981 epoch total loss 1.46526456\n",
      "Trained batch 15 batch loss 1.51538348 epoch total loss 1.46860576\n",
      "Trained batch 16 batch loss 1.54970932 epoch total loss 1.47367477\n",
      "Trained batch 17 batch loss 1.47964096 epoch total loss 1.47402573\n",
      "Trained batch 18 batch loss 1.37459779 epoch total loss 1.46850193\n",
      "Trained batch 19 batch loss 1.44277871 epoch total loss 1.46714818\n",
      "Trained batch 20 batch loss 1.43840182 epoch total loss 1.46571088\n",
      "Trained batch 21 batch loss 1.43667161 epoch total loss 1.46432805\n",
      "Trained batch 22 batch loss 1.34350383 epoch total loss 1.45883596\n",
      "Trained batch 23 batch loss 1.31642044 epoch total loss 1.45264399\n",
      "Trained batch 24 batch loss 1.38503969 epoch total loss 1.44982719\n",
      "Trained batch 25 batch loss 1.35930884 epoch total loss 1.44620657\n",
      "Trained batch 26 batch loss 1.48905206 epoch total loss 1.4478544\n",
      "Trained batch 27 batch loss 1.48434949 epoch total loss 1.44920599\n",
      "Trained batch 28 batch loss 1.51707721 epoch total loss 1.45163\n",
      "Trained batch 29 batch loss 1.49864721 epoch total loss 1.45325124\n",
      "Trained batch 30 batch loss 1.57778013 epoch total loss 1.45740235\n",
      "Trained batch 31 batch loss 1.49265957 epoch total loss 1.45853961\n",
      "Trained batch 32 batch loss 1.53627956 epoch total loss 1.46096897\n",
      "Trained batch 33 batch loss 1.42516613 epoch total loss 1.45988405\n",
      "Trained batch 34 batch loss 1.37988293 epoch total loss 1.45753109\n",
      "Trained batch 35 batch loss 1.48758268 epoch total loss 1.45838976\n",
      "Trained batch 36 batch loss 1.50409675 epoch total loss 1.45965934\n",
      "Trained batch 37 batch loss 1.37739944 epoch total loss 1.45743608\n",
      "Trained batch 38 batch loss 1.38091767 epoch total loss 1.4554224\n",
      "Trained batch 39 batch loss 1.34324884 epoch total loss 1.45254624\n",
      "Trained batch 40 batch loss 1.35193992 epoch total loss 1.45003104\n",
      "Trained batch 41 batch loss 1.36306453 epoch total loss 1.44791\n",
      "Trained batch 42 batch loss 1.43040991 epoch total loss 1.4474932\n",
      "Trained batch 43 batch loss 1.38740873 epoch total loss 1.44609594\n",
      "Trained batch 44 batch loss 1.25487423 epoch total loss 1.44175\n",
      "Trained batch 45 batch loss 1.25046492 epoch total loss 1.43749917\n",
      "Trained batch 46 batch loss 1.31852913 epoch total loss 1.4349128\n",
      "Trained batch 47 batch loss 1.34802544 epoch total loss 1.4330641\n",
      "Trained batch 48 batch loss 1.41838741 epoch total loss 1.43275833\n",
      "Trained batch 49 batch loss 1.3722533 epoch total loss 1.43152356\n",
      "Trained batch 50 batch loss 1.40788674 epoch total loss 1.4310509\n",
      "Trained batch 51 batch loss 1.36154366 epoch total loss 1.42968798\n",
      "Trained batch 52 batch loss 1.53331172 epoch total loss 1.43168068\n",
      "Trained batch 53 batch loss 1.38311839 epoch total loss 1.43076444\n",
      "Trained batch 54 batch loss 1.3432951 epoch total loss 1.4291445\n",
      "Trained batch 55 batch loss 1.25492144 epoch total loss 1.42597687\n",
      "Trained batch 56 batch loss 1.34331203 epoch total loss 1.4245007\n",
      "Trained batch 57 batch loss 1.39166844 epoch total loss 1.4239248\n",
      "Trained batch 58 batch loss 1.4108485 epoch total loss 1.42369938\n",
      "Trained batch 59 batch loss 1.42294288 epoch total loss 1.4236865\n",
      "Trained batch 60 batch loss 1.61599195 epoch total loss 1.42689157\n",
      "Trained batch 61 batch loss 1.50837982 epoch total loss 1.42822742\n",
      "Trained batch 62 batch loss 1.51446676 epoch total loss 1.42961836\n",
      "Trained batch 63 batch loss 1.28865373 epoch total loss 1.4273808\n",
      "Trained batch 64 batch loss 1.44783044 epoch total loss 1.42770028\n",
      "Trained batch 65 batch loss 1.35922623 epoch total loss 1.42664683\n",
      "Trained batch 66 batch loss 1.40541339 epoch total loss 1.42632496\n",
      "Trained batch 67 batch loss 1.43113947 epoch total loss 1.42639685\n",
      "Trained batch 68 batch loss 1.48575473 epoch total loss 1.42726982\n",
      "Trained batch 69 batch loss 1.39842296 epoch total loss 1.42685163\n",
      "Trained batch 70 batch loss 1.32574201 epoch total loss 1.42540729\n",
      "Trained batch 71 batch loss 1.2546556 epoch total loss 1.42300236\n",
      "Trained batch 72 batch loss 1.30080378 epoch total loss 1.42130518\n",
      "Trained batch 73 batch loss 1.49196374 epoch total loss 1.42227304\n",
      "Trained batch 74 batch loss 1.50390291 epoch total loss 1.4233762\n",
      "Trained batch 75 batch loss 1.51202297 epoch total loss 1.42455816\n",
      "Trained batch 76 batch loss 1.56813264 epoch total loss 1.42644727\n",
      "Trained batch 77 batch loss 1.45458221 epoch total loss 1.42681265\n",
      "Trained batch 78 batch loss 1.47200847 epoch total loss 1.42739213\n",
      "Trained batch 79 batch loss 1.54773104 epoch total loss 1.42891538\n",
      "Trained batch 80 batch loss 1.42051768 epoch total loss 1.42881036\n",
      "Trained batch 81 batch loss 1.44489431 epoch total loss 1.42900896\n",
      "Trained batch 82 batch loss 1.47596455 epoch total loss 1.42958164\n",
      "Trained batch 83 batch loss 1.49193025 epoch total loss 1.43033278\n",
      "Trained batch 84 batch loss 1.45442271 epoch total loss 1.4306196\n",
      "Trained batch 85 batch loss 1.49762106 epoch total loss 1.43140781\n",
      "Trained batch 86 batch loss 1.4210217 epoch total loss 1.43128705\n",
      "Trained batch 87 batch loss 1.47453475 epoch total loss 1.43178403\n",
      "Trained batch 88 batch loss 1.61848068 epoch total loss 1.43390572\n",
      "Trained batch 89 batch loss 1.45755732 epoch total loss 1.43417144\n",
      "Trained batch 90 batch loss 1.48695529 epoch total loss 1.43475795\n",
      "Trained batch 91 batch loss 1.47779775 epoch total loss 1.43523097\n",
      "Trained batch 92 batch loss 1.4435811 epoch total loss 1.43532181\n",
      "Trained batch 93 batch loss 1.34647846 epoch total loss 1.43436658\n",
      "Trained batch 94 batch loss 1.39530802 epoch total loss 1.43395102\n",
      "Trained batch 95 batch loss 1.3690697 epoch total loss 1.43326807\n",
      "Trained batch 96 batch loss 1.43800652 epoch total loss 1.4333173\n",
      "Trained batch 97 batch loss 1.39777899 epoch total loss 1.43295097\n",
      "Trained batch 98 batch loss 1.5184809 epoch total loss 1.4338237\n",
      "Trained batch 99 batch loss 1.4338553 epoch total loss 1.43382406\n",
      "Trained batch 100 batch loss 1.42201114 epoch total loss 1.43370593\n",
      "Trained batch 101 batch loss 1.329597 epoch total loss 1.432675\n",
      "Trained batch 102 batch loss 1.40744686 epoch total loss 1.43242764\n",
      "Trained batch 103 batch loss 1.43603694 epoch total loss 1.43246269\n",
      "Trained batch 104 batch loss 1.35028887 epoch total loss 1.43167257\n",
      "Trained batch 105 batch loss 1.3326993 epoch total loss 1.43073\n",
      "Trained batch 106 batch loss 1.3955574 epoch total loss 1.43039823\n",
      "Trained batch 107 batch loss 1.33892298 epoch total loss 1.42954338\n",
      "Trained batch 108 batch loss 1.48810518 epoch total loss 1.43008554\n",
      "Trained batch 109 batch loss 1.42684495 epoch total loss 1.43005586\n",
      "Trained batch 110 batch loss 1.35151839 epoch total loss 1.42934179\n",
      "Trained batch 111 batch loss 1.42882228 epoch total loss 1.42933714\n",
      "Trained batch 112 batch loss 1.28901148 epoch total loss 1.42808425\n",
      "Trained batch 113 batch loss 1.34153795 epoch total loss 1.42731833\n",
      "Trained batch 114 batch loss 1.46028376 epoch total loss 1.42760754\n",
      "Trained batch 115 batch loss 1.32259297 epoch total loss 1.42669427\n",
      "Trained batch 116 batch loss 1.23970461 epoch total loss 1.42508221\n",
      "Trained batch 117 batch loss 1.38201463 epoch total loss 1.42471421\n",
      "Trained batch 118 batch loss 1.31117189 epoch total loss 1.42375195\n",
      "Trained batch 119 batch loss 1.26225019 epoch total loss 1.42239487\n",
      "Trained batch 120 batch loss 1.28627419 epoch total loss 1.42126048\n",
      "Trained batch 121 batch loss 1.23728347 epoch total loss 1.41974008\n",
      "Trained batch 122 batch loss 1.33267653 epoch total loss 1.41902637\n",
      "Trained batch 123 batch loss 1.35922241 epoch total loss 1.41854012\n",
      "Trained batch 124 batch loss 1.35683906 epoch total loss 1.41804254\n",
      "Trained batch 125 batch loss 1.37590337 epoch total loss 1.41770542\n",
      "Trained batch 126 batch loss 1.51394629 epoch total loss 1.41846931\n",
      "Trained batch 127 batch loss 1.46594393 epoch total loss 1.41884303\n",
      "Trained batch 128 batch loss 1.42710841 epoch total loss 1.41890764\n",
      "Trained batch 129 batch loss 1.47275174 epoch total loss 1.41932499\n",
      "Trained batch 130 batch loss 1.33628702 epoch total loss 1.41868627\n",
      "Trained batch 131 batch loss 1.39243484 epoch total loss 1.41848588\n",
      "Trained batch 132 batch loss 1.32938695 epoch total loss 1.41781092\n",
      "Trained batch 133 batch loss 1.31840801 epoch total loss 1.41706359\n",
      "Trained batch 134 batch loss 1.44998097 epoch total loss 1.41730917\n",
      "Trained batch 135 batch loss 1.45735717 epoch total loss 1.41760576\n",
      "Trained batch 136 batch loss 1.38372779 epoch total loss 1.41735673\n",
      "Trained batch 137 batch loss 1.45021927 epoch total loss 1.41759658\n",
      "Trained batch 138 batch loss 1.37035275 epoch total loss 1.41725421\n",
      "Trained batch 139 batch loss 1.44319844 epoch total loss 1.41744089\n",
      "Trained batch 140 batch loss 1.47223794 epoch total loss 1.41783226\n",
      "Trained batch 141 batch loss 1.4597156 epoch total loss 1.41812932\n",
      "Trained batch 142 batch loss 1.42956185 epoch total loss 1.41820991\n",
      "Trained batch 143 batch loss 1.32765675 epoch total loss 1.41757655\n",
      "Trained batch 144 batch loss 1.39271963 epoch total loss 1.41740394\n",
      "Trained batch 145 batch loss 1.40776956 epoch total loss 1.41733754\n",
      "Trained batch 146 batch loss 1.350034 epoch total loss 1.41687655\n",
      "Trained batch 147 batch loss 1.3467052 epoch total loss 1.41639924\n",
      "Trained batch 148 batch loss 1.45086038 epoch total loss 1.41663218\n",
      "Trained batch 149 batch loss 1.42486024 epoch total loss 1.41668737\n",
      "Trained batch 150 batch loss 1.31487393 epoch total loss 1.41600871\n",
      "Trained batch 151 batch loss 1.35736644 epoch total loss 1.41562033\n",
      "Trained batch 152 batch loss 1.45446956 epoch total loss 1.41587591\n",
      "Trained batch 153 batch loss 1.50805426 epoch total loss 1.4164784\n",
      "Trained batch 154 batch loss 1.28028822 epoch total loss 1.41559398\n",
      "Trained batch 155 batch loss 1.20537186 epoch total loss 1.41423774\n",
      "Trained batch 156 batch loss 1.27326751 epoch total loss 1.41333413\n",
      "Trained batch 157 batch loss 1.39909875 epoch total loss 1.41324341\n",
      "Trained batch 158 batch loss 1.50715673 epoch total loss 1.41383779\n",
      "Trained batch 159 batch loss 1.4607532 epoch total loss 1.41413283\n",
      "Trained batch 160 batch loss 1.41783524 epoch total loss 1.41415596\n",
      "Trained batch 161 batch loss 1.45700717 epoch total loss 1.41442204\n",
      "Trained batch 162 batch loss 1.440207 epoch total loss 1.41458118\n",
      "Trained batch 163 batch loss 1.35409474 epoch total loss 1.41421008\n",
      "Trained batch 164 batch loss 1.41490066 epoch total loss 1.41421437\n",
      "Trained batch 165 batch loss 1.34962332 epoch total loss 1.41382289\n",
      "Trained batch 166 batch loss 1.36275196 epoch total loss 1.41351521\n",
      "Trained batch 167 batch loss 1.38332844 epoch total loss 1.41333449\n",
      "Trained batch 168 batch loss 1.36556721 epoch total loss 1.41305017\n",
      "Trained batch 169 batch loss 1.4210813 epoch total loss 1.41309762\n",
      "Trained batch 170 batch loss 1.4329052 epoch total loss 1.41321421\n",
      "Trained batch 171 batch loss 1.40432882 epoch total loss 1.41316223\n",
      "Trained batch 172 batch loss 1.39618516 epoch total loss 1.41306353\n",
      "Trained batch 173 batch loss 1.44594479 epoch total loss 1.41325355\n",
      "Trained batch 174 batch loss 1.54127157 epoch total loss 1.41398931\n",
      "Trained batch 175 batch loss 1.4179914 epoch total loss 1.41401207\n",
      "Trained batch 176 batch loss 1.44563615 epoch total loss 1.41419172\n",
      "Trained batch 177 batch loss 1.3508637 epoch total loss 1.41383398\n",
      "Trained batch 178 batch loss 1.26098907 epoch total loss 1.41297531\n",
      "Trained batch 179 batch loss 1.40994263 epoch total loss 1.41295838\n",
      "Trained batch 180 batch loss 1.27555192 epoch total loss 1.41219497\n",
      "Trained batch 181 batch loss 1.27710271 epoch total loss 1.4114486\n",
      "Trained batch 182 batch loss 1.2637 epoch total loss 1.41063678\n",
      "Trained batch 183 batch loss 1.21532333 epoch total loss 1.40956962\n",
      "Trained batch 184 batch loss 1.21263671 epoch total loss 1.40849936\n",
      "Trained batch 185 batch loss 1.34317446 epoch total loss 1.40814626\n",
      "Trained batch 186 batch loss 1.19261646 epoch total loss 1.40698755\n",
      "Trained batch 187 batch loss 1.3642931 epoch total loss 1.40675914\n",
      "Trained batch 188 batch loss 1.35455036 epoch total loss 1.4064815\n",
      "Trained batch 189 batch loss 1.41335011 epoch total loss 1.40651786\n",
      "Trained batch 190 batch loss 1.2793237 epoch total loss 1.4058485\n",
      "Trained batch 191 batch loss 1.27789903 epoch total loss 1.40517855\n",
      "Trained batch 192 batch loss 1.39500475 epoch total loss 1.40512562\n",
      "Trained batch 193 batch loss 1.41490936 epoch total loss 1.4051764\n",
      "Trained batch 194 batch loss 1.47129953 epoch total loss 1.40551722\n",
      "Trained batch 195 batch loss 1.44480896 epoch total loss 1.4057188\n",
      "Trained batch 196 batch loss 1.47145355 epoch total loss 1.40605426\n",
      "Trained batch 197 batch loss 1.48770416 epoch total loss 1.40646875\n",
      "Trained batch 198 batch loss 1.62143302 epoch total loss 1.40755439\n",
      "Trained batch 199 batch loss 1.4049089 epoch total loss 1.40754104\n",
      "Trained batch 200 batch loss 1.46252239 epoch total loss 1.40781605\n",
      "Trained batch 201 batch loss 1.55065298 epoch total loss 1.40852666\n",
      "Trained batch 202 batch loss 1.40773404 epoch total loss 1.40852284\n",
      "Trained batch 203 batch loss 1.37642097 epoch total loss 1.40836477\n",
      "Trained batch 204 batch loss 1.42754984 epoch total loss 1.40845883\n",
      "Trained batch 205 batch loss 1.57103777 epoch total loss 1.40925193\n",
      "Trained batch 206 batch loss 1.52395105 epoch total loss 1.40980875\n",
      "Trained batch 207 batch loss 1.42762 epoch total loss 1.4098947\n",
      "Trained batch 208 batch loss 1.42579436 epoch total loss 1.40997112\n",
      "Trained batch 209 batch loss 1.44343495 epoch total loss 1.4101311\n",
      "Trained batch 210 batch loss 1.43688893 epoch total loss 1.41025853\n",
      "Trained batch 211 batch loss 1.48164034 epoch total loss 1.41059685\n",
      "Trained batch 212 batch loss 1.4864018 epoch total loss 1.41095436\n",
      "Trained batch 213 batch loss 1.5488596 epoch total loss 1.41160178\n",
      "Trained batch 214 batch loss 1.49752 epoch total loss 1.41200328\n",
      "Trained batch 215 batch loss 1.31271482 epoch total loss 1.41154146\n",
      "Trained batch 216 batch loss 1.25182724 epoch total loss 1.41080201\n",
      "Trained batch 217 batch loss 1.50591075 epoch total loss 1.41124034\n",
      "Trained batch 218 batch loss 1.54656768 epoch total loss 1.41186118\n",
      "Trained batch 219 batch loss 1.48417187 epoch total loss 1.41219127\n",
      "Trained batch 220 batch loss 1.44227982 epoch total loss 1.41232812\n",
      "Trained batch 221 batch loss 1.58510089 epoch total loss 1.41311\n",
      "Trained batch 222 batch loss 1.50329745 epoch total loss 1.41351616\n",
      "Trained batch 223 batch loss 1.4139576 epoch total loss 1.41351819\n",
      "Trained batch 224 batch loss 1.30012655 epoch total loss 1.41301215\n",
      "Trained batch 225 batch loss 1.41473079 epoch total loss 1.41301978\n",
      "Trained batch 226 batch loss 1.28229129 epoch total loss 1.41244125\n",
      "Trained batch 227 batch loss 1.51728678 epoch total loss 1.41290307\n",
      "Trained batch 228 batch loss 1.45038164 epoch total loss 1.41306746\n",
      "Trained batch 229 batch loss 1.53707409 epoch total loss 1.41360903\n",
      "Trained batch 230 batch loss 1.4465611 epoch total loss 1.41375232\n",
      "Trained batch 231 batch loss 1.4647584 epoch total loss 1.41397309\n",
      "Trained batch 232 batch loss 1.49992287 epoch total loss 1.41434348\n",
      "Trained batch 233 batch loss 1.47129548 epoch total loss 1.41458786\n",
      "Trained batch 234 batch loss 1.3666954 epoch total loss 1.41438317\n",
      "Trained batch 235 batch loss 1.30054021 epoch total loss 1.41389871\n",
      "Trained batch 236 batch loss 1.36767268 epoch total loss 1.41370285\n",
      "Trained batch 237 batch loss 1.36599481 epoch total loss 1.41350162\n",
      "Trained batch 238 batch loss 1.34264481 epoch total loss 1.41320395\n",
      "Trained batch 239 batch loss 1.45079172 epoch total loss 1.41336119\n",
      "Trained batch 240 batch loss 1.32598662 epoch total loss 1.41299713\n",
      "Trained batch 241 batch loss 1.47093642 epoch total loss 1.41323769\n",
      "Trained batch 242 batch loss 1.3896693 epoch total loss 1.4131403\n",
      "Trained batch 243 batch loss 1.53916967 epoch total loss 1.41365898\n",
      "Trained batch 244 batch loss 1.5903945 epoch total loss 1.41438329\n",
      "Trained batch 245 batch loss 1.48552334 epoch total loss 1.41467369\n",
      "Trained batch 246 batch loss 1.35475731 epoch total loss 1.41443014\n",
      "Trained batch 247 batch loss 1.37820566 epoch total loss 1.41428351\n",
      "Trained batch 248 batch loss 1.33370769 epoch total loss 1.41395867\n",
      "Trained batch 249 batch loss 1.36557639 epoch total loss 1.41376436\n",
      "Trained batch 250 batch loss 1.43081594 epoch total loss 1.41383255\n",
      "Trained batch 251 batch loss 1.45056546 epoch total loss 1.41397882\n",
      "Trained batch 252 batch loss 1.51069188 epoch total loss 1.41436255\n",
      "Trained batch 253 batch loss 1.54866087 epoch total loss 1.41489351\n",
      "Trained batch 254 batch loss 1.52910411 epoch total loss 1.41534317\n",
      "Trained batch 255 batch loss 1.5578835 epoch total loss 1.41590214\n",
      "Trained batch 256 batch loss 1.5591712 epoch total loss 1.41646183\n",
      "Trained batch 257 batch loss 1.50531149 epoch total loss 1.41680753\n",
      "Trained batch 258 batch loss 1.50036716 epoch total loss 1.41713142\n",
      "Trained batch 259 batch loss 1.44474053 epoch total loss 1.417238\n",
      "Trained batch 260 batch loss 1.47386622 epoch total loss 1.41745579\n",
      "Trained batch 261 batch loss 1.38564992 epoch total loss 1.41733396\n",
      "Trained batch 262 batch loss 1.38069665 epoch total loss 1.41719413\n",
      "Trained batch 263 batch loss 1.35198748 epoch total loss 1.41694629\n",
      "Trained batch 264 batch loss 1.33431029 epoch total loss 1.41663325\n",
      "Trained batch 265 batch loss 1.48990691 epoch total loss 1.41690969\n",
      "Trained batch 266 batch loss 1.4684763 epoch total loss 1.41710353\n",
      "Trained batch 267 batch loss 1.43297648 epoch total loss 1.41716301\n",
      "Trained batch 268 batch loss 1.45278 epoch total loss 1.41729605\n",
      "Trained batch 269 batch loss 1.39530802 epoch total loss 1.41721416\n",
      "Trained batch 270 batch loss 1.53009689 epoch total loss 1.41763222\n",
      "Trained batch 271 batch loss 1.4295156 epoch total loss 1.41767609\n",
      "Trained batch 272 batch loss 1.44722509 epoch total loss 1.41778469\n",
      "Trained batch 273 batch loss 1.50675178 epoch total loss 1.41811061\n",
      "Trained batch 274 batch loss 1.4660567 epoch total loss 1.41828561\n",
      "Trained batch 275 batch loss 1.52865756 epoch total loss 1.41868699\n",
      "Trained batch 276 batch loss 1.42876768 epoch total loss 1.41872346\n",
      "Trained batch 277 batch loss 1.44717717 epoch total loss 1.41882622\n",
      "Trained batch 278 batch loss 1.44001055 epoch total loss 1.4189024\n",
      "Trained batch 279 batch loss 1.56794155 epoch total loss 1.41943657\n",
      "Trained batch 280 batch loss 1.47999907 epoch total loss 1.41965294\n",
      "Trained batch 281 batch loss 1.42815661 epoch total loss 1.41968322\n",
      "Trained batch 282 batch loss 1.4542 epoch total loss 1.41980553\n",
      "Trained batch 283 batch loss 1.4397459 epoch total loss 1.4198761\n",
      "Trained batch 284 batch loss 1.43061471 epoch total loss 1.41991377\n",
      "Trained batch 285 batch loss 1.42231214 epoch total loss 1.41992223\n",
      "Trained batch 286 batch loss 1.37749386 epoch total loss 1.41977382\n",
      "Trained batch 287 batch loss 1.51297963 epoch total loss 1.42009854\n",
      "Trained batch 288 batch loss 1.38898969 epoch total loss 1.41999054\n",
      "Trained batch 289 batch loss 1.41270494 epoch total loss 1.41996539\n",
      "Trained batch 290 batch loss 1.41706896 epoch total loss 1.41995549\n",
      "Trained batch 291 batch loss 1.44570327 epoch total loss 1.42004395\n",
      "Trained batch 292 batch loss 1.4220525 epoch total loss 1.42005086\n",
      "Trained batch 293 batch loss 1.38822424 epoch total loss 1.41994214\n",
      "Trained batch 294 batch loss 1.39287567 epoch total loss 1.41985011\n",
      "Trained batch 295 batch loss 1.41523266 epoch total loss 1.41983449\n",
      "Trained batch 296 batch loss 1.38734782 epoch total loss 1.4197247\n",
      "Trained batch 297 batch loss 1.33521366 epoch total loss 1.41944015\n",
      "Trained batch 298 batch loss 1.33517194 epoch total loss 1.41915739\n",
      "Trained batch 299 batch loss 1.42847371 epoch total loss 1.4191885\n",
      "Trained batch 300 batch loss 1.41999781 epoch total loss 1.41919124\n",
      "Trained batch 301 batch loss 1.40140688 epoch total loss 1.41913211\n",
      "Trained batch 302 batch loss 1.40409219 epoch total loss 1.41908228\n",
      "Trained batch 303 batch loss 1.49416018 epoch total loss 1.41933\n",
      "Trained batch 304 batch loss 1.40564537 epoch total loss 1.41928506\n",
      "Trained batch 305 batch loss 1.41862798 epoch total loss 1.41928291\n",
      "Trained batch 306 batch loss 1.36587286 epoch total loss 1.41910839\n",
      "Trained batch 307 batch loss 1.3677038 epoch total loss 1.4189409\n",
      "Trained batch 308 batch loss 1.38523436 epoch total loss 1.41883147\n",
      "Trained batch 309 batch loss 1.416116 epoch total loss 1.41882265\n",
      "Trained batch 310 batch loss 1.2727226 epoch total loss 1.41835141\n",
      "Trained batch 311 batch loss 1.36082768 epoch total loss 1.41816652\n",
      "Trained batch 312 batch loss 1.34841621 epoch total loss 1.41794288\n",
      "Trained batch 313 batch loss 1.32885468 epoch total loss 1.41765833\n",
      "Trained batch 314 batch loss 1.31349707 epoch total loss 1.41732657\n",
      "Trained batch 315 batch loss 1.38278568 epoch total loss 1.4172169\n",
      "Trained batch 316 batch loss 1.25250554 epoch total loss 1.41669571\n",
      "Trained batch 317 batch loss 1.41815376 epoch total loss 1.41670024\n",
      "Trained batch 318 batch loss 1.57148492 epoch total loss 1.41718698\n",
      "Trained batch 319 batch loss 1.37569129 epoch total loss 1.41705692\n",
      "Trained batch 320 batch loss 1.61528969 epoch total loss 1.41767645\n",
      "Trained batch 321 batch loss 1.63006 epoch total loss 1.41833806\n",
      "Trained batch 322 batch loss 1.47447038 epoch total loss 1.41851234\n",
      "Trained batch 323 batch loss 1.53955829 epoch total loss 1.41888714\n",
      "Trained batch 324 batch loss 1.53777266 epoch total loss 1.41925406\n",
      "Trained batch 325 batch loss 1.40149856 epoch total loss 1.41919935\n",
      "Trained batch 326 batch loss 1.43851113 epoch total loss 1.41925859\n",
      "Trained batch 327 batch loss 1.37181699 epoch total loss 1.41911364\n",
      "Trained batch 328 batch loss 1.35534525 epoch total loss 1.41891921\n",
      "Trained batch 329 batch loss 1.25734448 epoch total loss 1.41842806\n",
      "Trained batch 330 batch loss 1.19558072 epoch total loss 1.41775286\n",
      "Trained batch 331 batch loss 1.42071891 epoch total loss 1.4177618\n",
      "Trained batch 332 batch loss 1.54070365 epoch total loss 1.41813207\n",
      "Trained batch 333 batch loss 1.48377156 epoch total loss 1.41832924\n",
      "Trained batch 334 batch loss 1.44983399 epoch total loss 1.41842353\n",
      "Trained batch 335 batch loss 1.37351596 epoch total loss 1.41828942\n",
      "Trained batch 336 batch loss 1.38718128 epoch total loss 1.4181968\n",
      "Trained batch 337 batch loss 1.44815087 epoch total loss 1.41828573\n",
      "Trained batch 338 batch loss 1.44654131 epoch total loss 1.41836929\n",
      "Trained batch 339 batch loss 1.30484772 epoch total loss 1.41803443\n",
      "Trained batch 340 batch loss 1.38076818 epoch total loss 1.41792476\n",
      "Trained batch 341 batch loss 1.43626952 epoch total loss 1.41797864\n",
      "Trained batch 342 batch loss 1.42773151 epoch total loss 1.41800714\n",
      "Trained batch 343 batch loss 1.44204855 epoch total loss 1.41807723\n",
      "Trained batch 344 batch loss 1.34444928 epoch total loss 1.41786313\n",
      "Trained batch 345 batch loss 1.34822249 epoch total loss 1.41766131\n",
      "Trained batch 346 batch loss 1.34223378 epoch total loss 1.41744339\n",
      "Trained batch 347 batch loss 1.36066413 epoch total loss 1.41727972\n",
      "Trained batch 348 batch loss 1.30111587 epoch total loss 1.41694593\n",
      "Trained batch 349 batch loss 1.34497452 epoch total loss 1.4167397\n",
      "Trained batch 350 batch loss 1.14899421 epoch total loss 1.41597462\n",
      "Trained batch 351 batch loss 1.24064851 epoch total loss 1.41547513\n",
      "Trained batch 352 batch loss 1.31556046 epoch total loss 1.41519129\n",
      "Trained batch 353 batch loss 1.32259095 epoch total loss 1.41492903\n",
      "Trained batch 354 batch loss 1.32019818 epoch total loss 1.41466141\n",
      "Trained batch 355 batch loss 1.37924182 epoch total loss 1.41456163\n",
      "Trained batch 356 batch loss 1.50430059 epoch total loss 1.41481364\n",
      "Trained batch 357 batch loss 1.36199927 epoch total loss 1.4146657\n",
      "Trained batch 358 batch loss 1.37782156 epoch total loss 1.41456282\n",
      "Trained batch 359 batch loss 1.30091548 epoch total loss 1.4142462\n",
      "Trained batch 360 batch loss 1.39753699 epoch total loss 1.41419971\n",
      "Trained batch 361 batch loss 1.41081345 epoch total loss 1.41419041\n",
      "Trained batch 362 batch loss 1.17932224 epoch total loss 1.41354156\n",
      "Trained batch 363 batch loss 1.12170315 epoch total loss 1.41273761\n",
      "Trained batch 364 batch loss 1.1725316 epoch total loss 1.41207767\n",
      "Trained batch 365 batch loss 1.28521776 epoch total loss 1.41173017\n",
      "Trained batch 366 batch loss 1.59057581 epoch total loss 1.41221881\n",
      "Trained batch 367 batch loss 1.5318712 epoch total loss 1.41254473\n",
      "Trained batch 368 batch loss 1.54514 epoch total loss 1.4129051\n",
      "Trained batch 369 batch loss 1.60104704 epoch total loss 1.41341507\n",
      "Trained batch 370 batch loss 1.48386872 epoch total loss 1.41360557\n",
      "Trained batch 371 batch loss 1.40196371 epoch total loss 1.41357422\n",
      "Trained batch 372 batch loss 1.39218056 epoch total loss 1.41351664\n",
      "Trained batch 373 batch loss 1.50725496 epoch total loss 1.41376793\n",
      "Trained batch 374 batch loss 1.45639515 epoch total loss 1.41388202\n",
      "Trained batch 375 batch loss 1.46788561 epoch total loss 1.41402602\n",
      "Trained batch 376 batch loss 1.48855424 epoch total loss 1.41422415\n",
      "Trained batch 377 batch loss 1.48553705 epoch total loss 1.41441333\n",
      "Trained batch 378 batch loss 1.36896074 epoch total loss 1.41429305\n",
      "Trained batch 379 batch loss 1.44398141 epoch total loss 1.41437137\n",
      "Trained batch 380 batch loss 1.41605866 epoch total loss 1.4143759\n",
      "Trained batch 381 batch loss 1.46713006 epoch total loss 1.4145143\n",
      "Trained batch 382 batch loss 1.42067897 epoch total loss 1.41453028\n",
      "Trained batch 383 batch loss 1.39143419 epoch total loss 1.41447\n",
      "Trained batch 384 batch loss 1.33349109 epoch total loss 1.41425908\n",
      "Trained batch 385 batch loss 1.24977982 epoch total loss 1.41383183\n",
      "Trained batch 386 batch loss 1.31679606 epoch total loss 1.41358042\n",
      "Trained batch 387 batch loss 1.28798974 epoch total loss 1.41325581\n",
      "Trained batch 388 batch loss 1.34048128 epoch total loss 1.41306818\n",
      "Trained batch 389 batch loss 1.25983834 epoch total loss 1.41267419\n",
      "Trained batch 390 batch loss 1.23405039 epoch total loss 1.41221631\n",
      "Trained batch 391 batch loss 1.35800719 epoch total loss 1.41207767\n",
      "Trained batch 392 batch loss 1.41425049 epoch total loss 1.41208327\n",
      "Trained batch 393 batch loss 1.48254466 epoch total loss 1.41226256\n",
      "Trained batch 394 batch loss 1.31681609 epoch total loss 1.41202033\n",
      "Trained batch 395 batch loss 1.35149264 epoch total loss 1.41186714\n",
      "Trained batch 396 batch loss 1.43741417 epoch total loss 1.41193163\n",
      "Trained batch 397 batch loss 1.41438019 epoch total loss 1.41193783\n",
      "Trained batch 398 batch loss 1.43535852 epoch total loss 1.4119966\n",
      "Trained batch 399 batch loss 1.46274304 epoch total loss 1.41212392\n",
      "Trained batch 400 batch loss 1.51944757 epoch total loss 1.41239226\n",
      "Trained batch 401 batch loss 1.3280611 epoch total loss 1.41218197\n",
      "Trained batch 402 batch loss 1.29462862 epoch total loss 1.41188955\n",
      "Trained batch 403 batch loss 1.45929027 epoch total loss 1.41200709\n",
      "Trained batch 404 batch loss 1.44757318 epoch total loss 1.41209519\n",
      "Trained batch 405 batch loss 1.37435913 epoch total loss 1.41200209\n",
      "Trained batch 406 batch loss 1.41159153 epoch total loss 1.41200113\n",
      "Trained batch 407 batch loss 1.38968921 epoch total loss 1.41194642\n",
      "Trained batch 408 batch loss 1.42518127 epoch total loss 1.41197872\n",
      "Trained batch 409 batch loss 1.35925126 epoch total loss 1.41184986\n",
      "Trained batch 410 batch loss 1.46586347 epoch total loss 1.4119817\n",
      "Trained batch 411 batch loss 1.39889133 epoch total loss 1.41194975\n",
      "Trained batch 412 batch loss 1.34850526 epoch total loss 1.41179574\n",
      "Trained batch 413 batch loss 1.42852354 epoch total loss 1.41183627\n",
      "Trained batch 414 batch loss 1.39472961 epoch total loss 1.4117949\n",
      "Trained batch 415 batch loss 1.40944481 epoch total loss 1.41178918\n",
      "Trained batch 416 batch loss 1.32730317 epoch total loss 1.41158617\n",
      "Trained batch 417 batch loss 1.28012204 epoch total loss 1.41127098\n",
      "Trained batch 418 batch loss 1.41196799 epoch total loss 1.41127264\n",
      "Trained batch 419 batch loss 1.4076606 epoch total loss 1.41126406\n",
      "Trained batch 420 batch loss 1.40158165 epoch total loss 1.41124105\n",
      "Trained batch 421 batch loss 1.44554424 epoch total loss 1.41132259\n",
      "Trained batch 422 batch loss 1.42860961 epoch total loss 1.41136348\n",
      "Trained batch 423 batch loss 1.47258282 epoch total loss 1.4115082\n",
      "Trained batch 424 batch loss 1.45168602 epoch total loss 1.41160297\n",
      "Trained batch 425 batch loss 1.45812798 epoch total loss 1.41171241\n",
      "Trained batch 426 batch loss 1.51672173 epoch total loss 1.41195893\n",
      "Trained batch 427 batch loss 1.3081553 epoch total loss 1.41171587\n",
      "Trained batch 428 batch loss 1.378479 epoch total loss 1.41163826\n",
      "Trained batch 429 batch loss 1.34694088 epoch total loss 1.41148734\n",
      "Trained batch 430 batch loss 1.34963191 epoch total loss 1.41134346\n",
      "Trained batch 431 batch loss 1.3192184 epoch total loss 1.41112971\n",
      "Trained batch 432 batch loss 1.27465296 epoch total loss 1.41081381\n",
      "Trained batch 433 batch loss 1.32777441 epoch total loss 1.410622\n",
      "Trained batch 434 batch loss 1.38319063 epoch total loss 1.4105587\n",
      "Trained batch 435 batch loss 1.33869171 epoch total loss 1.41039348\n",
      "Trained batch 436 batch loss 1.25475788 epoch total loss 1.41003656\n",
      "Trained batch 437 batch loss 1.32234263 epoch total loss 1.40983582\n",
      "Trained batch 438 batch loss 1.34041166 epoch total loss 1.40967727\n",
      "Trained batch 439 batch loss 1.30129242 epoch total loss 1.40943038\n",
      "Trained batch 440 batch loss 1.44001412 epoch total loss 1.40949988\n",
      "Trained batch 441 batch loss 1.26854861 epoch total loss 1.40918028\n",
      "Trained batch 442 batch loss 1.25288749 epoch total loss 1.40882659\n",
      "Trained batch 443 batch loss 1.11655617 epoch total loss 1.40816689\n",
      "Trained batch 444 batch loss 1.11188936 epoch total loss 1.40749955\n",
      "Trained batch 445 batch loss 1.35026693 epoch total loss 1.40737104\n",
      "Trained batch 446 batch loss 1.41094911 epoch total loss 1.40737903\n",
      "Trained batch 447 batch loss 1.45826268 epoch total loss 1.40749288\n",
      "Trained batch 448 batch loss 1.40317976 epoch total loss 1.40748322\n",
      "Trained batch 449 batch loss 1.39192104 epoch total loss 1.40744853\n",
      "Trained batch 450 batch loss 1.35706675 epoch total loss 1.40733659\n",
      "Trained batch 451 batch loss 1.25137424 epoch total loss 1.40699077\n",
      "Trained batch 452 batch loss 1.29268146 epoch total loss 1.40673792\n",
      "Trained batch 453 batch loss 1.33827686 epoch total loss 1.40658665\n",
      "Trained batch 454 batch loss 1.41462564 epoch total loss 1.40660441\n",
      "Trained batch 455 batch loss 1.34853125 epoch total loss 1.40647674\n",
      "Trained batch 456 batch loss 1.32926989 epoch total loss 1.40630746\n",
      "Trained batch 457 batch loss 1.30882311 epoch total loss 1.40609419\n",
      "Trained batch 458 batch loss 1.38161302 epoch total loss 1.40604067\n",
      "Trained batch 459 batch loss 1.42375529 epoch total loss 1.40607929\n",
      "Trained batch 460 batch loss 1.48624849 epoch total loss 1.40625358\n",
      "Trained batch 461 batch loss 1.35673022 epoch total loss 1.40614617\n",
      "Trained batch 462 batch loss 1.27824497 epoch total loss 1.40586936\n",
      "Trained batch 463 batch loss 1.33275759 epoch total loss 1.40571153\n",
      "Trained batch 464 batch loss 1.30871487 epoch total loss 1.40550244\n",
      "Trained batch 465 batch loss 1.23263884 epoch total loss 1.40513074\n",
      "Trained batch 466 batch loss 1.35686195 epoch total loss 1.40502715\n",
      "Trained batch 467 batch loss 1.30417454 epoch total loss 1.40481126\n",
      "Trained batch 468 batch loss 1.3459959 epoch total loss 1.40468562\n",
      "Trained batch 469 batch loss 1.33358395 epoch total loss 1.40453398\n",
      "Trained batch 470 batch loss 1.23511314 epoch total loss 1.40417349\n",
      "Trained batch 471 batch loss 1.32676673 epoch total loss 1.40400922\n",
      "Trained batch 472 batch loss 1.27836931 epoch total loss 1.40374303\n",
      "Trained batch 473 batch loss 1.41806984 epoch total loss 1.40377331\n",
      "Trained batch 474 batch loss 1.41287029 epoch total loss 1.4037925\n",
      "Trained batch 475 batch loss 1.45187569 epoch total loss 1.40389383\n",
      "Trained batch 476 batch loss 1.40673256 epoch total loss 1.40389979\n",
      "Trained batch 477 batch loss 1.46524203 epoch total loss 1.40402842\n",
      "Trained batch 478 batch loss 1.38987 epoch total loss 1.40399885\n",
      "Trained batch 479 batch loss 1.35838687 epoch total loss 1.4039036\n",
      "Trained batch 480 batch loss 1.38423729 epoch total loss 1.4038626\n",
      "Trained batch 481 batch loss 1.3368957 epoch total loss 1.40372348\n",
      "Trained batch 482 batch loss 1.51287723 epoch total loss 1.40394986\n",
      "Trained batch 483 batch loss 1.49499726 epoch total loss 1.40413845\n",
      "Trained batch 484 batch loss 1.31575501 epoch total loss 1.40395582\n",
      "Trained batch 485 batch loss 1.42487943 epoch total loss 1.40399885\n",
      "Trained batch 486 batch loss 1.52547443 epoch total loss 1.40424883\n",
      "Trained batch 487 batch loss 1.42784965 epoch total loss 1.40429723\n",
      "Trained batch 488 batch loss 1.38659525 epoch total loss 1.40426099\n",
      "Trained batch 489 batch loss 1.38259971 epoch total loss 1.40421677\n",
      "Trained batch 490 batch loss 1.33134425 epoch total loss 1.40406811\n",
      "Trained batch 491 batch loss 1.30478382 epoch total loss 1.40386593\n",
      "Trained batch 492 batch loss 1.37361383 epoch total loss 1.40380442\n",
      "Trained batch 493 batch loss 1.36408138 epoch total loss 1.40372384\n",
      "Trained batch 494 batch loss 1.36363328 epoch total loss 1.40364265\n",
      "Trained batch 495 batch loss 1.30873168 epoch total loss 1.40345085\n",
      "Trained batch 496 batch loss 1.30441213 epoch total loss 1.40325117\n",
      "Trained batch 497 batch loss 1.33985746 epoch total loss 1.40312362\n",
      "Trained batch 498 batch loss 1.24450064 epoch total loss 1.40280509\n",
      "Trained batch 499 batch loss 1.35503 epoch total loss 1.40270936\n",
      "Trained batch 500 batch loss 1.36796749 epoch total loss 1.40263987\n",
      "Trained batch 501 batch loss 1.41077852 epoch total loss 1.40265608\n",
      "Trained batch 502 batch loss 1.4332875 epoch total loss 1.40271711\n",
      "Trained batch 503 batch loss 1.46127319 epoch total loss 1.40283346\n",
      "Trained batch 504 batch loss 1.47360134 epoch total loss 1.40297389\n",
      "Trained batch 505 batch loss 1.59191656 epoch total loss 1.40334797\n",
      "Trained batch 506 batch loss 1.6043359 epoch total loss 1.40374517\n",
      "Trained batch 507 batch loss 1.28230238 epoch total loss 1.40350556\n",
      "Trained batch 508 batch loss 1.29535818 epoch total loss 1.40329266\n",
      "Trained batch 509 batch loss 1.34711599 epoch total loss 1.40318227\n",
      "Trained batch 510 batch loss 1.26802588 epoch total loss 1.40291727\n",
      "Trained batch 511 batch loss 1.30141401 epoch total loss 1.40271854\n",
      "Trained batch 512 batch loss 1.33449149 epoch total loss 1.40258527\n",
      "Trained batch 513 batch loss 1.32672203 epoch total loss 1.40243733\n",
      "Trained batch 514 batch loss 1.36925459 epoch total loss 1.40237284\n",
      "Trained batch 515 batch loss 1.24627519 epoch total loss 1.40206969\n",
      "Trained batch 516 batch loss 1.28811932 epoch total loss 1.40184891\n",
      "Trained batch 517 batch loss 1.3163085 epoch total loss 1.40168345\n",
      "Trained batch 518 batch loss 1.3214215 epoch total loss 1.40152848\n",
      "Trained batch 519 batch loss 1.32201076 epoch total loss 1.40137529\n",
      "Trained batch 520 batch loss 1.24326658 epoch total loss 1.40107131\n",
      "Trained batch 521 batch loss 1.28573751 epoch total loss 1.40084994\n",
      "Trained batch 522 batch loss 1.30579519 epoch total loss 1.40066791\n",
      "Trained batch 523 batch loss 1.33707345 epoch total loss 1.40054631\n",
      "Trained batch 524 batch loss 1.31360245 epoch total loss 1.40038037\n",
      "Trained batch 525 batch loss 1.49836659 epoch total loss 1.40056694\n",
      "Trained batch 526 batch loss 1.41649628 epoch total loss 1.40059733\n",
      "Trained batch 527 batch loss 1.47931254 epoch total loss 1.4007467\n",
      "Trained batch 528 batch loss 1.78695881 epoch total loss 1.40147817\n",
      "Trained batch 529 batch loss 1.52201307 epoch total loss 1.4017061\n",
      "Trained batch 530 batch loss 1.50535154 epoch total loss 1.4019016\n",
      "Trained batch 531 batch loss 1.48760176 epoch total loss 1.40206301\n",
      "Trained batch 532 batch loss 1.43548822 epoch total loss 1.40212584\n",
      "Trained batch 533 batch loss 1.41503549 epoch total loss 1.40215015\n",
      "Trained batch 534 batch loss 1.47116244 epoch total loss 1.40227938\n",
      "Trained batch 535 batch loss 1.32041931 epoch total loss 1.40212643\n",
      "Trained batch 536 batch loss 1.42615533 epoch total loss 1.40217125\n",
      "Trained batch 537 batch loss 1.4084053 epoch total loss 1.40218282\n",
      "Trained batch 538 batch loss 1.47320282 epoch total loss 1.40231478\n",
      "Trained batch 539 batch loss 1.39412057 epoch total loss 1.40229964\n",
      "Trained batch 540 batch loss 1.37603831 epoch total loss 1.40225101\n",
      "Trained batch 541 batch loss 1.37443936 epoch total loss 1.40219963\n",
      "Trained batch 542 batch loss 1.31971765 epoch total loss 1.4020474\n",
      "Trained batch 543 batch loss 1.50018167 epoch total loss 1.40222812\n",
      "Trained batch 544 batch loss 1.26968884 epoch total loss 1.40198445\n",
      "Trained batch 545 batch loss 1.45978129 epoch total loss 1.40209055\n",
      "Trained batch 546 batch loss 1.24127328 epoch total loss 1.40179598\n",
      "Trained batch 547 batch loss 1.30559456 epoch total loss 1.40162015\n",
      "Trained batch 548 batch loss 1.27166343 epoch total loss 1.40138304\n",
      "Trained batch 549 batch loss 1.36160254 epoch total loss 1.40131044\n",
      "Trained batch 550 batch loss 1.40350556 epoch total loss 1.4013145\n",
      "Trained batch 551 batch loss 1.39665341 epoch total loss 1.40130603\n",
      "Trained batch 552 batch loss 1.3969804 epoch total loss 1.40129817\n",
      "Trained batch 553 batch loss 1.38522673 epoch total loss 1.4012692\n",
      "Trained batch 554 batch loss 1.32627034 epoch total loss 1.40113389\n",
      "Trained batch 555 batch loss 1.28518629 epoch total loss 1.40092492\n",
      "Trained batch 556 batch loss 1.39117098 epoch total loss 1.4009074\n",
      "Trained batch 557 batch loss 1.29278946 epoch total loss 1.40071321\n",
      "Trained batch 558 batch loss 1.40219986 epoch total loss 1.40071595\n",
      "Trained batch 559 batch loss 1.27226186 epoch total loss 1.40048611\n",
      "Trained batch 560 batch loss 1.39044559 epoch total loss 1.40046823\n",
      "Trained batch 561 batch loss 1.41529751 epoch total loss 1.40049458\n",
      "Trained batch 562 batch loss 1.28429842 epoch total loss 1.40028787\n",
      "Trained batch 563 batch loss 1.29878211 epoch total loss 1.40010762\n",
      "Trained batch 564 batch loss 1.26275051 epoch total loss 1.39986408\n",
      "Trained batch 565 batch loss 1.39108074 epoch total loss 1.39984846\n",
      "Trained batch 566 batch loss 1.42178297 epoch total loss 1.39988708\n",
      "Trained batch 567 batch loss 1.5186317 epoch total loss 1.40009654\n",
      "Trained batch 568 batch loss 1.59944654 epoch total loss 1.40044749\n",
      "Trained batch 569 batch loss 1.6469841 epoch total loss 1.40088069\n",
      "Trained batch 570 batch loss 1.62315273 epoch total loss 1.40127075\n",
      "Trained batch 571 batch loss 1.3962 epoch total loss 1.40126181\n",
      "Trained batch 572 batch loss 1.43211746 epoch total loss 1.40131581\n",
      "Trained batch 573 batch loss 1.25094342 epoch total loss 1.40105331\n",
      "Trained batch 574 batch loss 1.36074531 epoch total loss 1.40098298\n",
      "Trained batch 575 batch loss 1.44464445 epoch total loss 1.40105891\n",
      "Trained batch 576 batch loss 1.53049779 epoch total loss 1.40128374\n",
      "Trained batch 577 batch loss 1.45503163 epoch total loss 1.40137684\n",
      "Trained batch 578 batch loss 1.31237876 epoch total loss 1.40122283\n",
      "Trained batch 579 batch loss 1.33112824 epoch total loss 1.40110171\n",
      "Trained batch 580 batch loss 1.29172778 epoch total loss 1.40091324\n",
      "Trained batch 581 batch loss 1.29509139 epoch total loss 1.40073109\n",
      "Trained batch 582 batch loss 1.33825731 epoch total loss 1.4006238\n",
      "Trained batch 583 batch loss 1.32666767 epoch total loss 1.40049684\n",
      "Trained batch 584 batch loss 1.23228 epoch total loss 1.40020883\n",
      "Trained batch 585 batch loss 1.32743633 epoch total loss 1.4000845\n",
      "Trained batch 586 batch loss 1.32771945 epoch total loss 1.39996099\n",
      "Trained batch 587 batch loss 1.39059651 epoch total loss 1.39994502\n",
      "Trained batch 588 batch loss 1.52800632 epoch total loss 1.40016294\n",
      "Trained batch 589 batch loss 1.56841588 epoch total loss 1.40044856\n",
      "Trained batch 590 batch loss 1.51317644 epoch total loss 1.40063965\n",
      "Trained batch 591 batch loss 1.46618438 epoch total loss 1.40075052\n",
      "Trained batch 592 batch loss 1.39327526 epoch total loss 1.40073788\n",
      "Trained batch 593 batch loss 1.37189615 epoch total loss 1.40068924\n",
      "Trained batch 594 batch loss 1.27632976 epoch total loss 1.40047979\n",
      "Trained batch 595 batch loss 1.42107069 epoch total loss 1.40051448\n",
      "Trained batch 596 batch loss 1.49246979 epoch total loss 1.40066874\n",
      "Trained batch 597 batch loss 1.28489125 epoch total loss 1.40047491\n",
      "Trained batch 598 batch loss 1.29073834 epoch total loss 1.40029132\n",
      "Trained batch 599 batch loss 1.34203732 epoch total loss 1.40019405\n",
      "Trained batch 600 batch loss 1.24719965 epoch total loss 1.39993906\n",
      "Trained batch 601 batch loss 1.44282258 epoch total loss 1.40001035\n",
      "Trained batch 602 batch loss 1.55308151 epoch total loss 1.40026474\n",
      "Trained batch 603 batch loss 1.57157946 epoch total loss 1.40054882\n",
      "Trained batch 604 batch loss 1.51360631 epoch total loss 1.40073597\n",
      "Trained batch 605 batch loss 1.43975544 epoch total loss 1.40080047\n",
      "Trained batch 606 batch loss 1.51197791 epoch total loss 1.40098393\n",
      "Trained batch 607 batch loss 1.52435851 epoch total loss 1.40118718\n",
      "Trained batch 608 batch loss 1.40456426 epoch total loss 1.40119267\n",
      "Trained batch 609 batch loss 1.51708937 epoch total loss 1.40138304\n",
      "Trained batch 610 batch loss 1.49813199 epoch total loss 1.40154159\n",
      "Trained batch 611 batch loss 1.37347817 epoch total loss 1.4014957\n",
      "Trained batch 612 batch loss 1.52652144 epoch total loss 1.4017\n",
      "Trained batch 613 batch loss 1.58496332 epoch total loss 1.401999\n",
      "Trained batch 614 batch loss 1.4868809 epoch total loss 1.40213716\n",
      "Trained batch 615 batch loss 1.27938724 epoch total loss 1.40193748\n",
      "Trained batch 616 batch loss 1.30852652 epoch total loss 1.40178597\n",
      "Trained batch 617 batch loss 1.16281652 epoch total loss 1.40139866\n",
      "Trained batch 618 batch loss 1.33124173 epoch total loss 1.40128517\n",
      "Trained batch 619 batch loss 1.2922523 epoch total loss 1.40110898\n",
      "Trained batch 620 batch loss 1.18466318 epoch total loss 1.40075994\n",
      "Trained batch 621 batch loss 1.12866557 epoch total loss 1.40032172\n",
      "Trained batch 622 batch loss 1.16094208 epoch total loss 1.39993691\n",
      "Trained batch 623 batch loss 1.27118313 epoch total loss 1.39973021\n",
      "Trained batch 624 batch loss 1.28948879 epoch total loss 1.39955354\n",
      "Trained batch 625 batch loss 1.33407128 epoch total loss 1.39944875\n",
      "Trained batch 626 batch loss 1.40783727 epoch total loss 1.3994621\n",
      "Trained batch 627 batch loss 1.42634523 epoch total loss 1.39950502\n",
      "Trained batch 628 batch loss 1.41583514 epoch total loss 1.39953101\n",
      "Trained batch 629 batch loss 1.46044135 epoch total loss 1.3996278\n",
      "Trained batch 630 batch loss 1.43379176 epoch total loss 1.39968204\n",
      "Trained batch 631 batch loss 1.28385198 epoch total loss 1.39949846\n",
      "Trained batch 632 batch loss 1.23047876 epoch total loss 1.39923108\n",
      "Trained batch 633 batch loss 1.11558568 epoch total loss 1.39878297\n",
      "Trained batch 634 batch loss 1.13775206 epoch total loss 1.39837122\n",
      "Trained batch 635 batch loss 1.40815771 epoch total loss 1.3983866\n",
      "Trained batch 636 batch loss 1.41177654 epoch total loss 1.3984077\n",
      "Trained batch 637 batch loss 1.423334 epoch total loss 1.39844692\n",
      "Trained batch 638 batch loss 1.43660426 epoch total loss 1.39850664\n",
      "Trained batch 639 batch loss 1.3497951 epoch total loss 1.39843047\n",
      "Trained batch 640 batch loss 1.36554599 epoch total loss 1.39837909\n",
      "Trained batch 641 batch loss 1.43786645 epoch total loss 1.3984406\n",
      "Trained batch 642 batch loss 1.36904263 epoch total loss 1.39839482\n",
      "Trained batch 643 batch loss 1.37641943 epoch total loss 1.39836061\n",
      "Trained batch 644 batch loss 1.45475256 epoch total loss 1.39844823\n",
      "Trained batch 645 batch loss 1.57359242 epoch total loss 1.39871979\n",
      "Trained batch 646 batch loss 1.52652466 epoch total loss 1.39891768\n",
      "Trained batch 647 batch loss 1.41042268 epoch total loss 1.39893544\n",
      "Trained batch 648 batch loss 1.40993345 epoch total loss 1.39895236\n",
      "Trained batch 649 batch loss 1.31538737 epoch total loss 1.3988235\n",
      "Trained batch 650 batch loss 1.2555356 epoch total loss 1.39860308\n",
      "Trained batch 651 batch loss 1.27437055 epoch total loss 1.39841223\n",
      "Trained batch 652 batch loss 1.4338088 epoch total loss 1.39846659\n",
      "Trained batch 653 batch loss 1.35972273 epoch total loss 1.39840734\n",
      "Trained batch 654 batch loss 1.37813854 epoch total loss 1.39837623\n",
      "Trained batch 655 batch loss 1.38791645 epoch total loss 1.39836037\n",
      "Trained batch 656 batch loss 1.39653051 epoch total loss 1.39835763\n",
      "Trained batch 657 batch loss 1.38366199 epoch total loss 1.39833522\n",
      "Trained batch 658 batch loss 1.33913207 epoch total loss 1.39824522\n",
      "Trained batch 659 batch loss 1.31122279 epoch total loss 1.39811313\n",
      "Trained batch 660 batch loss 1.2921735 epoch total loss 1.39795268\n",
      "Trained batch 661 batch loss 1.33338654 epoch total loss 1.39785492\n",
      "Trained batch 662 batch loss 1.45429015 epoch total loss 1.39794016\n",
      "Trained batch 663 batch loss 1.35316646 epoch total loss 1.39787257\n",
      "Trained batch 664 batch loss 1.38724685 epoch total loss 1.39785671\n",
      "Trained batch 665 batch loss 1.34193635 epoch total loss 1.39777255\n",
      "Trained batch 666 batch loss 1.36659396 epoch total loss 1.3977257\n",
      "Trained batch 667 batch loss 1.42537737 epoch total loss 1.39776707\n",
      "Trained batch 668 batch loss 1.23262703 epoch total loss 1.39751983\n",
      "Trained batch 669 batch loss 1.32634103 epoch total loss 1.39741349\n",
      "Trained batch 670 batch loss 1.37529457 epoch total loss 1.39738047\n",
      "Trained batch 671 batch loss 1.45822811 epoch total loss 1.39747119\n",
      "Trained batch 672 batch loss 1.33878517 epoch total loss 1.39738393\n",
      "Trained batch 673 batch loss 1.22638345 epoch total loss 1.39712977\n",
      "Trained batch 674 batch loss 1.43608451 epoch total loss 1.39718759\n",
      "Trained batch 675 batch loss 1.41006243 epoch total loss 1.39720666\n",
      "Trained batch 676 batch loss 1.4597733 epoch total loss 1.39729929\n",
      "Trained batch 677 batch loss 1.42258024 epoch total loss 1.3973366\n",
      "Trained batch 678 batch loss 1.41329539 epoch total loss 1.39736009\n",
      "Trained batch 679 batch loss 1.36159968 epoch total loss 1.3973074\n",
      "Trained batch 680 batch loss 1.32172573 epoch total loss 1.39719629\n",
      "Trained batch 681 batch loss 1.41359651 epoch total loss 1.39722025\n",
      "Trained batch 682 batch loss 1.36645341 epoch total loss 1.39717519\n",
      "Trained batch 683 batch loss 1.40580833 epoch total loss 1.39718783\n",
      "Trained batch 684 batch loss 1.36033058 epoch total loss 1.39713395\n",
      "Trained batch 685 batch loss 1.33304667 epoch total loss 1.39704049\n",
      "Trained batch 686 batch loss 1.24973917 epoch total loss 1.39682579\n",
      "Trained batch 687 batch loss 1.22548568 epoch total loss 1.39657629\n",
      "Trained batch 688 batch loss 1.24535847 epoch total loss 1.39635658\n",
      "Trained batch 689 batch loss 1.20105422 epoch total loss 1.3960731\n",
      "Trained batch 690 batch loss 1.32034171 epoch total loss 1.39596331\n",
      "Trained batch 691 batch loss 1.40867877 epoch total loss 1.39598167\n",
      "Trained batch 692 batch loss 1.51142502 epoch total loss 1.39614856\n",
      "Trained batch 693 batch loss 1.46133101 epoch total loss 1.3962425\n",
      "Trained batch 694 batch loss 1.51416397 epoch total loss 1.39641249\n",
      "Trained batch 695 batch loss 1.47786212 epoch total loss 1.39652956\n",
      "Trained batch 696 batch loss 1.37127852 epoch total loss 1.39649332\n",
      "Trained batch 697 batch loss 1.39304256 epoch total loss 1.39648843\n",
      "Trained batch 698 batch loss 1.3205173 epoch total loss 1.39637959\n",
      "Trained batch 699 batch loss 1.42664886 epoch total loss 1.39642286\n",
      "Trained batch 700 batch loss 1.3957361 epoch total loss 1.39642191\n",
      "Trained batch 701 batch loss 1.32197988 epoch total loss 1.39631569\n",
      "Trained batch 702 batch loss 1.44388485 epoch total loss 1.3963834\n",
      "Trained batch 703 batch loss 1.41474605 epoch total loss 1.39640951\n",
      "Trained batch 704 batch loss 1.55444407 epoch total loss 1.39663398\n",
      "Trained batch 705 batch loss 1.45673645 epoch total loss 1.39671922\n",
      "Trained batch 706 batch loss 1.43411589 epoch total loss 1.39677227\n",
      "Trained batch 707 batch loss 1.3589406 epoch total loss 1.39671874\n",
      "Trained batch 708 batch loss 1.27423096 epoch total loss 1.39654577\n",
      "Trained batch 709 batch loss 1.42015481 epoch total loss 1.39657903\n",
      "Trained batch 710 batch loss 1.32780886 epoch total loss 1.39648223\n",
      "Trained batch 711 batch loss 1.30758691 epoch total loss 1.3963573\n",
      "Trained batch 712 batch loss 1.37905967 epoch total loss 1.39633298\n",
      "Trained batch 713 batch loss 1.35297728 epoch total loss 1.39627218\n",
      "Trained batch 714 batch loss 1.3681457 epoch total loss 1.39623284\n",
      "Trained batch 715 batch loss 1.34341 epoch total loss 1.39615893\n",
      "Trained batch 716 batch loss 1.38610923 epoch total loss 1.39614487\n",
      "Trained batch 717 batch loss 1.26235068 epoch total loss 1.39595819\n",
      "Trained batch 718 batch loss 1.38182688 epoch total loss 1.39593852\n",
      "Trained batch 719 batch loss 1.41187549 epoch total loss 1.39596069\n",
      "Trained batch 720 batch loss 1.27910805 epoch total loss 1.39579844\n",
      "Trained batch 721 batch loss 1.27784872 epoch total loss 1.39563477\n",
      "Trained batch 722 batch loss 1.35533178 epoch total loss 1.39557898\n",
      "Trained batch 723 batch loss 1.43184304 epoch total loss 1.39562917\n",
      "Trained batch 724 batch loss 1.48055434 epoch total loss 1.39574635\n",
      "Trained batch 725 batch loss 1.1947844 epoch total loss 1.39546919\n",
      "Trained batch 726 batch loss 1.12273049 epoch total loss 1.39509356\n",
      "Trained batch 727 batch loss 1.21203589 epoch total loss 1.39484167\n",
      "Trained batch 728 batch loss 1.28604794 epoch total loss 1.3946923\n",
      "Trained batch 729 batch loss 1.31318128 epoch total loss 1.39458048\n",
      "Trained batch 730 batch loss 1.20215154 epoch total loss 1.39431691\n",
      "Trained batch 731 batch loss 1.38263381 epoch total loss 1.39430094\n",
      "Trained batch 732 batch loss 1.30883074 epoch total loss 1.39418411\n",
      "Trained batch 733 batch loss 1.43174 epoch total loss 1.39423537\n",
      "Trained batch 734 batch loss 1.36695814 epoch total loss 1.39419818\n",
      "Trained batch 735 batch loss 1.48378074 epoch total loss 1.39432013\n",
      "Trained batch 736 batch loss 1.53495836 epoch total loss 1.39451122\n",
      "Trained batch 737 batch loss 1.50148487 epoch total loss 1.3946563\n",
      "Trained batch 738 batch loss 1.34247553 epoch total loss 1.39458561\n",
      "Trained batch 739 batch loss 1.47185397 epoch total loss 1.39469016\n",
      "Trained batch 740 batch loss 1.4945699 epoch total loss 1.39482522\n",
      "Trained batch 741 batch loss 1.39726782 epoch total loss 1.39482844\n",
      "Trained batch 742 batch loss 1.29993153 epoch total loss 1.39470053\n",
      "Trained batch 743 batch loss 1.25931573 epoch total loss 1.39451826\n",
      "Trained batch 744 batch loss 1.27291906 epoch total loss 1.39435482\n",
      "Trained batch 745 batch loss 1.34554458 epoch total loss 1.39428937\n",
      "Trained batch 746 batch loss 1.43611908 epoch total loss 1.39434552\n",
      "Trained batch 747 batch loss 1.40166783 epoch total loss 1.3943553\n",
      "Trained batch 748 batch loss 1.34308648 epoch total loss 1.39428675\n",
      "Trained batch 749 batch loss 1.40067208 epoch total loss 1.39429522\n",
      "Trained batch 750 batch loss 1.29764926 epoch total loss 1.39416635\n",
      "Trained batch 751 batch loss 1.31216788 epoch total loss 1.39405715\n",
      "Trained batch 752 batch loss 1.3665483 epoch total loss 1.39402056\n",
      "Trained batch 753 batch loss 1.45976567 epoch total loss 1.39410782\n",
      "Trained batch 754 batch loss 1.43957686 epoch total loss 1.39416814\n",
      "Trained batch 755 batch loss 1.35197711 epoch total loss 1.39411211\n",
      "Trained batch 756 batch loss 1.39164138 epoch total loss 1.39410889\n",
      "Trained batch 757 batch loss 1.31741858 epoch total loss 1.39400744\n",
      "Trained batch 758 batch loss 1.30288351 epoch total loss 1.39388728\n",
      "Trained batch 759 batch loss 1.36685991 epoch total loss 1.39385164\n",
      "Trained batch 760 batch loss 1.34941888 epoch total loss 1.39379299\n",
      "Trained batch 761 batch loss 1.38253093 epoch total loss 1.39377832\n",
      "Trained batch 762 batch loss 1.43971741 epoch total loss 1.39383852\n",
      "Trained batch 763 batch loss 1.4015305 epoch total loss 1.39384854\n",
      "Trained batch 764 batch loss 1.23166013 epoch total loss 1.39363635\n",
      "Trained batch 765 batch loss 1.30052328 epoch total loss 1.39351463\n",
      "Trained batch 766 batch loss 1.33849049 epoch total loss 1.39344287\n",
      "Trained batch 767 batch loss 1.30405903 epoch total loss 1.39332628\n",
      "Trained batch 768 batch loss 1.247365 epoch total loss 1.39313614\n",
      "Trained batch 769 batch loss 1.28889751 epoch total loss 1.39300072\n",
      "Trained batch 770 batch loss 1.21285701 epoch total loss 1.39276683\n",
      "Trained batch 771 batch loss 1.37209952 epoch total loss 1.39273989\n",
      "Trained batch 772 batch loss 1.24023902 epoch total loss 1.39254236\n",
      "Trained batch 773 batch loss 1.25385284 epoch total loss 1.39236307\n",
      "Trained batch 774 batch loss 1.33105445 epoch total loss 1.3922838\n",
      "Trained batch 775 batch loss 1.31754696 epoch total loss 1.39218736\n",
      "Trained batch 776 batch loss 1.41374886 epoch total loss 1.39221501\n",
      "Trained batch 777 batch loss 1.42999029 epoch total loss 1.39226365\n",
      "Trained batch 778 batch loss 1.48363674 epoch total loss 1.39238107\n",
      "Trained batch 779 batch loss 1.37352133 epoch total loss 1.39235687\n",
      "Trained batch 780 batch loss 1.30077839 epoch total loss 1.39223945\n",
      "Trained batch 781 batch loss 1.30555415 epoch total loss 1.39212847\n",
      "Trained batch 782 batch loss 1.22953737 epoch total loss 1.39192045\n",
      "Trained batch 783 batch loss 1.294312 epoch total loss 1.39179587\n",
      "Trained batch 784 batch loss 1.42056644 epoch total loss 1.39183247\n",
      "Trained batch 785 batch loss 1.38950288 epoch total loss 1.39182949\n",
      "Trained batch 786 batch loss 1.41026604 epoch total loss 1.39185297\n",
      "Trained batch 787 batch loss 1.38114214 epoch total loss 1.39183939\n",
      "Trained batch 788 batch loss 1.25088942 epoch total loss 1.39166045\n",
      "Trained batch 789 batch loss 1.31850433 epoch total loss 1.39156771\n",
      "Trained batch 790 batch loss 1.33651495 epoch total loss 1.39149809\n",
      "Trained batch 791 batch loss 1.34059608 epoch total loss 1.39143372\n",
      "Trained batch 792 batch loss 1.25606477 epoch total loss 1.39126277\n",
      "Trained batch 793 batch loss 1.23569155 epoch total loss 1.39106667\n",
      "Trained batch 794 batch loss 1.39138424 epoch total loss 1.39106703\n",
      "Trained batch 795 batch loss 1.41792178 epoch total loss 1.39110088\n",
      "Trained batch 796 batch loss 1.38192654 epoch total loss 1.39108932\n",
      "Trained batch 797 batch loss 1.41721392 epoch total loss 1.39112222\n",
      "Trained batch 798 batch loss 1.29899347 epoch total loss 1.39100671\n",
      "Trained batch 799 batch loss 1.16921306 epoch total loss 1.39072907\n",
      "Trained batch 800 batch loss 1.20230556 epoch total loss 1.39049351\n",
      "Trained batch 801 batch loss 1.2059536 epoch total loss 1.39026308\n",
      "Trained batch 802 batch loss 1.34591091 epoch total loss 1.39020777\n",
      "Trained batch 803 batch loss 1.44616485 epoch total loss 1.3902775\n",
      "Trained batch 804 batch loss 1.47701335 epoch total loss 1.39038539\n",
      "Trained batch 805 batch loss 1.5083046 epoch total loss 1.3905319\n",
      "Trained batch 806 batch loss 1.23767221 epoch total loss 1.39034224\n",
      "Trained batch 807 batch loss 1.23811841 epoch total loss 1.39015365\n",
      "Trained batch 808 batch loss 1.30940294 epoch total loss 1.39005375\n",
      "Trained batch 809 batch loss 1.27285409 epoch total loss 1.38990891\n",
      "Trained batch 810 batch loss 1.38002622 epoch total loss 1.38989663\n",
      "Trained batch 811 batch loss 1.36905146 epoch total loss 1.38987088\n",
      "Trained batch 812 batch loss 1.42122686 epoch total loss 1.38990963\n",
      "Trained batch 813 batch loss 1.42185092 epoch total loss 1.38994884\n",
      "Trained batch 814 batch loss 1.46399093 epoch total loss 1.3900398\n",
      "Trained batch 815 batch loss 1.44667494 epoch total loss 1.3901093\n",
      "Trained batch 816 batch loss 1.36322355 epoch total loss 1.3900764\n",
      "Trained batch 817 batch loss 1.30637085 epoch total loss 1.389974\n",
      "Trained batch 818 batch loss 1.36411858 epoch total loss 1.38994241\n",
      "Trained batch 819 batch loss 1.3906728 epoch total loss 1.38994324\n",
      "Trained batch 820 batch loss 1.33206689 epoch total loss 1.38987267\n",
      "Trained batch 821 batch loss 1.31108522 epoch total loss 1.38977659\n",
      "Trained batch 822 batch loss 1.23965836 epoch total loss 1.38959396\n",
      "Trained batch 823 batch loss 1.27820814 epoch total loss 1.38945854\n",
      "Trained batch 824 batch loss 1.31288409 epoch total loss 1.38936567\n",
      "Trained batch 825 batch loss 1.4174906 epoch total loss 1.38939977\n",
      "Trained batch 826 batch loss 1.37976205 epoch total loss 1.38938808\n",
      "Trained batch 827 batch loss 1.33364069 epoch total loss 1.38932061\n",
      "Trained batch 828 batch loss 1.28074777 epoch total loss 1.38918948\n",
      "Trained batch 829 batch loss 1.22924566 epoch total loss 1.3889966\n",
      "Trained batch 830 batch loss 1.25646281 epoch total loss 1.38883686\n",
      "Trained batch 831 batch loss 1.25452781 epoch total loss 1.38867521\n",
      "Trained batch 832 batch loss 1.30346346 epoch total loss 1.38857281\n",
      "Trained batch 833 batch loss 1.41508007 epoch total loss 1.38860464\n",
      "Trained batch 834 batch loss 1.28125155 epoch total loss 1.38847589\n",
      "Trained batch 835 batch loss 1.35006917 epoch total loss 1.38842988\n",
      "Trained batch 836 batch loss 1.41057706 epoch total loss 1.38845634\n",
      "Trained batch 837 batch loss 1.36865282 epoch total loss 1.38843274\n",
      "Trained batch 838 batch loss 1.59950733 epoch total loss 1.38868451\n",
      "Trained batch 839 batch loss 1.63251257 epoch total loss 1.38897526\n",
      "Trained batch 840 batch loss 1.36248338 epoch total loss 1.38894367\n",
      "Trained batch 841 batch loss 1.32356524 epoch total loss 1.38886595\n",
      "Trained batch 842 batch loss 1.34472871 epoch total loss 1.3888135\n",
      "Trained batch 843 batch loss 1.38635874 epoch total loss 1.38881063\n",
      "Trained batch 844 batch loss 1.54114544 epoch total loss 1.38899112\n",
      "Trained batch 845 batch loss 1.41963625 epoch total loss 1.38902736\n",
      "Trained batch 846 batch loss 1.34313905 epoch total loss 1.38897312\n",
      "Trained batch 847 batch loss 1.10308778 epoch total loss 1.38863552\n",
      "Trained batch 848 batch loss 1.13251543 epoch total loss 1.38833356\n",
      "Trained batch 849 batch loss 1.29022408 epoch total loss 1.38821805\n",
      "Trained batch 850 batch loss 1.37770653 epoch total loss 1.38820577\n",
      "Trained batch 851 batch loss 1.7293551 epoch total loss 1.38860667\n",
      "Trained batch 852 batch loss 1.49020624 epoch total loss 1.38872588\n",
      "Trained batch 853 batch loss 1.32743144 epoch total loss 1.38865399\n",
      "Trained batch 854 batch loss 1.35398769 epoch total loss 1.38861346\n",
      "Trained batch 855 batch loss 1.49847 epoch total loss 1.38874185\n",
      "Trained batch 856 batch loss 1.41590297 epoch total loss 1.38877356\n",
      "Trained batch 857 batch loss 1.40119731 epoch total loss 1.3887881\n",
      "Trained batch 858 batch loss 1.38685691 epoch total loss 1.38878584\n",
      "Trained batch 859 batch loss 1.40425456 epoch total loss 1.38880384\n",
      "Trained batch 860 batch loss 1.3697859 epoch total loss 1.38878179\n",
      "Trained batch 861 batch loss 1.41680896 epoch total loss 1.38881421\n",
      "Trained batch 862 batch loss 1.35543823 epoch total loss 1.38877559\n",
      "Trained batch 863 batch loss 1.28190863 epoch total loss 1.38865161\n",
      "Trained batch 864 batch loss 1.28679121 epoch total loss 1.38853371\n",
      "Trained batch 865 batch loss 1.39025593 epoch total loss 1.38853574\n",
      "Trained batch 866 batch loss 1.29964304 epoch total loss 1.3884331\n",
      "Trained batch 867 batch loss 1.27989292 epoch total loss 1.38830793\n",
      "Trained batch 868 batch loss 1.26172733 epoch total loss 1.38816214\n",
      "Trained batch 869 batch loss 1.3855629 epoch total loss 1.38815916\n",
      "Trained batch 870 batch loss 1.3726368 epoch total loss 1.38814139\n",
      "Trained batch 871 batch loss 1.38627815 epoch total loss 1.38813913\n",
      "Trained batch 872 batch loss 1.35991728 epoch total loss 1.3881067\n",
      "Trained batch 873 batch loss 1.33539057 epoch total loss 1.38804638\n",
      "Trained batch 874 batch loss 1.38630557 epoch total loss 1.38804448\n",
      "Trained batch 875 batch loss 1.38290358 epoch total loss 1.38803864\n",
      "Trained batch 876 batch loss 1.35565329 epoch total loss 1.3880018\n",
      "Trained batch 877 batch loss 1.27915442 epoch total loss 1.3878777\n",
      "Trained batch 878 batch loss 1.32143462 epoch total loss 1.387802\n",
      "Trained batch 879 batch loss 1.43175936 epoch total loss 1.38785195\n",
      "Trained batch 880 batch loss 1.38669229 epoch total loss 1.38785064\n",
      "Trained batch 881 batch loss 1.44455218 epoch total loss 1.38791502\n",
      "Trained batch 882 batch loss 1.30706954 epoch total loss 1.38782346\n",
      "Trained batch 883 batch loss 1.31049848 epoch total loss 1.38773596\n",
      "Trained batch 884 batch loss 1.18899393 epoch total loss 1.38751113\n",
      "Trained batch 885 batch loss 1.28774023 epoch total loss 1.38739836\n",
      "Trained batch 886 batch loss 1.28917241 epoch total loss 1.3872875\n",
      "Trained batch 887 batch loss 1.35413861 epoch total loss 1.38725007\n",
      "Trained batch 888 batch loss 1.47158432 epoch total loss 1.38734508\n",
      "Trained batch 889 batch loss 1.34829009 epoch total loss 1.38730109\n",
      "Trained batch 890 batch loss 1.42153621 epoch total loss 1.38733947\n",
      "Trained batch 891 batch loss 1.41667652 epoch total loss 1.38737237\n",
      "Trained batch 892 batch loss 1.30929613 epoch total loss 1.38728487\n",
      "Trained batch 893 batch loss 1.37598729 epoch total loss 1.38727224\n",
      "Trained batch 894 batch loss 1.45211792 epoch total loss 1.38734484\n",
      "Trained batch 895 batch loss 1.30724156 epoch total loss 1.38725531\n",
      "Trained batch 896 batch loss 1.39119303 epoch total loss 1.38725972\n",
      "Trained batch 897 batch loss 1.39658809 epoch total loss 1.38727021\n",
      "Trained batch 898 batch loss 1.25069439 epoch total loss 1.3871181\n",
      "Trained batch 899 batch loss 1.32962608 epoch total loss 1.38705409\n",
      "Trained batch 900 batch loss 1.22947645 epoch total loss 1.38687909\n",
      "Trained batch 901 batch loss 1.26231015 epoch total loss 1.3867408\n",
      "Trained batch 902 batch loss 1.4796505 epoch total loss 1.3868438\n",
      "Trained batch 903 batch loss 1.45421743 epoch total loss 1.38691843\n",
      "Trained batch 904 batch loss 1.55819142 epoch total loss 1.38710797\n",
      "Trained batch 905 batch loss 1.51820636 epoch total loss 1.38725281\n",
      "Trained batch 906 batch loss 1.48307252 epoch total loss 1.38735843\n",
      "Trained batch 907 batch loss 1.26650596 epoch total loss 1.38722515\n",
      "Trained batch 908 batch loss 1.10470438 epoch total loss 1.38691413\n",
      "Trained batch 909 batch loss 1.20474839 epoch total loss 1.38671362\n",
      "Trained batch 910 batch loss 1.11829031 epoch total loss 1.3864187\n",
      "Trained batch 911 batch loss 1.21894646 epoch total loss 1.38623488\n",
      "Trained batch 912 batch loss 1.33145571 epoch total loss 1.3861748\n",
      "Trained batch 913 batch loss 1.27347422 epoch total loss 1.3860513\n",
      "Trained batch 914 batch loss 1.31416798 epoch total loss 1.38597274\n",
      "Trained batch 915 batch loss 1.42311156 epoch total loss 1.38601327\n",
      "Trained batch 916 batch loss 1.46239078 epoch total loss 1.38609672\n",
      "Trained batch 917 batch loss 1.4281019 epoch total loss 1.38614249\n",
      "Trained batch 918 batch loss 1.33869743 epoch total loss 1.38609087\n",
      "Trained batch 919 batch loss 1.29406238 epoch total loss 1.38599074\n",
      "Trained batch 920 batch loss 1.37938941 epoch total loss 1.38598359\n",
      "Trained batch 921 batch loss 1.34424829 epoch total loss 1.38593817\n",
      "Trained batch 922 batch loss 1.43405402 epoch total loss 1.38599038\n",
      "Trained batch 923 batch loss 1.405527 epoch total loss 1.3860116\n",
      "Trained batch 924 batch loss 1.59555733 epoch total loss 1.38623834\n",
      "Trained batch 925 batch loss 1.50974631 epoch total loss 1.38637197\n",
      "Trained batch 926 batch loss 1.47441435 epoch total loss 1.38646698\n",
      "Trained batch 927 batch loss 1.52550745 epoch total loss 1.38661695\n",
      "Trained batch 928 batch loss 1.4909128 epoch total loss 1.38672936\n",
      "Trained batch 929 batch loss 1.45444942 epoch total loss 1.38680232\n",
      "Trained batch 930 batch loss 1.52701092 epoch total loss 1.386953\n",
      "Trained batch 931 batch loss 1.45563293 epoch total loss 1.38702691\n",
      "Trained batch 932 batch loss 1.27045608 epoch total loss 1.38690186\n",
      "Trained batch 933 batch loss 1.41522 epoch total loss 1.38693213\n",
      "Trained batch 934 batch loss 1.4705236 epoch total loss 1.38702166\n",
      "Trained batch 935 batch loss 1.45854568 epoch total loss 1.38709819\n",
      "Trained batch 936 batch loss 1.40805984 epoch total loss 1.3871206\n",
      "Trained batch 937 batch loss 1.45437539 epoch total loss 1.38719237\n",
      "Trained batch 938 batch loss 1.48967576 epoch total loss 1.38730156\n",
      "Trained batch 939 batch loss 1.33321798 epoch total loss 1.38724399\n",
      "Trained batch 940 batch loss 1.35308969 epoch total loss 1.38720763\n",
      "Trained batch 941 batch loss 1.30705178 epoch total loss 1.38712239\n",
      "Trained batch 942 batch loss 1.16655302 epoch total loss 1.38688827\n",
      "Trained batch 943 batch loss 1.24166942 epoch total loss 1.38673425\n",
      "Trained batch 944 batch loss 1.21428978 epoch total loss 1.3865515\n",
      "Trained batch 945 batch loss 1.26775491 epoch total loss 1.38642573\n",
      "Trained batch 946 batch loss 1.11540556 epoch total loss 1.38613927\n",
      "Trained batch 947 batch loss 1.06555569 epoch total loss 1.38580072\n",
      "Trained batch 948 batch loss 1.07290244 epoch total loss 1.38547063\n",
      "Trained batch 949 batch loss 1.14163172 epoch total loss 1.38521361\n",
      "Trained batch 950 batch loss 1.33358252 epoch total loss 1.38515937\n",
      "Trained batch 951 batch loss 1.32824421 epoch total loss 1.38509953\n",
      "Trained batch 952 batch loss 1.30670416 epoch total loss 1.38501716\n",
      "Trained batch 953 batch loss 1.31451392 epoch total loss 1.38494313\n",
      "Trained batch 954 batch loss 1.42182326 epoch total loss 1.38498187\n",
      "Trained batch 955 batch loss 1.39509153 epoch total loss 1.38499248\n",
      "Trained batch 956 batch loss 1.31171596 epoch total loss 1.38491595\n",
      "Trained batch 957 batch loss 1.29266679 epoch total loss 1.38481963\n",
      "Trained batch 958 batch loss 1.3353126 epoch total loss 1.38476789\n",
      "Trained batch 959 batch loss 1.3140198 epoch total loss 1.3846941\n",
      "Trained batch 960 batch loss 1.33317518 epoch total loss 1.38464034\n",
      "Trained batch 961 batch loss 1.44778883 epoch total loss 1.38470602\n",
      "Trained batch 962 batch loss 1.3769536 epoch total loss 1.38469803\n",
      "Trained batch 963 batch loss 1.27531469 epoch total loss 1.38458431\n",
      "Trained batch 964 batch loss 1.4168812 epoch total loss 1.38461781\n",
      "Trained batch 965 batch loss 1.28050148 epoch total loss 1.38450992\n",
      "Trained batch 966 batch loss 1.35017943 epoch total loss 1.38447452\n",
      "Trained batch 967 batch loss 1.31156325 epoch total loss 1.38439906\n",
      "Trained batch 968 batch loss 1.32397985 epoch total loss 1.38433659\n",
      "Trained batch 969 batch loss 1.39114583 epoch total loss 1.38434362\n",
      "Trained batch 970 batch loss 1.38290524 epoch total loss 1.38434219\n",
      "Trained batch 971 batch loss 1.42135108 epoch total loss 1.38438034\n",
      "Trained batch 972 batch loss 1.32573104 epoch total loss 1.3843199\n",
      "Trained batch 973 batch loss 1.4168731 epoch total loss 1.3843534\n",
      "Trained batch 974 batch loss 1.35447824 epoch total loss 1.38432276\n",
      "Trained batch 975 batch loss 1.42098975 epoch total loss 1.38436031\n",
      "Trained batch 976 batch loss 1.25871587 epoch total loss 1.38423157\n",
      "Trained batch 977 batch loss 1.37304306 epoch total loss 1.38422012\n",
      "Trained batch 978 batch loss 1.32208669 epoch total loss 1.3841567\n",
      "Trained batch 979 batch loss 1.27765536 epoch total loss 1.38404787\n",
      "Trained batch 980 batch loss 1.40190387 epoch total loss 1.3840661\n",
      "Trained batch 981 batch loss 1.17174184 epoch total loss 1.38384962\n",
      "Trained batch 982 batch loss 1.18315291 epoch total loss 1.3836453\n",
      "Trained batch 983 batch loss 1.44663608 epoch total loss 1.38370931\n",
      "Trained batch 984 batch loss 1.45988202 epoch total loss 1.38378668\n",
      "Trained batch 985 batch loss 1.37554145 epoch total loss 1.38377833\n",
      "Trained batch 986 batch loss 1.28510869 epoch total loss 1.38367832\n",
      "Trained batch 987 batch loss 1.20533741 epoch total loss 1.3834976\n",
      "Trained batch 988 batch loss 1.2250917 epoch total loss 1.38333726\n",
      "Trained batch 989 batch loss 1.23791182 epoch total loss 1.38319016\n",
      "Trained batch 990 batch loss 1.22444272 epoch total loss 1.38302994\n",
      "Trained batch 991 batch loss 1.15666974 epoch total loss 1.38280141\n",
      "Trained batch 992 batch loss 1.32654381 epoch total loss 1.38274467\n",
      "Trained batch 993 batch loss 1.22890759 epoch total loss 1.3825897\n",
      "Trained batch 994 batch loss 1.34380674 epoch total loss 1.38255072\n",
      "Trained batch 995 batch loss 1.41080165 epoch total loss 1.38257909\n",
      "Trained batch 996 batch loss 1.34137559 epoch total loss 1.38253772\n",
      "Trained batch 997 batch loss 1.32899034 epoch total loss 1.38248396\n",
      "Trained batch 998 batch loss 1.33607936 epoch total loss 1.38243747\n",
      "Trained batch 999 batch loss 1.37194896 epoch total loss 1.38242698\n",
      "Trained batch 1000 batch loss 1.3440218 epoch total loss 1.38238859\n",
      "Trained batch 1001 batch loss 1.09704971 epoch total loss 1.38210344\n",
      "Trained batch 1002 batch loss 1.19120336 epoch total loss 1.38191295\n",
      "Trained batch 1003 batch loss 1.31329811 epoch total loss 1.38184452\n",
      "Trained batch 1004 batch loss 1.26054358 epoch total loss 1.38172376\n",
      "Trained batch 1005 batch loss 1.23824084 epoch total loss 1.38158095\n",
      "Trained batch 1006 batch loss 1.259866 epoch total loss 1.38146007\n",
      "Trained batch 1007 batch loss 1.27285171 epoch total loss 1.38135219\n",
      "Trained batch 1008 batch loss 1.43238473 epoch total loss 1.38140273\n",
      "Trained batch 1009 batch loss 1.37218022 epoch total loss 1.38139367\n",
      "Trained batch 1010 batch loss 1.44689941 epoch total loss 1.38145852\n",
      "Trained batch 1011 batch loss 1.28322947 epoch total loss 1.38136125\n",
      "Trained batch 1012 batch loss 1.25092 epoch total loss 1.3812325\n",
      "Trained batch 1013 batch loss 1.23317266 epoch total loss 1.38108623\n",
      "Trained batch 1014 batch loss 1.26566505 epoch total loss 1.38097239\n",
      "Trained batch 1015 batch loss 1.45645642 epoch total loss 1.38104677\n",
      "Trained batch 1016 batch loss 1.43950725 epoch total loss 1.38110423\n",
      "Trained batch 1017 batch loss 1.41138756 epoch total loss 1.38113403\n",
      "Trained batch 1018 batch loss 1.28943861 epoch total loss 1.38104391\n",
      "Trained batch 1019 batch loss 1.29578018 epoch total loss 1.38096023\n",
      "Trained batch 1020 batch loss 1.36676311 epoch total loss 1.3809464\n",
      "Trained batch 1021 batch loss 1.30701125 epoch total loss 1.38087392\n",
      "Trained batch 1022 batch loss 1.28335452 epoch total loss 1.38077855\n",
      "Trained batch 1023 batch loss 1.2862947 epoch total loss 1.38068616\n",
      "Trained batch 1024 batch loss 1.27620244 epoch total loss 1.38058412\n",
      "Trained batch 1025 batch loss 1.26448846 epoch total loss 1.38047087\n",
      "Trained batch 1026 batch loss 1.26762474 epoch total loss 1.38036084\n",
      "Trained batch 1027 batch loss 1.32966518 epoch total loss 1.38031149\n",
      "Trained batch 1028 batch loss 1.4434247 epoch total loss 1.380373\n",
      "Trained batch 1029 batch loss 1.5963223 epoch total loss 1.38058281\n",
      "Trained batch 1030 batch loss 1.45037913 epoch total loss 1.38065064\n",
      "Trained batch 1031 batch loss 1.45029593 epoch total loss 1.38071823\n",
      "Trained batch 1032 batch loss 1.3034997 epoch total loss 1.38064337\n",
      "Trained batch 1033 batch loss 1.29769683 epoch total loss 1.38056314\n",
      "Trained batch 1034 batch loss 1.27555692 epoch total loss 1.38046157\n",
      "Trained batch 1035 batch loss 1.29716873 epoch total loss 1.38038099\n",
      "Trained batch 1036 batch loss 1.37781882 epoch total loss 1.38037848\n",
      "Trained batch 1037 batch loss 1.30997789 epoch total loss 1.38031054\n",
      "Trained batch 1038 batch loss 1.4080236 epoch total loss 1.38033736\n",
      "Trained batch 1039 batch loss 1.33674097 epoch total loss 1.3802954\n",
      "Trained batch 1040 batch loss 1.33739781 epoch total loss 1.38025415\n",
      "Trained batch 1041 batch loss 1.36976254 epoch total loss 1.38024414\n",
      "Trained batch 1042 batch loss 1.28514886 epoch total loss 1.38015282\n",
      "Trained batch 1043 batch loss 1.3081696 epoch total loss 1.38008392\n",
      "Trained batch 1044 batch loss 1.27796221 epoch total loss 1.37998605\n",
      "Trained batch 1045 batch loss 1.14067316 epoch total loss 1.37975705\n",
      "Trained batch 1046 batch loss 1.20977163 epoch total loss 1.37959445\n",
      "Trained batch 1047 batch loss 1.34310675 epoch total loss 1.37955964\n",
      "Trained batch 1048 batch loss 1.29739952 epoch total loss 1.3794812\n",
      "Trained batch 1049 batch loss 1.35085821 epoch total loss 1.3794539\n",
      "Trained batch 1050 batch loss 1.32079744 epoch total loss 1.37939799\n",
      "Trained batch 1051 batch loss 1.30634499 epoch total loss 1.37932861\n",
      "Trained batch 1052 batch loss 1.37867737 epoch total loss 1.37932789\n",
      "Trained batch 1053 batch loss 1.36757827 epoch total loss 1.37931669\n",
      "Trained batch 1054 batch loss 1.34436154 epoch total loss 1.37928355\n",
      "Trained batch 1055 batch loss 1.40303302 epoch total loss 1.37930608\n",
      "Trained batch 1056 batch loss 1.29871845 epoch total loss 1.37922978\n",
      "Trained batch 1057 batch loss 1.29244041 epoch total loss 1.37914777\n",
      "Trained batch 1058 batch loss 1.23065376 epoch total loss 1.37900746\n",
      "Trained batch 1059 batch loss 1.14873552 epoch total loss 1.3787899\n",
      "Trained batch 1060 batch loss 1.34563386 epoch total loss 1.37875867\n",
      "Trained batch 1061 batch loss 1.2931726 epoch total loss 1.37867796\n",
      "Trained batch 1062 batch loss 1.39507592 epoch total loss 1.37869334\n",
      "Trained batch 1063 batch loss 1.25903583 epoch total loss 1.37858081\n",
      "Trained batch 1064 batch loss 1.32185221 epoch total loss 1.37852752\n",
      "Trained batch 1065 batch loss 1.348773 epoch total loss 1.37849963\n",
      "Trained batch 1066 batch loss 1.29541683 epoch total loss 1.37842166\n",
      "Trained batch 1067 batch loss 1.22398818 epoch total loss 1.37827694\n",
      "Trained batch 1068 batch loss 1.22936368 epoch total loss 1.37813747\n",
      "Trained batch 1069 batch loss 1.17244387 epoch total loss 1.37794507\n",
      "Trained batch 1070 batch loss 1.20000792 epoch total loss 1.37777877\n",
      "Trained batch 1071 batch loss 1.30202651 epoch total loss 1.37770796\n",
      "Trained batch 1072 batch loss 1.21650231 epoch total loss 1.37755764\n",
      "Trained batch 1073 batch loss 1.33526635 epoch total loss 1.3775183\n",
      "Trained batch 1074 batch loss 1.27280796 epoch total loss 1.37742078\n",
      "Trained batch 1075 batch loss 1.18887031 epoch total loss 1.37724543\n",
      "Trained batch 1076 batch loss 1.31697178 epoch total loss 1.3771894\n",
      "Trained batch 1077 batch loss 1.35621428 epoch total loss 1.37717\n",
      "Trained batch 1078 batch loss 1.39657068 epoch total loss 1.37718797\n",
      "Trained batch 1079 batch loss 1.47578931 epoch total loss 1.3772794\n",
      "Trained batch 1080 batch loss 1.36116529 epoch total loss 1.3772645\n",
      "Trained batch 1081 batch loss 1.41360176 epoch total loss 1.37729812\n",
      "Trained batch 1082 batch loss 1.30368567 epoch total loss 1.37723\n",
      "Trained batch 1083 batch loss 1.33713329 epoch total loss 1.37719309\n",
      "Trained batch 1084 batch loss 1.29916918 epoch total loss 1.37712109\n",
      "Trained batch 1085 batch loss 1.24293971 epoch total loss 1.37699747\n",
      "Trained batch 1086 batch loss 1.30484438 epoch total loss 1.37693095\n",
      "Trained batch 1087 batch loss 1.29570436 epoch total loss 1.37685621\n",
      "Trained batch 1088 batch loss 1.33698487 epoch total loss 1.37681961\n",
      "Trained batch 1089 batch loss 1.34293342 epoch total loss 1.3767885\n",
      "Trained batch 1090 batch loss 1.28775096 epoch total loss 1.37670672\n",
      "Trained batch 1091 batch loss 1.2387253 epoch total loss 1.37658036\n",
      "Trained batch 1092 batch loss 1.37340808 epoch total loss 1.37657738\n",
      "Trained batch 1093 batch loss 1.32538915 epoch total loss 1.37653065\n",
      "Trained batch 1094 batch loss 1.40236735 epoch total loss 1.37655425\n",
      "Trained batch 1095 batch loss 1.40574348 epoch total loss 1.37658095\n",
      "Trained batch 1096 batch loss 1.19792473 epoch total loss 1.37641788\n",
      "Trained batch 1097 batch loss 1.23834288 epoch total loss 1.37629199\n",
      "Trained batch 1098 batch loss 1.31930554 epoch total loss 1.37624013\n",
      "Trained batch 1099 batch loss 1.41228688 epoch total loss 1.37627292\n",
      "Trained batch 1100 batch loss 1.40153527 epoch total loss 1.3762958\n",
      "Trained batch 1101 batch loss 1.37527645 epoch total loss 1.37629485\n",
      "Trained batch 1102 batch loss 1.51207888 epoch total loss 1.37641811\n",
      "Trained batch 1103 batch loss 1.41492271 epoch total loss 1.37645304\n",
      "Trained batch 1104 batch loss 1.41616762 epoch total loss 1.37648892\n",
      "Trained batch 1105 batch loss 1.28107834 epoch total loss 1.37640262\n",
      "Trained batch 1106 batch loss 1.31993032 epoch total loss 1.37635159\n",
      "Trained batch 1107 batch loss 1.31392741 epoch total loss 1.37629521\n",
      "Trained batch 1108 batch loss 1.30264974 epoch total loss 1.37622869\n",
      "Trained batch 1109 batch loss 1.33152843 epoch total loss 1.3761884\n",
      "Trained batch 1110 batch loss 1.27559972 epoch total loss 1.37609792\n",
      "Trained batch 1111 batch loss 1.30226922 epoch total loss 1.3760314\n",
      "Trained batch 1112 batch loss 1.27033627 epoch total loss 1.37593639\n",
      "Trained batch 1113 batch loss 1.34705353 epoch total loss 1.3759104\n",
      "Trained batch 1114 batch loss 1.32274151 epoch total loss 1.37586272\n",
      "Trained batch 1115 batch loss 1.26731801 epoch total loss 1.37576532\n",
      "Trained batch 1116 batch loss 1.35492039 epoch total loss 1.37574673\n",
      "Trained batch 1117 batch loss 1.42899776 epoch total loss 1.37579441\n",
      "Trained batch 1118 batch loss 1.27493489 epoch total loss 1.37570417\n",
      "Trained batch 1119 batch loss 1.34360147 epoch total loss 1.37567544\n",
      "Trained batch 1120 batch loss 1.31523275 epoch total loss 1.37562144\n",
      "Trained batch 1121 batch loss 1.35054171 epoch total loss 1.37559915\n",
      "Trained batch 1122 batch loss 1.47826087 epoch total loss 1.3756907\n",
      "Trained batch 1123 batch loss 1.38854659 epoch total loss 1.37570214\n",
      "Trained batch 1124 batch loss 1.26865411 epoch total loss 1.37560689\n",
      "Trained batch 1125 batch loss 1.42773128 epoch total loss 1.37565327\n",
      "Trained batch 1126 batch loss 1.2006855 epoch total loss 1.37549782\n",
      "Trained batch 1127 batch loss 1.36344576 epoch total loss 1.37548709\n",
      "Trained batch 1128 batch loss 1.49317431 epoch total loss 1.3755914\n",
      "Trained batch 1129 batch loss 1.3559916 epoch total loss 1.37557399\n",
      "Trained batch 1130 batch loss 1.35751092 epoch total loss 1.37555802\n",
      "Trained batch 1131 batch loss 1.34206319 epoch total loss 1.37552845\n",
      "Trained batch 1132 batch loss 1.39429843 epoch total loss 1.37554502\n",
      "Trained batch 1133 batch loss 1.24169648 epoch total loss 1.37542689\n",
      "Trained batch 1134 batch loss 1.25785375 epoch total loss 1.37532318\n",
      "Trained batch 1135 batch loss 1.27636337 epoch total loss 1.37523592\n",
      "Trained batch 1136 batch loss 1.34135413 epoch total loss 1.37520611\n",
      "Trained batch 1137 batch loss 1.35325384 epoch total loss 1.3751868\n",
      "Trained batch 1138 batch loss 1.26581395 epoch total loss 1.37509072\n",
      "Trained batch 1139 batch loss 1.32825959 epoch total loss 1.37504959\n",
      "Trained batch 1140 batch loss 1.3814764 epoch total loss 1.37505519\n",
      "Trained batch 1141 batch loss 1.24676776 epoch total loss 1.3749429\n",
      "Trained batch 1142 batch loss 1.35375786 epoch total loss 1.3749243\n",
      "Trained batch 1143 batch loss 1.36028731 epoch total loss 1.37491143\n",
      "Trained batch 1144 batch loss 1.31874239 epoch total loss 1.37486231\n",
      "Trained batch 1145 batch loss 1.35487819 epoch total loss 1.37484491\n",
      "Trained batch 1146 batch loss 1.32009804 epoch total loss 1.37479711\n",
      "Trained batch 1147 batch loss 1.18768907 epoch total loss 1.37463403\n",
      "Trained batch 1148 batch loss 1.28269398 epoch total loss 1.37455392\n",
      "Trained batch 1149 batch loss 1.39090312 epoch total loss 1.3745681\n",
      "Trained batch 1150 batch loss 1.2964344 epoch total loss 1.37450016\n",
      "Trained batch 1151 batch loss 1.27175307 epoch total loss 1.37441087\n",
      "Trained batch 1152 batch loss 1.23221564 epoch total loss 1.37428737\n",
      "Trained batch 1153 batch loss 1.26870608 epoch total loss 1.37419581\n",
      "Trained batch 1154 batch loss 1.23452508 epoch total loss 1.3740747\n",
      "Trained batch 1155 batch loss 1.24500751 epoch total loss 1.373963\n",
      "Trained batch 1156 batch loss 1.33104861 epoch total loss 1.37392592\n",
      "Trained batch 1157 batch loss 1.42285 epoch total loss 1.37396812\n",
      "Trained batch 1158 batch loss 1.43414259 epoch total loss 1.3740201\n",
      "Trained batch 1159 batch loss 1.33404541 epoch total loss 1.37398553\n",
      "Trained batch 1160 batch loss 1.27536535 epoch total loss 1.37390053\n",
      "Trained batch 1161 batch loss 1.34487891 epoch total loss 1.3738755\n",
      "Trained batch 1162 batch loss 1.14809096 epoch total loss 1.37368119\n",
      "Trained batch 1163 batch loss 1.31768918 epoch total loss 1.37363303\n",
      "Trained batch 1164 batch loss 1.31188726 epoch total loss 1.3735801\n",
      "Trained batch 1165 batch loss 1.37598038 epoch total loss 1.37358212\n",
      "Trained batch 1166 batch loss 1.38868761 epoch total loss 1.373595\n",
      "Trained batch 1167 batch loss 1.31001818 epoch total loss 1.37354064\n",
      "Trained batch 1168 batch loss 1.22153831 epoch total loss 1.37341046\n",
      "Trained batch 1169 batch loss 1.29585707 epoch total loss 1.37334418\n",
      "Trained batch 1170 batch loss 1.24498332 epoch total loss 1.37323451\n",
      "Trained batch 1171 batch loss 1.13400888 epoch total loss 1.37303019\n",
      "Trained batch 1172 batch loss 1.13388884 epoch total loss 1.37282622\n",
      "Trained batch 1173 batch loss 1.24257898 epoch total loss 1.37271512\n",
      "Trained batch 1174 batch loss 1.48626554 epoch total loss 1.37281179\n",
      "Trained batch 1175 batch loss 1.41586661 epoch total loss 1.37284839\n",
      "Trained batch 1176 batch loss 1.42527473 epoch total loss 1.3728931\n",
      "Trained batch 1177 batch loss 1.48581016 epoch total loss 1.37298906\n",
      "Trained batch 1178 batch loss 1.36239958 epoch total loss 1.37298\n",
      "Trained batch 1179 batch loss 1.2735858 epoch total loss 1.37289572\n",
      "Trained batch 1180 batch loss 1.27881277 epoch total loss 1.37281597\n",
      "Trained batch 1181 batch loss 1.29469609 epoch total loss 1.37274981\n",
      "Trained batch 1182 batch loss 1.30498421 epoch total loss 1.37269247\n",
      "Trained batch 1183 batch loss 1.34476149 epoch total loss 1.37266874\n",
      "Trained batch 1184 batch loss 1.45620418 epoch total loss 1.37273932\n",
      "Trained batch 1185 batch loss 1.35789812 epoch total loss 1.3727268\n",
      "Trained batch 1186 batch loss 1.38823283 epoch total loss 1.37273979\n",
      "Trained batch 1187 batch loss 1.40119457 epoch total loss 1.37276387\n",
      "Trained batch 1188 batch loss 1.45003271 epoch total loss 1.37282896\n",
      "Trained batch 1189 batch loss 1.53664792 epoch total loss 1.37296665\n",
      "Trained batch 1190 batch loss 1.41104317 epoch total loss 1.37299871\n",
      "Trained batch 1191 batch loss 1.50229204 epoch total loss 1.37310719\n",
      "Trained batch 1192 batch loss 1.39253759 epoch total loss 1.37312353\n",
      "Trained batch 1193 batch loss 1.37226748 epoch total loss 1.37312293\n",
      "Trained batch 1194 batch loss 1.28581452 epoch total loss 1.37304974\n",
      "Trained batch 1195 batch loss 1.2709496 epoch total loss 1.37296438\n",
      "Trained batch 1196 batch loss 1.22804654 epoch total loss 1.37284315\n",
      "Trained batch 1197 batch loss 1.41225863 epoch total loss 1.37287605\n",
      "Trained batch 1198 batch loss 1.40843832 epoch total loss 1.37290573\n",
      "Trained batch 1199 batch loss 1.40357494 epoch total loss 1.37293136\n",
      "Trained batch 1200 batch loss 1.28140366 epoch total loss 1.37285507\n",
      "Trained batch 1201 batch loss 1.14146435 epoch total loss 1.37266243\n",
      "Trained batch 1202 batch loss 1.26800597 epoch total loss 1.3725754\n",
      "Trained batch 1203 batch loss 1.29816985 epoch total loss 1.37251353\n",
      "Trained batch 1204 batch loss 1.29796159 epoch total loss 1.37245166\n",
      "Trained batch 1205 batch loss 1.37522185 epoch total loss 1.37245393\n",
      "Trained batch 1206 batch loss 1.3580935 epoch total loss 1.37244213\n",
      "Trained batch 1207 batch loss 1.33923626 epoch total loss 1.37241459\n",
      "Trained batch 1208 batch loss 1.36151028 epoch total loss 1.37240553\n",
      "Trained batch 1209 batch loss 1.33081841 epoch total loss 1.37237108\n",
      "Trained batch 1210 batch loss 1.31923056 epoch total loss 1.37232721\n",
      "Trained batch 1211 batch loss 1.29066753 epoch total loss 1.37225974\n",
      "Trained batch 1212 batch loss 1.27560735 epoch total loss 1.37218\n",
      "Trained batch 1213 batch loss 1.40143716 epoch total loss 1.37220418\n",
      "Trained batch 1214 batch loss 1.33219087 epoch total loss 1.37217116\n",
      "Trained batch 1215 batch loss 1.37955296 epoch total loss 1.37217724\n",
      "Trained batch 1216 batch loss 1.38440144 epoch total loss 1.37218726\n",
      "Trained batch 1217 batch loss 1.3520782 epoch total loss 1.37217069\n",
      "Trained batch 1218 batch loss 1.30172467 epoch total loss 1.37211287\n",
      "Trained batch 1219 batch loss 1.23460972 epoch total loss 1.3720001\n",
      "Trained batch 1220 batch loss 1.17722476 epoch total loss 1.37184048\n",
      "Trained batch 1221 batch loss 1.27611089 epoch total loss 1.37176204\n",
      "Trained batch 1222 batch loss 1.24665523 epoch total loss 1.37165976\n",
      "Trained batch 1223 batch loss 1.15982378 epoch total loss 1.37148654\n",
      "Trained batch 1224 batch loss 1.22053206 epoch total loss 1.37136316\n",
      "Trained batch 1225 batch loss 1.17198026 epoch total loss 1.37120044\n",
      "Trained batch 1226 batch loss 1.29986322 epoch total loss 1.37114227\n",
      "Trained batch 1227 batch loss 1.20540321 epoch total loss 1.3710072\n",
      "Trained batch 1228 batch loss 1.36335981 epoch total loss 1.37100101\n",
      "Trained batch 1229 batch loss 1.4934144 epoch total loss 1.37110054\n",
      "Trained batch 1230 batch loss 1.20713091 epoch total loss 1.37096727\n",
      "Trained batch 1231 batch loss 1.39101398 epoch total loss 1.3709836\n",
      "Trained batch 1232 batch loss 1.34055829 epoch total loss 1.37095892\n",
      "Trained batch 1233 batch loss 1.30796051 epoch total loss 1.37090778\n",
      "Trained batch 1234 batch loss 1.29788518 epoch total loss 1.37084866\n",
      "Trained batch 1235 batch loss 1.39954972 epoch total loss 1.3708719\n",
      "Trained batch 1236 batch loss 1.42605686 epoch total loss 1.37091649\n",
      "Trained batch 1237 batch loss 1.3536377 epoch total loss 1.37090254\n",
      "Trained batch 1238 batch loss 1.25519443 epoch total loss 1.37080908\n",
      "Trained batch 1239 batch loss 1.2564187 epoch total loss 1.37071681\n",
      "Trained batch 1240 batch loss 1.3448292 epoch total loss 1.37069595\n",
      "Trained batch 1241 batch loss 1.24730849 epoch total loss 1.37059653\n",
      "Trained batch 1242 batch loss 1.27419055 epoch total loss 1.37051892\n",
      "Trained batch 1243 batch loss 1.2224555 epoch total loss 1.37039971\n",
      "Trained batch 1244 batch loss 1.29554129 epoch total loss 1.37033951\n",
      "Trained batch 1245 batch loss 1.22245789 epoch total loss 1.37022078\n",
      "Trained batch 1246 batch loss 1.3341465 epoch total loss 1.37019169\n",
      "Trained batch 1247 batch loss 1.35395813 epoch total loss 1.37017882\n",
      "Trained batch 1248 batch loss 1.24451101 epoch total loss 1.37007809\n",
      "Trained batch 1249 batch loss 1.22829795 epoch total loss 1.36996448\n",
      "Trained batch 1250 batch loss 1.29392385 epoch total loss 1.36990368\n",
      "Trained batch 1251 batch loss 1.33414364 epoch total loss 1.36987507\n",
      "Trained batch 1252 batch loss 1.33115339 epoch total loss 1.3698442\n",
      "Trained batch 1253 batch loss 1.33709073 epoch total loss 1.36981797\n",
      "Trained batch 1254 batch loss 1.30454111 epoch total loss 1.369766\n",
      "Trained batch 1255 batch loss 1.21393561 epoch total loss 1.3696419\n",
      "Trained batch 1256 batch loss 1.17641187 epoch total loss 1.369488\n",
      "Trained batch 1257 batch loss 1.27613771 epoch total loss 1.36941373\n",
      "Trained batch 1258 batch loss 1.26376438 epoch total loss 1.36932969\n",
      "Trained batch 1259 batch loss 1.25920665 epoch total loss 1.36924219\n",
      "Trained batch 1260 batch loss 1.40900016 epoch total loss 1.36927378\n",
      "Trained batch 1261 batch loss 1.39626193 epoch total loss 1.36929524\n",
      "Trained batch 1262 batch loss 1.29242301 epoch total loss 1.36923432\n",
      "Trained batch 1263 batch loss 1.37962186 epoch total loss 1.36924255\n",
      "Trained batch 1264 batch loss 1.29907537 epoch total loss 1.36918712\n",
      "Trained batch 1265 batch loss 1.34790587 epoch total loss 1.36917031\n",
      "Trained batch 1266 batch loss 1.42342508 epoch total loss 1.3692131\n",
      "Trained batch 1267 batch loss 1.37516 epoch total loss 1.36921775\n",
      "Trained batch 1268 batch loss 1.38246214 epoch total loss 1.36922824\n",
      "Trained batch 1269 batch loss 1.39573014 epoch total loss 1.36924911\n",
      "Trained batch 1270 batch loss 1.37317514 epoch total loss 1.3692522\n",
      "Trained batch 1271 batch loss 1.34649181 epoch total loss 1.36923432\n",
      "Trained batch 1272 batch loss 1.36517572 epoch total loss 1.3692311\n",
      "Trained batch 1273 batch loss 1.39318764 epoch total loss 1.36924994\n",
      "Trained batch 1274 batch loss 1.30643821 epoch total loss 1.36920059\n",
      "Trained batch 1275 batch loss 1.3538152 epoch total loss 1.36918855\n",
      "Trained batch 1276 batch loss 1.31603622 epoch total loss 1.36914682\n",
      "Trained batch 1277 batch loss 1.29573464 epoch total loss 1.36908937\n",
      "Trained batch 1278 batch loss 1.31052923 epoch total loss 1.36904359\n",
      "Trained batch 1279 batch loss 1.16559899 epoch total loss 1.36888456\n",
      "Trained batch 1280 batch loss 1.27720404 epoch total loss 1.36881292\n",
      "Trained batch 1281 batch loss 1.27269959 epoch total loss 1.36873794\n",
      "Trained batch 1282 batch loss 1.37145853 epoch total loss 1.36874008\n",
      "Trained batch 1283 batch loss 1.35082459 epoch total loss 1.36872613\n",
      "Trained batch 1284 batch loss 1.29593945 epoch total loss 1.36866939\n",
      "Trained batch 1285 batch loss 1.25081301 epoch total loss 1.36857772\n",
      "Trained batch 1286 batch loss 1.24333429 epoch total loss 1.36848021\n",
      "Trained batch 1287 batch loss 1.23387289 epoch total loss 1.36837566\n",
      "Trained batch 1288 batch loss 1.31039155 epoch total loss 1.36833072\n",
      "Trained batch 1289 batch loss 1.32191777 epoch total loss 1.3682946\n",
      "Trained batch 1290 batch loss 1.1816783 epoch total loss 1.36815\n",
      "Trained batch 1291 batch loss 1.3248179 epoch total loss 1.36811638\n",
      "Trained batch 1292 batch loss 1.267187 epoch total loss 1.3680383\n",
      "Trained batch 1293 batch loss 1.29810762 epoch total loss 1.36798418\n",
      "Trained batch 1294 batch loss 1.25129747 epoch total loss 1.36789405\n",
      "Trained batch 1295 batch loss 1.32137883 epoch total loss 1.36785817\n",
      "Trained batch 1296 batch loss 1.25131071 epoch total loss 1.36776829\n",
      "Trained batch 1297 batch loss 1.31869864 epoch total loss 1.3677305\n",
      "Trained batch 1298 batch loss 1.21029305 epoch total loss 1.36760914\n",
      "Trained batch 1299 batch loss 1.29663956 epoch total loss 1.36755455\n",
      "Trained batch 1300 batch loss 1.35460544 epoch total loss 1.36754465\n",
      "Trained batch 1301 batch loss 1.29865289 epoch total loss 1.36749172\n",
      "Trained batch 1302 batch loss 1.34377027 epoch total loss 1.36747348\n",
      "Trained batch 1303 batch loss 1.29453051 epoch total loss 1.36741745\n",
      "Trained batch 1304 batch loss 1.31940413 epoch total loss 1.36738074\n",
      "Trained batch 1305 batch loss 1.36698377 epoch total loss 1.36738038\n",
      "Trained batch 1306 batch loss 1.31569767 epoch total loss 1.3673408\n",
      "Trained batch 1307 batch loss 1.18051136 epoch total loss 1.36719787\n",
      "Trained batch 1308 batch loss 1.25189579 epoch total loss 1.36710978\n",
      "Trained batch 1309 batch loss 1.23691142 epoch total loss 1.36701035\n",
      "Trained batch 1310 batch loss 1.21235442 epoch total loss 1.36689234\n",
      "Trained batch 1311 batch loss 1.23522937 epoch total loss 1.36679184\n",
      "Trained batch 1312 batch loss 1.33139205 epoch total loss 1.3667649\n",
      "Trained batch 1313 batch loss 1.26055431 epoch total loss 1.36668396\n",
      "Trained batch 1314 batch loss 1.30435503 epoch total loss 1.36663651\n",
      "Trained batch 1315 batch loss 1.25322235 epoch total loss 1.36655021\n",
      "Trained batch 1316 batch loss 1.36037529 epoch total loss 1.36654556\n",
      "Trained batch 1317 batch loss 1.4129703 epoch total loss 1.36658072\n",
      "Trained batch 1318 batch loss 1.32252824 epoch total loss 1.36654735\n",
      "Trained batch 1319 batch loss 1.25010419 epoch total loss 1.36645901\n",
      "Trained batch 1320 batch loss 1.16996348 epoch total loss 1.36631012\n",
      "Trained batch 1321 batch loss 1.12374341 epoch total loss 1.36612654\n",
      "Trained batch 1322 batch loss 1.09501719 epoch total loss 1.3659215\n",
      "Trained batch 1323 batch loss 1.07814121 epoch total loss 1.36570394\n",
      "Trained batch 1324 batch loss 1.02371967 epoch total loss 1.36544561\n",
      "Trained batch 1325 batch loss 1.18442225 epoch total loss 1.365309\n",
      "Trained batch 1326 batch loss 1.40958798 epoch total loss 1.36534238\n",
      "Trained batch 1327 batch loss 1.42648458 epoch total loss 1.36538851\n",
      "Trained batch 1328 batch loss 1.48619747 epoch total loss 1.36547947\n",
      "Trained batch 1329 batch loss 1.31965137 epoch total loss 1.36544502\n",
      "Trained batch 1330 batch loss 1.47141886 epoch total loss 1.36552465\n",
      "Trained batch 1331 batch loss 1.34770727 epoch total loss 1.3655113\n",
      "Trained batch 1332 batch loss 1.35703623 epoch total loss 1.36550486\n",
      "Trained batch 1333 batch loss 1.48933482 epoch total loss 1.36559784\n",
      "Trained batch 1334 batch loss 1.38329065 epoch total loss 1.36561108\n",
      "Trained batch 1335 batch loss 1.3028667 epoch total loss 1.36556411\n",
      "Trained batch 1336 batch loss 1.222229 epoch total loss 1.36545682\n",
      "Trained batch 1337 batch loss 1.28873968 epoch total loss 1.36539936\n",
      "Trained batch 1338 batch loss 1.14399743 epoch total loss 1.3652339\n",
      "Trained batch 1339 batch loss 1.34443951 epoch total loss 1.3652184\n",
      "Trained batch 1340 batch loss 1.36836767 epoch total loss 1.36522079\n",
      "Trained batch 1341 batch loss 1.34724879 epoch total loss 1.36520743\n",
      "Trained batch 1342 batch loss 1.38543499 epoch total loss 1.36522245\n",
      "Trained batch 1343 batch loss 1.3425231 epoch total loss 1.36520553\n",
      "Trained batch 1344 batch loss 1.3957541 epoch total loss 1.3652283\n",
      "Trained batch 1345 batch loss 1.34333861 epoch total loss 1.36521208\n",
      "Trained batch 1346 batch loss 1.2704736 epoch total loss 1.36514163\n",
      "Trained batch 1347 batch loss 1.3505578 epoch total loss 1.3651309\n",
      "Trained batch 1348 batch loss 1.29461646 epoch total loss 1.36507857\n",
      "Trained batch 1349 batch loss 1.29416227 epoch total loss 1.365026\n",
      "Trained batch 1350 batch loss 1.31545913 epoch total loss 1.36498928\n",
      "Trained batch 1351 batch loss 1.23676586 epoch total loss 1.36489439\n",
      "Trained batch 1352 batch loss 1.24117589 epoch total loss 1.36480284\n",
      "Trained batch 1353 batch loss 1.3799305 epoch total loss 1.36481404\n",
      "Trained batch 1354 batch loss 1.41573036 epoch total loss 1.36485171\n",
      "Trained batch 1355 batch loss 1.38552892 epoch total loss 1.36486685\n",
      "Trained batch 1356 batch loss 1.41793203 epoch total loss 1.36490607\n",
      "Trained batch 1357 batch loss 1.55033 epoch total loss 1.36504269\n",
      "Trained batch 1358 batch loss 1.33291268 epoch total loss 1.36501896\n",
      "Trained batch 1359 batch loss 1.377473 epoch total loss 1.36502814\n",
      "Trained batch 1360 batch loss 1.41231906 epoch total loss 1.36506295\n",
      "Trained batch 1361 batch loss 1.25322807 epoch total loss 1.3649807\n",
      "Trained batch 1362 batch loss 1.39588761 epoch total loss 1.36500347\n",
      "Trained batch 1363 batch loss 1.35228312 epoch total loss 1.36499405\n",
      "Trained batch 1364 batch loss 1.25441623 epoch total loss 1.36491299\n",
      "Trained batch 1365 batch loss 1.18005347 epoch total loss 1.36477757\n",
      "Trained batch 1366 batch loss 1.22377098 epoch total loss 1.36467433\n",
      "Trained batch 1367 batch loss 1.27903271 epoch total loss 1.36461174\n",
      "Trained batch 1368 batch loss 1.19373727 epoch total loss 1.36448681\n",
      "Trained batch 1369 batch loss 1.22017694 epoch total loss 1.36438143\n",
      "Trained batch 1370 batch loss 1.24167347 epoch total loss 1.36429191\n",
      "Trained batch 1371 batch loss 1.32155144 epoch total loss 1.36426067\n",
      "Trained batch 1372 batch loss 1.2493484 epoch total loss 1.36417699\n",
      "Trained batch 1373 batch loss 1.29892147 epoch total loss 1.36412942\n",
      "Trained batch 1374 batch loss 1.28691602 epoch total loss 1.36407316\n",
      "Trained batch 1375 batch loss 1.42487824 epoch total loss 1.3641175\n",
      "Trained batch 1376 batch loss 1.2876004 epoch total loss 1.36406183\n",
      "Trained batch 1377 batch loss 1.42089379 epoch total loss 1.36410308\n",
      "Trained batch 1378 batch loss 1.35847306 epoch total loss 1.36409903\n",
      "Trained batch 1379 batch loss 1.28612757 epoch total loss 1.36404252\n",
      "Trained batch 1380 batch loss 1.20816803 epoch total loss 1.36392951\n",
      "Trained batch 1381 batch loss 1.33716094 epoch total loss 1.3639102\n",
      "Trained batch 1382 batch loss 1.29731607 epoch total loss 1.36386204\n",
      "Trained batch 1383 batch loss 1.33757567 epoch total loss 1.36384296\n",
      "Trained batch 1384 batch loss 1.42589748 epoch total loss 1.36388779\n",
      "Trained batch 1385 batch loss 1.37201881 epoch total loss 1.36389375\n",
      "Trained batch 1386 batch loss 1.26775503 epoch total loss 1.36382437\n",
      "Trained batch 1387 batch loss 1.28467798 epoch total loss 1.36376727\n",
      "Trained batch 1388 batch loss 1.3005116 epoch total loss 1.36372173\n",
      "Epoch 2 train loss 1.3637217283248901\n",
      "Validated batch 1 batch loss 1.31528461\n",
      "Validated batch 2 batch loss 1.31608653\n",
      "Validated batch 3 batch loss 1.24403667\n",
      "Validated batch 4 batch loss 1.37752485\n",
      "Validated batch 5 batch loss 1.3136611\n",
      "Validated batch 6 batch loss 1.32671499\n",
      "Validated batch 7 batch loss 1.4058243\n",
      "Validated batch 8 batch loss 1.35082579\n",
      "Validated batch 9 batch loss 1.35125327\n",
      "Validated batch 10 batch loss 1.30433536\n",
      "Validated batch 11 batch loss 1.38951766\n",
      "Validated batch 12 batch loss 1.32555521\n",
      "Validated batch 13 batch loss 1.30697\n",
      "Validated batch 14 batch loss 1.4138937\n",
      "Validated batch 15 batch loss 1.3655107\n",
      "Validated batch 16 batch loss 1.32493424\n",
      "Validated batch 17 batch loss 1.4496361\n",
      "Validated batch 18 batch loss 1.18528795\n",
      "Validated batch 19 batch loss 1.35885406\n",
      "Validated batch 20 batch loss 1.14511919\n",
      "Validated batch 21 batch loss 1.33262753\n",
      "Validated batch 22 batch loss 1.41701603\n",
      "Validated batch 23 batch loss 1.24982798\n",
      "Validated batch 24 batch loss 1.40873897\n",
      "Validated batch 25 batch loss 1.32395172\n",
      "Validated batch 26 batch loss 1.25114036\n",
      "Validated batch 27 batch loss 1.22034729\n",
      "Validated batch 28 batch loss 1.25900567\n",
      "Validated batch 29 batch loss 1.3424952\n",
      "Validated batch 30 batch loss 1.27904582\n",
      "Validated batch 31 batch loss 1.26913905\n",
      "Validated batch 32 batch loss 1.33605909\n",
      "Validated batch 33 batch loss 1.36173666\n",
      "Validated batch 34 batch loss 1.23811841\n",
      "Validated batch 35 batch loss 1.21045232\n",
      "Validated batch 36 batch loss 1.34190106\n",
      "Validated batch 37 batch loss 1.36733985\n",
      "Validated batch 38 batch loss 1.44927049\n",
      "Validated batch 39 batch loss 1.41471112\n",
      "Validated batch 40 batch loss 1.28493381\n",
      "Validated batch 41 batch loss 1.4446063\n",
      "Validated batch 42 batch loss 1.32101381\n",
      "Validated batch 43 batch loss 1.34336865\n",
      "Validated batch 44 batch loss 1.37036133\n",
      "Validated batch 45 batch loss 1.10423851\n",
      "Validated batch 46 batch loss 1.34368706\n",
      "Validated batch 47 batch loss 1.27322531\n",
      "Validated batch 48 batch loss 1.21531785\n",
      "Validated batch 49 batch loss 1.28209567\n",
      "Validated batch 50 batch loss 1.24840367\n",
      "Validated batch 51 batch loss 1.30125701\n",
      "Validated batch 52 batch loss 1.30811405\n",
      "Validated batch 53 batch loss 1.32378423\n",
      "Validated batch 54 batch loss 1.26296937\n",
      "Validated batch 55 batch loss 1.33669806\n",
      "Validated batch 56 batch loss 1.30770659\n",
      "Validated batch 57 batch loss 1.33240497\n",
      "Validated batch 58 batch loss 1.37614989\n",
      "Validated batch 59 batch loss 1.25084496\n",
      "Validated batch 60 batch loss 1.25761294\n",
      "Validated batch 61 batch loss 1.34517145\n",
      "Validated batch 62 batch loss 1.28233588\n",
      "Validated batch 63 batch loss 1.43931317\n",
      "Validated batch 64 batch loss 1.33550262\n",
      "Validated batch 65 batch loss 1.20784247\n",
      "Validated batch 66 batch loss 1.37420058\n",
      "Validated batch 67 batch loss 1.30292463\n",
      "Validated batch 68 batch loss 1.23634815\n",
      "Validated batch 69 batch loss 1.33135533\n",
      "Validated batch 70 batch loss 1.2660712\n",
      "Validated batch 71 batch loss 1.35700083\n",
      "Validated batch 72 batch loss 1.27148747\n",
      "Validated batch 73 batch loss 1.18720949\n",
      "Validated batch 74 batch loss 1.23341537\n",
      "Validated batch 75 batch loss 1.39632094\n",
      "Validated batch 76 batch loss 1.24858665\n",
      "Validated batch 77 batch loss 1.21621466\n",
      "Validated batch 78 batch loss 1.29146183\n",
      "Validated batch 79 batch loss 1.29805303\n",
      "Validated batch 80 batch loss 1.21224236\n",
      "Validated batch 81 batch loss 1.32400346\n",
      "Validated batch 82 batch loss 1.30320215\n",
      "Validated batch 83 batch loss 1.24560905\n",
      "Validated batch 84 batch loss 1.35858572\n",
      "Validated batch 85 batch loss 1.41931224\n",
      "Validated batch 86 batch loss 1.26375031\n",
      "Validated batch 87 batch loss 1.4038862\n",
      "Validated batch 88 batch loss 1.19114697\n",
      "Validated batch 89 batch loss 1.25512099\n",
      "Validated batch 90 batch loss 1.2553035\n",
      "Validated batch 91 batch loss 1.31586683\n",
      "Validated batch 92 batch loss 1.45965576\n",
      "Validated batch 93 batch loss 1.22385776\n",
      "Validated batch 94 batch loss 1.32085299\n",
      "Validated batch 95 batch loss 1.38016236\n",
      "Validated batch 96 batch loss 1.25060678\n",
      "Validated batch 97 batch loss 1.36431158\n",
      "Validated batch 98 batch loss 1.41738629\n",
      "Validated batch 99 batch loss 1.14752293\n",
      "Validated batch 100 batch loss 1.25796843\n",
      "Validated batch 101 batch loss 1.24492145\n",
      "Validated batch 102 batch loss 1.34420395\n",
      "Validated batch 103 batch loss 1.31046712\n",
      "Validated batch 104 batch loss 1.20520616\n",
      "Validated batch 105 batch loss 1.10009122\n",
      "Validated batch 106 batch loss 1.29649282\n",
      "Validated batch 107 batch loss 1.28218234\n",
      "Validated batch 108 batch loss 1.30364656\n",
      "Validated batch 109 batch loss 1.28907502\n",
      "Validated batch 110 batch loss 1.22637141\n",
      "Validated batch 111 batch loss 1.31355739\n",
      "Validated batch 112 batch loss 1.38799334\n",
      "Validated batch 113 batch loss 1.31034613\n",
      "Validated batch 114 batch loss 1.29966426\n",
      "Validated batch 115 batch loss 1.17697966\n",
      "Validated batch 116 batch loss 1.33504868\n",
      "Validated batch 117 batch loss 1.32662129\n",
      "Validated batch 118 batch loss 1.30337024\n",
      "Validated batch 119 batch loss 1.29098332\n",
      "Validated batch 120 batch loss 1.31798971\n",
      "Validated batch 121 batch loss 1.43168616\n",
      "Validated batch 122 batch loss 1.24667883\n",
      "Validated batch 123 batch loss 1.35140514\n",
      "Validated batch 124 batch loss 1.30459893\n",
      "Validated batch 125 batch loss 1.35857296\n",
      "Validated batch 126 batch loss 1.33443141\n",
      "Validated batch 127 batch loss 1.22744679\n",
      "Validated batch 128 batch loss 1.35860562\n",
      "Validated batch 129 batch loss 1.37179518\n",
      "Validated batch 130 batch loss 1.35225224\n",
      "Validated batch 131 batch loss 1.30362499\n",
      "Validated batch 132 batch loss 1.32256317\n",
      "Validated batch 133 batch loss 1.25519419\n",
      "Validated batch 134 batch loss 1.27280688\n",
      "Validated batch 135 batch loss 1.30234933\n",
      "Validated batch 136 batch loss 1.23457098\n",
      "Validated batch 137 batch loss 1.3248235\n",
      "Validated batch 138 batch loss 1.30758202\n",
      "Validated batch 139 batch loss 1.46178508\n",
      "Validated batch 140 batch loss 1.31325412\n",
      "Validated batch 141 batch loss 1.29512429\n",
      "Validated batch 142 batch loss 1.29145217\n",
      "Validated batch 143 batch loss 1.24297154\n",
      "Validated batch 144 batch loss 1.31039417\n",
      "Validated batch 145 batch loss 1.30761087\n",
      "Validated batch 146 batch loss 1.25942099\n",
      "Validated batch 147 batch loss 1.25512326\n",
      "Validated batch 148 batch loss 1.31854892\n",
      "Validated batch 149 batch loss 1.28390348\n",
      "Validated batch 150 batch loss 1.35002434\n",
      "Validated batch 151 batch loss 1.32042933\n",
      "Validated batch 152 batch loss 1.21308935\n",
      "Validated batch 153 batch loss 1.28845155\n",
      "Validated batch 154 batch loss 1.32079959\n",
      "Validated batch 155 batch loss 1.28900671\n",
      "Validated batch 156 batch loss 1.3510679\n",
      "Validated batch 157 batch loss 1.36058772\n",
      "Validated batch 158 batch loss 1.51303\n",
      "Validated batch 159 batch loss 1.4579829\n",
      "Validated batch 160 batch loss 1.32534611\n",
      "Validated batch 161 batch loss 1.19752038\n",
      "Validated batch 162 batch loss 1.23312354\n",
      "Validated batch 163 batch loss 1.22402871\n",
      "Validated batch 164 batch loss 1.26928461\n",
      "Validated batch 165 batch loss 1.26573944\n",
      "Validated batch 166 batch loss 1.23834288\n",
      "Validated batch 167 batch loss 1.32651782\n",
      "Validated batch 168 batch loss 1.30631745\n",
      "Validated batch 169 batch loss 1.35306692\n",
      "Validated batch 170 batch loss 1.40290773\n",
      "Validated batch 171 batch loss 1.37588084\n",
      "Validated batch 172 batch loss 1.32034802\n",
      "Validated batch 173 batch loss 1.4014256\n",
      "Validated batch 174 batch loss 1.3635118\n",
      "Validated batch 175 batch loss 1.35468304\n",
      "Validated batch 176 batch loss 1.37030447\n",
      "Validated batch 177 batch loss 1.43465948\n",
      "Validated batch 178 batch loss 1.36058164\n",
      "Validated batch 179 batch loss 1.26045299\n",
      "Validated batch 180 batch loss 1.26972628\n",
      "Validated batch 181 batch loss 1.35233259\n",
      "Validated batch 182 batch loss 1.42441189\n",
      "Validated batch 183 batch loss 1.30886948\n",
      "Validated batch 184 batch loss 1.34655368\n",
      "Validated batch 185 batch loss 1.2764281\n",
      "Epoch 2 val loss 1.3103587627410889\n",
      "Model /aiffel/aiffel/mpii/mine/model-epoch-2-loss-1.3104.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.25335586 epoch total loss 1.25335586\n",
      "Trained batch 2 batch loss 1.17767477 epoch total loss 1.21551538\n",
      "Trained batch 3 batch loss 1.2371527 epoch total loss 1.22272778\n",
      "Trained batch 4 batch loss 1.26701915 epoch total loss 1.23380065\n",
      "Trained batch 5 batch loss 1.30248165 epoch total loss 1.2475369\n",
      "Trained batch 6 batch loss 1.26333344 epoch total loss 1.25016963\n",
      "Trained batch 7 batch loss 1.38412738 epoch total loss 1.26930642\n",
      "Trained batch 8 batch loss 1.40593064 epoch total loss 1.28638446\n",
      "Trained batch 9 batch loss 1.34147656 epoch total loss 1.29250574\n",
      "Trained batch 10 batch loss 1.3572588 epoch total loss 1.29898107\n",
      "Trained batch 11 batch loss 1.34968209 epoch total loss 1.3035903\n",
      "Trained batch 12 batch loss 1.30411303 epoch total loss 1.30363381\n",
      "Trained batch 13 batch loss 1.31756687 epoch total loss 1.30470574\n",
      "Trained batch 14 batch loss 1.40769053 epoch total loss 1.31206167\n",
      "Trained batch 15 batch loss 1.42720342 epoch total loss 1.31973791\n",
      "Trained batch 16 batch loss 1.26670408 epoch total loss 1.3164233\n",
      "Trained batch 17 batch loss 1.35428989 epoch total loss 1.31865072\n",
      "Trained batch 18 batch loss 1.51061404 epoch total loss 1.32931542\n",
      "Trained batch 19 batch loss 1.43643868 epoch total loss 1.33495355\n",
      "Trained batch 20 batch loss 1.34473562 epoch total loss 1.33544266\n",
      "Trained batch 21 batch loss 1.25950694 epoch total loss 1.33182657\n",
      "Trained batch 22 batch loss 1.31599498 epoch total loss 1.33110702\n",
      "Trained batch 23 batch loss 1.28108191 epoch total loss 1.32893193\n",
      "Trained batch 24 batch loss 1.30713582 epoch total loss 1.32802379\n",
      "Trained batch 25 batch loss 1.20906591 epoch total loss 1.32326555\n",
      "Trained batch 26 batch loss 1.12742054 epoch total loss 1.31573308\n",
      "Trained batch 27 batch loss 1.24652457 epoch total loss 1.31316984\n",
      "Trained batch 28 batch loss 1.30073357 epoch total loss 1.31272566\n",
      "Trained batch 29 batch loss 1.26805961 epoch total loss 1.31118536\n",
      "Trained batch 30 batch loss 1.32205164 epoch total loss 1.31154764\n",
      "Trained batch 31 batch loss 1.29556537 epoch total loss 1.31103206\n",
      "Trained batch 32 batch loss 1.22140741 epoch total loss 1.30823135\n",
      "Trained batch 33 batch loss 1.22258484 epoch total loss 1.30563593\n",
      "Trained batch 34 batch loss 1.38845301 epoch total loss 1.30807185\n",
      "Trained batch 35 batch loss 1.2705307 epoch total loss 1.30699921\n",
      "Trained batch 36 batch loss 1.32445014 epoch total loss 1.30748403\n",
      "Trained batch 37 batch loss 1.29570436 epoch total loss 1.30716562\n",
      "Trained batch 38 batch loss 1.24694 epoch total loss 1.30558074\n",
      "Trained batch 39 batch loss 1.17328238 epoch total loss 1.30218852\n",
      "Trained batch 40 batch loss 1.20210743 epoch total loss 1.29968643\n",
      "Trained batch 41 batch loss 1.32929337 epoch total loss 1.30040848\n",
      "Trained batch 42 batch loss 1.33621097 epoch total loss 1.30126095\n",
      "Trained batch 43 batch loss 1.34566677 epoch total loss 1.30229366\n",
      "Trained batch 44 batch loss 1.34542215 epoch total loss 1.3032738\n",
      "Trained batch 45 batch loss 1.35048759 epoch total loss 1.30432296\n",
      "Trained batch 46 batch loss 1.32031906 epoch total loss 1.30467069\n",
      "Trained batch 47 batch loss 1.41821873 epoch total loss 1.30708671\n",
      "Trained batch 48 batch loss 1.33572388 epoch total loss 1.30768335\n",
      "Trained batch 49 batch loss 1.37634194 epoch total loss 1.30908453\n",
      "Trained batch 50 batch loss 1.30172658 epoch total loss 1.30893743\n",
      "Trained batch 51 batch loss 1.21123481 epoch total loss 1.30702162\n",
      "Trained batch 52 batch loss 1.21987915 epoch total loss 1.30534577\n",
      "Trained batch 53 batch loss 1.16930461 epoch total loss 1.30277896\n",
      "Trained batch 54 batch loss 1.10500407 epoch total loss 1.29911649\n",
      "Trained batch 55 batch loss 1.23677242 epoch total loss 1.29798293\n",
      "Trained batch 56 batch loss 1.07996559 epoch total loss 1.29408967\n",
      "Trained batch 57 batch loss 1.01703405 epoch total loss 1.28922915\n",
      "Trained batch 58 batch loss 1.0064683 epoch total loss 1.28435397\n",
      "Trained batch 59 batch loss 0.981858373 epoch total loss 1.2792269\n",
      "Trained batch 60 batch loss 1.22192717 epoch total loss 1.27827191\n",
      "Trained batch 61 batch loss 1.26840425 epoch total loss 1.27811\n",
      "Trained batch 62 batch loss 1.29719591 epoch total loss 1.27841794\n",
      "Trained batch 63 batch loss 1.22977471 epoch total loss 1.27764583\n",
      "Trained batch 64 batch loss 1.2543745 epoch total loss 1.27728212\n",
      "Trained batch 65 batch loss 1.32865071 epoch total loss 1.27807236\n",
      "Trained batch 66 batch loss 1.37223566 epoch total loss 1.27949917\n",
      "Trained batch 67 batch loss 1.29986739 epoch total loss 1.27980316\n",
      "Trained batch 68 batch loss 1.30289781 epoch total loss 1.28014278\n",
      "Trained batch 69 batch loss 1.22946215 epoch total loss 1.27940822\n",
      "Trained batch 70 batch loss 1.30328465 epoch total loss 1.27974927\n",
      "Trained batch 71 batch loss 1.24405873 epoch total loss 1.27924657\n",
      "Trained batch 72 batch loss 1.24201465 epoch total loss 1.27872944\n",
      "Trained batch 73 batch loss 1.21808779 epoch total loss 1.27789867\n",
      "Trained batch 74 batch loss 1.40100384 epoch total loss 1.27956223\n",
      "Trained batch 75 batch loss 1.34697354 epoch total loss 1.28046107\n",
      "Trained batch 76 batch loss 1.31387246 epoch total loss 1.28090072\n",
      "Trained batch 77 batch loss 1.44463253 epoch total loss 1.28302717\n",
      "Trained batch 78 batch loss 1.34784698 epoch total loss 1.28385818\n",
      "Trained batch 79 batch loss 1.36227608 epoch total loss 1.28485084\n",
      "Trained batch 80 batch loss 1.3810339 epoch total loss 1.28605306\n",
      "Trained batch 81 batch loss 1.31688178 epoch total loss 1.2864337\n",
      "Trained batch 82 batch loss 1.27915812 epoch total loss 1.28634501\n",
      "Trained batch 83 batch loss 1.25796819 epoch total loss 1.28600299\n",
      "Trained batch 84 batch loss 1.27213478 epoch total loss 1.28583789\n",
      "Trained batch 85 batch loss 1.1927352 epoch total loss 1.28474259\n",
      "Trained batch 86 batch loss 1.2521745 epoch total loss 1.28436387\n",
      "Trained batch 87 batch loss 1.30387652 epoch total loss 1.28458822\n",
      "Trained batch 88 batch loss 1.31398201 epoch total loss 1.28492212\n",
      "Trained batch 89 batch loss 1.32238817 epoch total loss 1.28534317\n",
      "Trained batch 90 batch loss 1.22718835 epoch total loss 1.28469694\n",
      "Trained batch 91 batch loss 1.27466512 epoch total loss 1.28458679\n",
      "Trained batch 92 batch loss 1.27744794 epoch total loss 1.28450918\n",
      "Trained batch 93 batch loss 1.25796032 epoch total loss 1.28422368\n",
      "Trained batch 94 batch loss 1.3352437 epoch total loss 1.28476644\n",
      "Trained batch 95 batch loss 1.33369493 epoch total loss 1.28528142\n",
      "Trained batch 96 batch loss 1.41258848 epoch total loss 1.28660762\n",
      "Trained batch 97 batch loss 1.48513627 epoch total loss 1.28865433\n",
      "Trained batch 98 batch loss 1.34600866 epoch total loss 1.28923953\n",
      "Trained batch 99 batch loss 1.31287026 epoch total loss 1.2894783\n",
      "Trained batch 100 batch loss 1.29050994 epoch total loss 1.28948855\n",
      "Trained batch 101 batch loss 1.21824944 epoch total loss 1.28878319\n",
      "Trained batch 102 batch loss 1.32664144 epoch total loss 1.28915441\n",
      "Trained batch 103 batch loss 1.32192385 epoch total loss 1.28947258\n",
      "Trained batch 104 batch loss 1.29330993 epoch total loss 1.28950942\n",
      "Trained batch 105 batch loss 1.27757359 epoch total loss 1.28939569\n",
      "Trained batch 106 batch loss 1.33174276 epoch total loss 1.28979516\n",
      "Trained batch 107 batch loss 1.29208171 epoch total loss 1.28981662\n",
      "Trained batch 108 batch loss 1.30686569 epoch total loss 1.28997445\n",
      "Trained batch 109 batch loss 1.23772812 epoch total loss 1.28949523\n",
      "Trained batch 110 batch loss 1.35597682 epoch total loss 1.2900995\n",
      "Trained batch 111 batch loss 1.48598516 epoch total loss 1.2918644\n",
      "Trained batch 112 batch loss 1.28606737 epoch total loss 1.29181266\n",
      "Trained batch 113 batch loss 1.19807112 epoch total loss 1.29098308\n",
      "Trained batch 114 batch loss 1.18790579 epoch total loss 1.290079\n",
      "Trained batch 115 batch loss 1.21164227 epoch total loss 1.28939688\n",
      "Trained batch 116 batch loss 1.25931978 epoch total loss 1.2891376\n",
      "Trained batch 117 batch loss 1.46315813 epoch total loss 1.29062498\n",
      "Trained batch 118 batch loss 1.3213973 epoch total loss 1.29088581\n",
      "Trained batch 119 batch loss 1.23970282 epoch total loss 1.2904557\n",
      "Trained batch 120 batch loss 1.33602715 epoch total loss 1.2908355\n",
      "Trained batch 121 batch loss 1.31003475 epoch total loss 1.29099405\n",
      "Trained batch 122 batch loss 1.27143455 epoch total loss 1.29083371\n",
      "Trained batch 123 batch loss 1.36593652 epoch total loss 1.2914443\n",
      "Trained batch 124 batch loss 1.27809715 epoch total loss 1.29133666\n",
      "Trained batch 125 batch loss 1.41966498 epoch total loss 1.29236329\n",
      "Trained batch 126 batch loss 1.41362381 epoch total loss 1.29332566\n",
      "Trained batch 127 batch loss 1.18832672 epoch total loss 1.29249883\n",
      "Trained batch 128 batch loss 1.24224806 epoch total loss 1.29210627\n",
      "Trained batch 129 batch loss 1.2206254 epoch total loss 1.29155219\n",
      "Trained batch 130 batch loss 1.27387547 epoch total loss 1.29141617\n",
      "Trained batch 131 batch loss 1.30489445 epoch total loss 1.29151917\n",
      "Trained batch 132 batch loss 1.35372353 epoch total loss 1.2919904\n",
      "Trained batch 133 batch loss 1.46385586 epoch total loss 1.29328263\n",
      "Trained batch 134 batch loss 1.51028132 epoch total loss 1.29490209\n",
      "Trained batch 135 batch loss 1.51240385 epoch total loss 1.2965132\n",
      "Trained batch 136 batch loss 1.44069624 epoch total loss 1.29757333\n",
      "Trained batch 137 batch loss 1.43967223 epoch total loss 1.29861045\n",
      "Trained batch 138 batch loss 1.37080526 epoch total loss 1.29913366\n",
      "Trained batch 139 batch loss 1.13646674 epoch total loss 1.29796326\n",
      "Trained batch 140 batch loss 1.1963613 epoch total loss 1.29723763\n",
      "Trained batch 141 batch loss 1.29493546 epoch total loss 1.2972213\n",
      "Trained batch 142 batch loss 1.43945575 epoch total loss 1.2982229\n",
      "Trained batch 143 batch loss 1.33983922 epoch total loss 1.29851401\n",
      "Trained batch 144 batch loss 1.28892791 epoch total loss 1.29844737\n",
      "Trained batch 145 batch loss 1.23060107 epoch total loss 1.29797947\n",
      "Trained batch 146 batch loss 1.20902634 epoch total loss 1.29737031\n",
      "Trained batch 147 batch loss 1.17463303 epoch total loss 1.29653537\n",
      "Trained batch 148 batch loss 1.20927167 epoch total loss 1.29594576\n",
      "Trained batch 149 batch loss 1.22799563 epoch total loss 1.29548967\n",
      "Trained batch 150 batch loss 1.17798913 epoch total loss 1.29470646\n",
      "Trained batch 151 batch loss 1.29056859 epoch total loss 1.29467905\n",
      "Trained batch 152 batch loss 1.32573581 epoch total loss 1.29488337\n",
      "Trained batch 153 batch loss 1.25101805 epoch total loss 1.29459667\n",
      "Trained batch 154 batch loss 1.29292226 epoch total loss 1.29458582\n",
      "Trained batch 155 batch loss 1.1506815 epoch total loss 1.29365742\n",
      "Trained batch 156 batch loss 1.15815222 epoch total loss 1.29278874\n",
      "Trained batch 157 batch loss 1.2286309 epoch total loss 1.29238021\n",
      "Trained batch 158 batch loss 1.36932063 epoch total loss 1.29286718\n",
      "Trained batch 159 batch loss 1.15376139 epoch total loss 1.29199231\n",
      "Trained batch 160 batch loss 1.25173879 epoch total loss 1.29174066\n",
      "Trained batch 161 batch loss 1.22257626 epoch total loss 1.29131114\n",
      "Trained batch 162 batch loss 1.40763712 epoch total loss 1.29202926\n",
      "Trained batch 163 batch loss 1.30029941 epoch total loss 1.29207993\n",
      "Trained batch 164 batch loss 1.39514637 epoch total loss 1.29270828\n",
      "Trained batch 165 batch loss 1.32077479 epoch total loss 1.29287839\n",
      "Trained batch 166 batch loss 1.14124298 epoch total loss 1.29196501\n",
      "Trained batch 167 batch loss 1.26044106 epoch total loss 1.29177618\n",
      "Trained batch 168 batch loss 1.33274078 epoch total loss 1.29202\n",
      "Trained batch 169 batch loss 1.37245727 epoch total loss 1.29249597\n",
      "Trained batch 170 batch loss 1.48951173 epoch total loss 1.2936548\n",
      "Trained batch 171 batch loss 1.2582438 epoch total loss 1.29344773\n",
      "Trained batch 172 batch loss 1.29718733 epoch total loss 1.29346943\n",
      "Trained batch 173 batch loss 1.16381168 epoch total loss 1.29272008\n",
      "Trained batch 174 batch loss 1.35713625 epoch total loss 1.29309022\n",
      "Trained batch 175 batch loss 1.32023716 epoch total loss 1.29324532\n",
      "Trained batch 176 batch loss 1.35563171 epoch total loss 1.29359984\n",
      "Trained batch 177 batch loss 1.35384905 epoch total loss 1.29394019\n",
      "Trained batch 178 batch loss 1.30577016 epoch total loss 1.29400671\n",
      "Trained batch 179 batch loss 1.22418368 epoch total loss 1.29361665\n",
      "Trained batch 180 batch loss 1.14356256 epoch total loss 1.29278302\n",
      "Trained batch 181 batch loss 1.28668356 epoch total loss 1.29274929\n",
      "Trained batch 182 batch loss 1.29327607 epoch total loss 1.29275215\n",
      "Trained batch 183 batch loss 1.37575006 epoch total loss 1.29320574\n",
      "Trained batch 184 batch loss 1.49387646 epoch total loss 1.29429638\n",
      "Trained batch 185 batch loss 1.41337943 epoch total loss 1.29494\n",
      "Trained batch 186 batch loss 1.38576603 epoch total loss 1.2954284\n",
      "Trained batch 187 batch loss 1.34343994 epoch total loss 1.29568517\n",
      "Trained batch 188 batch loss 1.357198 epoch total loss 1.29601228\n",
      "Trained batch 189 batch loss 1.34865427 epoch total loss 1.29629076\n",
      "Trained batch 190 batch loss 1.36216044 epoch total loss 1.29663754\n",
      "Trained batch 191 batch loss 1.38512135 epoch total loss 1.29710078\n",
      "Trained batch 192 batch loss 1.33600426 epoch total loss 1.29730332\n",
      "Trained batch 193 batch loss 1.40679622 epoch total loss 1.29787064\n",
      "Trained batch 194 batch loss 1.31841409 epoch total loss 1.29797661\n",
      "Trained batch 195 batch loss 1.33218408 epoch total loss 1.29815209\n",
      "Trained batch 196 batch loss 1.37422991 epoch total loss 1.29854023\n",
      "Trained batch 197 batch loss 1.33841586 epoch total loss 1.29874265\n",
      "Trained batch 198 batch loss 1.11813879 epoch total loss 1.29783046\n",
      "Trained batch 199 batch loss 1.14069748 epoch total loss 1.29704082\n",
      "Trained batch 200 batch loss 1.10057163 epoch total loss 1.29605854\n",
      "Trained batch 201 batch loss 1.22183824 epoch total loss 1.29568923\n",
      "Trained batch 202 batch loss 1.361624 epoch total loss 1.29601562\n",
      "Trained batch 203 batch loss 1.43119812 epoch total loss 1.29668164\n",
      "Trained batch 204 batch loss 1.49318588 epoch total loss 1.29764497\n",
      "Trained batch 205 batch loss 1.18233848 epoch total loss 1.29708254\n",
      "Trained batch 206 batch loss 1.20079482 epoch total loss 1.29661512\n",
      "Trained batch 207 batch loss 1.35271919 epoch total loss 1.29688621\n",
      "Trained batch 208 batch loss 1.34709561 epoch total loss 1.2971276\n",
      "Trained batch 209 batch loss 1.30007148 epoch total loss 1.29714179\n",
      "Trained batch 210 batch loss 1.40272784 epoch total loss 1.29764462\n",
      "Trained batch 211 batch loss 1.43292296 epoch total loss 1.29828572\n",
      "Trained batch 212 batch loss 1.49122 epoch total loss 1.29919577\n",
      "Trained batch 213 batch loss 1.3239274 epoch total loss 1.29931188\n",
      "Trained batch 214 batch loss 1.33765173 epoch total loss 1.29949093\n",
      "Trained batch 215 batch loss 1.29104912 epoch total loss 1.29945171\n",
      "Trained batch 216 batch loss 1.2329849 epoch total loss 1.29914391\n",
      "Trained batch 217 batch loss 1.21309412 epoch total loss 1.29874742\n",
      "Trained batch 218 batch loss 1.27210951 epoch total loss 1.29862511\n",
      "Trained batch 219 batch loss 1.30987525 epoch total loss 1.29867649\n",
      "Trained batch 220 batch loss 1.38846493 epoch total loss 1.29908466\n",
      "Trained batch 221 batch loss 1.3990171 epoch total loss 1.29953682\n",
      "Trained batch 222 batch loss 1.44822919 epoch total loss 1.30020666\n",
      "Trained batch 223 batch loss 1.51318431 epoch total loss 1.30116165\n",
      "Trained batch 224 batch loss 1.47289908 epoch total loss 1.3019284\n",
      "Trained batch 225 batch loss 1.24454868 epoch total loss 1.30167329\n",
      "Trained batch 226 batch loss 1.20935309 epoch total loss 1.30126476\n",
      "Trained batch 227 batch loss 1.33245587 epoch total loss 1.30140221\n",
      "Trained batch 228 batch loss 1.1898632 epoch total loss 1.30091298\n",
      "Trained batch 229 batch loss 1.10510743 epoch total loss 1.30005789\n",
      "Trained batch 230 batch loss 1.23912096 epoch total loss 1.299793\n",
      "Trained batch 231 batch loss 1.21018124 epoch total loss 1.2994051\n",
      "Trained batch 232 batch loss 1.19936693 epoch total loss 1.29897392\n",
      "Trained batch 233 batch loss 1.1201793 epoch total loss 1.29820657\n",
      "Trained batch 234 batch loss 1.1961118 epoch total loss 1.29777014\n",
      "Trained batch 235 batch loss 1.22289121 epoch total loss 1.29745162\n",
      "Trained batch 236 batch loss 1.27188802 epoch total loss 1.29734325\n",
      "Trained batch 237 batch loss 1.24661362 epoch total loss 1.29712915\n",
      "Trained batch 238 batch loss 1.11686325 epoch total loss 1.2963717\n",
      "Trained batch 239 batch loss 1.30718637 epoch total loss 1.296417\n",
      "Trained batch 240 batch loss 1.29378557 epoch total loss 1.29640603\n",
      "Trained batch 241 batch loss 1.25801444 epoch total loss 1.29624677\n",
      "Trained batch 242 batch loss 1.35079074 epoch total loss 1.29647219\n",
      "Trained batch 243 batch loss 1.26739216 epoch total loss 1.29635251\n",
      "Trained batch 244 batch loss 1.18725467 epoch total loss 1.29590547\n",
      "Trained batch 245 batch loss 1.12810767 epoch total loss 1.29522061\n",
      "Trained batch 246 batch loss 1.17796063 epoch total loss 1.2947439\n",
      "Trained batch 247 batch loss 1.1555934 epoch total loss 1.29418039\n",
      "Trained batch 248 batch loss 1.24727178 epoch total loss 1.29399133\n",
      "Trained batch 249 batch loss 1.16449261 epoch total loss 1.29347122\n",
      "Trained batch 250 batch loss 1.19809508 epoch total loss 1.29308975\n",
      "Trained batch 251 batch loss 1.231112 epoch total loss 1.29284275\n",
      "Trained batch 252 batch loss 1.27389264 epoch total loss 1.29276764\n",
      "Trained batch 253 batch loss 1.37125719 epoch total loss 1.29307783\n",
      "Trained batch 254 batch loss 1.23380494 epoch total loss 1.29284441\n",
      "Trained batch 255 batch loss 1.35589063 epoch total loss 1.29309165\n",
      "Trained batch 256 batch loss 1.25254202 epoch total loss 1.29293323\n",
      "Trained batch 257 batch loss 1.19602776 epoch total loss 1.29255617\n",
      "Trained batch 258 batch loss 1.2846384 epoch total loss 1.29252541\n",
      "Trained batch 259 batch loss 1.4244206 epoch total loss 1.29303467\n",
      "Trained batch 260 batch loss 1.39154589 epoch total loss 1.29341352\n",
      "Trained batch 261 batch loss 1.2092495 epoch total loss 1.29309106\n",
      "Trained batch 262 batch loss 1.28057432 epoch total loss 1.29304326\n",
      "Trained batch 263 batch loss 1.34970593 epoch total loss 1.29325867\n",
      "Trained batch 264 batch loss 1.3500371 epoch total loss 1.29347384\n",
      "Trained batch 265 batch loss 1.43412566 epoch total loss 1.29400456\n",
      "Trained batch 266 batch loss 1.46113515 epoch total loss 1.29463279\n",
      "Trained batch 267 batch loss 1.38861966 epoch total loss 1.2949847\n",
      "Trained batch 268 batch loss 1.29366779 epoch total loss 1.29497981\n",
      "Trained batch 269 batch loss 1.31284738 epoch total loss 1.29504621\n",
      "Trained batch 270 batch loss 1.28416419 epoch total loss 1.2950058\n",
      "Trained batch 271 batch loss 1.35365736 epoch total loss 1.29522228\n",
      "Trained batch 272 batch loss 1.37137628 epoch total loss 1.29550231\n",
      "Trained batch 273 batch loss 1.37164068 epoch total loss 1.29578114\n",
      "Trained batch 274 batch loss 1.44543433 epoch total loss 1.29632735\n",
      "Trained batch 275 batch loss 1.40404642 epoch total loss 1.29671907\n",
      "Trained batch 276 batch loss 1.45573151 epoch total loss 1.29729521\n",
      "Trained batch 277 batch loss 1.48627353 epoch total loss 1.29797733\n",
      "Trained batch 278 batch loss 1.39521229 epoch total loss 1.29832709\n",
      "Trained batch 279 batch loss 1.46397698 epoch total loss 1.29892087\n",
      "Trained batch 280 batch loss 1.41577017 epoch total loss 1.29933822\n",
      "Trained batch 281 batch loss 1.37270045 epoch total loss 1.29959929\n",
      "Trained batch 282 batch loss 1.55954015 epoch total loss 1.30052114\n",
      "Trained batch 283 batch loss 1.51091814 epoch total loss 1.30126452\n",
      "Trained batch 284 batch loss 1.43628931 epoch total loss 1.30173993\n",
      "Trained batch 285 batch loss 1.32116973 epoch total loss 1.30180812\n",
      "Trained batch 286 batch loss 1.48796964 epoch total loss 1.30245912\n",
      "Trained batch 287 batch loss 1.43075764 epoch total loss 1.30290616\n",
      "Trained batch 288 batch loss 1.28264332 epoch total loss 1.30283582\n",
      "Trained batch 289 batch loss 1.31587 epoch total loss 1.30288088\n",
      "Trained batch 290 batch loss 1.34059191 epoch total loss 1.30301094\n",
      "Trained batch 291 batch loss 1.27837157 epoch total loss 1.3029263\n",
      "Trained batch 292 batch loss 1.38139379 epoch total loss 1.30319512\n",
      "Trained batch 293 batch loss 1.28821588 epoch total loss 1.30314386\n",
      "Trained batch 294 batch loss 1.34993982 epoch total loss 1.30330312\n",
      "Trained batch 295 batch loss 1.3597523 epoch total loss 1.30349445\n",
      "Trained batch 296 batch loss 1.33040106 epoch total loss 1.30358541\n",
      "Trained batch 297 batch loss 1.41412294 epoch total loss 1.30395758\n",
      "Trained batch 298 batch loss 1.31901538 epoch total loss 1.30400801\n",
      "Trained batch 299 batch loss 1.26095 epoch total loss 1.303864\n",
      "Trained batch 300 batch loss 1.19973207 epoch total loss 1.30351698\n",
      "Trained batch 301 batch loss 1.23476195 epoch total loss 1.30328858\n",
      "Trained batch 302 batch loss 1.30228043 epoch total loss 1.30328524\n",
      "Trained batch 303 batch loss 1.31305027 epoch total loss 1.30331743\n",
      "Trained batch 304 batch loss 1.198331 epoch total loss 1.30297208\n",
      "Trained batch 305 batch loss 1.21849883 epoch total loss 1.30269516\n",
      "Trained batch 306 batch loss 1.20375586 epoch total loss 1.30237186\n",
      "Trained batch 307 batch loss 1.1892525 epoch total loss 1.30200338\n",
      "Trained batch 308 batch loss 1.2174865 epoch total loss 1.30172896\n",
      "Trained batch 309 batch loss 1.14863348 epoch total loss 1.30123353\n",
      "Trained batch 310 batch loss 1.19932663 epoch total loss 1.30090475\n",
      "Trained batch 311 batch loss 1.19900453 epoch total loss 1.30057716\n",
      "Trained batch 312 batch loss 1.19559574 epoch total loss 1.30024064\n",
      "Trained batch 313 batch loss 1.15972519 epoch total loss 1.29979169\n",
      "Trained batch 314 batch loss 1.21534932 epoch total loss 1.29952288\n",
      "Trained batch 315 batch loss 1.19703472 epoch total loss 1.29919744\n",
      "Trained batch 316 batch loss 1.18367076 epoch total loss 1.29883194\n",
      "Trained batch 317 batch loss 1.29972887 epoch total loss 1.2988348\n",
      "Trained batch 318 batch loss 1.4643867 epoch total loss 1.29935539\n",
      "Trained batch 319 batch loss 1.32780981 epoch total loss 1.29944456\n",
      "Trained batch 320 batch loss 1.32996821 epoch total loss 1.29953992\n",
      "Trained batch 321 batch loss 1.54941154 epoch total loss 1.30031836\n",
      "Trained batch 322 batch loss 1.4824779 epoch total loss 1.30088413\n",
      "Trained batch 323 batch loss 1.4630518 epoch total loss 1.30138612\n",
      "Trained batch 324 batch loss 1.24894094 epoch total loss 1.30122423\n",
      "Trained batch 325 batch loss 1.24736559 epoch total loss 1.30105853\n",
      "Trained batch 326 batch loss 1.34534025 epoch total loss 1.30119431\n",
      "Trained batch 327 batch loss 1.34068871 epoch total loss 1.30131519\n",
      "Trained batch 328 batch loss 1.39053237 epoch total loss 1.30158722\n",
      "Trained batch 329 batch loss 1.30782676 epoch total loss 1.30160618\n",
      "Trained batch 330 batch loss 1.34499383 epoch total loss 1.30173767\n",
      "Trained batch 331 batch loss 1.1583643 epoch total loss 1.30130446\n",
      "Trained batch 332 batch loss 1.22160625 epoch total loss 1.30106449\n",
      "Trained batch 333 batch loss 1.31635332 epoch total loss 1.30111039\n",
      "Trained batch 334 batch loss 1.30435383 epoch total loss 1.30112\n",
      "Trained batch 335 batch loss 1.42126656 epoch total loss 1.30147874\n",
      "Trained batch 336 batch loss 1.20082188 epoch total loss 1.30117917\n",
      "Trained batch 337 batch loss 1.31856179 epoch total loss 1.30123079\n",
      "Trained batch 338 batch loss 1.24159217 epoch total loss 1.30105424\n",
      "Trained batch 339 batch loss 1.23889101 epoch total loss 1.3008709\n",
      "Trained batch 340 batch loss 1.29201484 epoch total loss 1.30084491\n",
      "Trained batch 341 batch loss 1.26247287 epoch total loss 1.30073237\n",
      "Trained batch 342 batch loss 1.41550815 epoch total loss 1.30106795\n",
      "Trained batch 343 batch loss 1.338238 epoch total loss 1.30117631\n",
      "Trained batch 344 batch loss 1.24742723 epoch total loss 1.30102\n",
      "Trained batch 345 batch loss 1.11371303 epoch total loss 1.30047715\n",
      "Trained batch 346 batch loss 1.31865346 epoch total loss 1.30052972\n",
      "Trained batch 347 batch loss 1.24263132 epoch total loss 1.30036294\n",
      "Trained batch 348 batch loss 1.28215694 epoch total loss 1.30031061\n",
      "Trained batch 349 batch loss 1.22791338 epoch total loss 1.30010319\n",
      "Trained batch 350 batch loss 1.34304023 epoch total loss 1.30022585\n",
      "Trained batch 351 batch loss 1.33293664 epoch total loss 1.30031908\n",
      "Trained batch 352 batch loss 1.25367141 epoch total loss 1.30018651\n",
      "Trained batch 353 batch loss 1.19028986 epoch total loss 1.29987514\n",
      "Trained batch 354 batch loss 1.27392101 epoch total loss 1.29980183\n",
      "Trained batch 355 batch loss 1.3640027 epoch total loss 1.29998267\n",
      "Trained batch 356 batch loss 1.34987617 epoch total loss 1.30012286\n",
      "Trained batch 357 batch loss 1.39810181 epoch total loss 1.3003974\n",
      "Trained batch 358 batch loss 1.27718496 epoch total loss 1.30033255\n",
      "Trained batch 359 batch loss 1.18326831 epoch total loss 1.30000639\n",
      "Trained batch 360 batch loss 1.26940382 epoch total loss 1.29992139\n",
      "Trained batch 361 batch loss 1.33175611 epoch total loss 1.30000961\n",
      "Trained batch 362 batch loss 1.32354343 epoch total loss 1.30007458\n",
      "Trained batch 363 batch loss 1.33573794 epoch total loss 1.30017281\n",
      "Trained batch 364 batch loss 1.44155145 epoch total loss 1.30056131\n",
      "Trained batch 365 batch loss 1.39589787 epoch total loss 1.3008225\n",
      "Trained batch 366 batch loss 1.37984693 epoch total loss 1.30103838\n",
      "Trained batch 367 batch loss 1.32812357 epoch total loss 1.30111217\n",
      "Trained batch 368 batch loss 1.23026204 epoch total loss 1.30091965\n",
      "Trained batch 369 batch loss 1.30316079 epoch total loss 1.30092573\n",
      "Trained batch 370 batch loss 1.30048442 epoch total loss 1.30092454\n",
      "Trained batch 371 batch loss 1.24446797 epoch total loss 1.30077231\n",
      "Trained batch 372 batch loss 1.31492722 epoch total loss 1.30081046\n",
      "Trained batch 373 batch loss 1.31050265 epoch total loss 1.30083644\n",
      "Trained batch 374 batch loss 1.28093 epoch total loss 1.30078328\n",
      "Trained batch 375 batch loss 1.34039855 epoch total loss 1.3008889\n",
      "Trained batch 376 batch loss 1.26304924 epoch total loss 1.30078828\n",
      "Trained batch 377 batch loss 1.2283895 epoch total loss 1.30059624\n",
      "Trained batch 378 batch loss 1.12041926 epoch total loss 1.30011964\n",
      "Trained batch 379 batch loss 1.3125093 epoch total loss 1.3001523\n",
      "Trained batch 380 batch loss 1.38725019 epoch total loss 1.30038142\n",
      "Trained batch 381 batch loss 1.37156773 epoch total loss 1.30056834\n",
      "Trained batch 382 batch loss 1.28725934 epoch total loss 1.30053353\n",
      "Trained batch 383 batch loss 1.29316354 epoch total loss 1.30051422\n",
      "Trained batch 384 batch loss 1.28076982 epoch total loss 1.30046284\n",
      "Trained batch 385 batch loss 1.33507681 epoch total loss 1.30055273\n",
      "Trained batch 386 batch loss 1.14965105 epoch total loss 1.30016184\n",
      "Trained batch 387 batch loss 1.23270333 epoch total loss 1.29998744\n",
      "Trained batch 388 batch loss 1.38922167 epoch total loss 1.30021751\n",
      "Trained batch 389 batch loss 1.3436172 epoch total loss 1.30032909\n",
      "Trained batch 390 batch loss 1.34514868 epoch total loss 1.30044401\n",
      "Trained batch 391 batch loss 1.20197916 epoch total loss 1.30019212\n",
      "Trained batch 392 batch loss 1.21889877 epoch total loss 1.29998481\n",
      "Trained batch 393 batch loss 1.29250133 epoch total loss 1.29996574\n",
      "Trained batch 394 batch loss 1.26724362 epoch total loss 1.29988277\n",
      "Trained batch 395 batch loss 1.17711699 epoch total loss 1.29957199\n",
      "Trained batch 396 batch loss 1.1994431 epoch total loss 1.29931915\n",
      "Trained batch 397 batch loss 1.20696652 epoch total loss 1.29908657\n",
      "Trained batch 398 batch loss 1.26540935 epoch total loss 1.29900193\n",
      "Trained batch 399 batch loss 1.23721087 epoch total loss 1.29884696\n",
      "Trained batch 400 batch loss 1.24800158 epoch total loss 1.29871976\n",
      "Trained batch 401 batch loss 1.36337686 epoch total loss 1.29888105\n",
      "Trained batch 402 batch loss 1.31453276 epoch total loss 1.29892\n",
      "Trained batch 403 batch loss 1.27910161 epoch total loss 1.2988708\n",
      "Trained batch 404 batch loss 1.32773256 epoch total loss 1.29894233\n",
      "Trained batch 405 batch loss 1.2591449 epoch total loss 1.2988441\n",
      "Trained batch 406 batch loss 1.37343907 epoch total loss 1.2990278\n",
      "Trained batch 407 batch loss 1.29478467 epoch total loss 1.29901743\n",
      "Trained batch 408 batch loss 1.24269605 epoch total loss 1.29887927\n",
      "Trained batch 409 batch loss 1.42838216 epoch total loss 1.299196\n",
      "Trained batch 410 batch loss 1.15095866 epoch total loss 1.29883432\n",
      "Trained batch 411 batch loss 1.30314684 epoch total loss 1.29884493\n",
      "Trained batch 412 batch loss 1.32575738 epoch total loss 1.29891014\n",
      "Trained batch 413 batch loss 1.31518006 epoch total loss 1.2989496\n",
      "Trained batch 414 batch loss 1.27399504 epoch total loss 1.29888928\n",
      "Trained batch 415 batch loss 1.20273399 epoch total loss 1.29865766\n",
      "Trained batch 416 batch loss 1.33038664 epoch total loss 1.29873395\n",
      "Trained batch 417 batch loss 1.18754506 epoch total loss 1.29846728\n",
      "Trained batch 418 batch loss 1.17940736 epoch total loss 1.29818249\n",
      "Trained batch 419 batch loss 1.36740422 epoch total loss 1.29834771\n",
      "Trained batch 420 batch loss 1.42071688 epoch total loss 1.29863906\n",
      "Trained batch 421 batch loss 1.25924206 epoch total loss 1.29854548\n",
      "Trained batch 422 batch loss 1.22595167 epoch total loss 1.29837334\n",
      "Trained batch 423 batch loss 1.20560622 epoch total loss 1.29815412\n",
      "Trained batch 424 batch loss 1.13892257 epoch total loss 1.29777861\n",
      "Trained batch 425 batch loss 1.15135813 epoch total loss 1.29743409\n",
      "Trained batch 426 batch loss 1.13847327 epoch total loss 1.29706097\n",
      "Trained batch 427 batch loss 1.11503077 epoch total loss 1.29663467\n",
      "Trained batch 428 batch loss 1.11561 epoch total loss 1.29621172\n",
      "Trained batch 429 batch loss 1.22276592 epoch total loss 1.29604053\n",
      "Trained batch 430 batch loss 1.21752977 epoch total loss 1.29585803\n",
      "Trained batch 431 batch loss 1.25997293 epoch total loss 1.2957747\n",
      "Trained batch 432 batch loss 1.41330278 epoch total loss 1.29604673\n",
      "Trained batch 433 batch loss 1.29146862 epoch total loss 1.29603612\n",
      "Trained batch 434 batch loss 1.26506221 epoch total loss 1.29596484\n",
      "Trained batch 435 batch loss 1.27700067 epoch total loss 1.29592121\n",
      "Trained batch 436 batch loss 1.38844681 epoch total loss 1.29613328\n",
      "Trained batch 437 batch loss 1.29030061 epoch total loss 1.29611993\n",
      "Trained batch 438 batch loss 1.26427889 epoch total loss 1.29604721\n",
      "Trained batch 439 batch loss 1.37504292 epoch total loss 1.29622722\n",
      "Trained batch 440 batch loss 1.32753778 epoch total loss 1.29629838\n",
      "Trained batch 441 batch loss 1.33090901 epoch total loss 1.29637694\n",
      "Trained batch 442 batch loss 1.35655344 epoch total loss 1.29651308\n",
      "Trained batch 443 batch loss 1.42281675 epoch total loss 1.29679811\n",
      "Trained batch 444 batch loss 1.34930694 epoch total loss 1.29691637\n",
      "Trained batch 445 batch loss 1.22537971 epoch total loss 1.29675567\n",
      "Trained batch 446 batch loss 1.38846135 epoch total loss 1.29696131\n",
      "Trained batch 447 batch loss 1.34018302 epoch total loss 1.29705811\n",
      "Trained batch 448 batch loss 1.35223258 epoch total loss 1.29718125\n",
      "Trained batch 449 batch loss 1.32352066 epoch total loss 1.29724\n",
      "Trained batch 450 batch loss 1.29713821 epoch total loss 1.29723966\n",
      "Trained batch 451 batch loss 1.28585637 epoch total loss 1.29721439\n",
      "Trained batch 452 batch loss 1.27985787 epoch total loss 1.297176\n",
      "Trained batch 453 batch loss 1.31289387 epoch total loss 1.29721057\n",
      "Trained batch 454 batch loss 1.33028293 epoch total loss 1.29728341\n",
      "Trained batch 455 batch loss 1.26155245 epoch total loss 1.29720485\n",
      "Trained batch 456 batch loss 1.32376361 epoch total loss 1.29726315\n",
      "Trained batch 457 batch loss 1.38712096 epoch total loss 1.29745984\n",
      "Trained batch 458 batch loss 1.26875138 epoch total loss 1.29739714\n",
      "Trained batch 459 batch loss 1.14869881 epoch total loss 1.29707313\n",
      "Trained batch 460 batch loss 1.25018978 epoch total loss 1.2969712\n",
      "Trained batch 461 batch loss 1.20044374 epoch total loss 1.29676175\n",
      "Trained batch 462 batch loss 1.18334341 epoch total loss 1.2965163\n",
      "Trained batch 463 batch loss 1.21289825 epoch total loss 1.2963357\n",
      "Trained batch 464 batch loss 1.33009648 epoch total loss 1.29640841\n",
      "Trained batch 465 batch loss 1.24696481 epoch total loss 1.29630208\n",
      "Trained batch 466 batch loss 1.21530366 epoch total loss 1.29612827\n",
      "Trained batch 467 batch loss 1.34210348 epoch total loss 1.29622674\n",
      "Trained batch 468 batch loss 1.35898268 epoch total loss 1.29636085\n",
      "Trained batch 469 batch loss 1.44133031 epoch total loss 1.29667008\n",
      "Trained batch 470 batch loss 1.48805809 epoch total loss 1.29707718\n",
      "Trained batch 471 batch loss 1.51516104 epoch total loss 1.29754019\n",
      "Trained batch 472 batch loss 1.3480581 epoch total loss 1.29764724\n",
      "Trained batch 473 batch loss 1.17414117 epoch total loss 1.29738617\n",
      "Trained batch 474 batch loss 1.3696512 epoch total loss 1.29753852\n",
      "Trained batch 475 batch loss 1.49994504 epoch total loss 1.29796469\n",
      "Trained batch 476 batch loss 1.47692943 epoch total loss 1.29834056\n",
      "Trained batch 477 batch loss 1.37375164 epoch total loss 1.29849875\n",
      "Trained batch 478 batch loss 1.33867252 epoch total loss 1.29858279\n",
      "Trained batch 479 batch loss 1.29191256 epoch total loss 1.29856896\n",
      "Trained batch 480 batch loss 1.30901551 epoch total loss 1.29859078\n",
      "Trained batch 481 batch loss 1.27197623 epoch total loss 1.29853535\n",
      "Trained batch 482 batch loss 1.31425071 epoch total loss 1.29856801\n",
      "Trained batch 483 batch loss 1.30714619 epoch total loss 1.29858577\n",
      "Trained batch 484 batch loss 1.42883897 epoch total loss 1.29885483\n",
      "Trained batch 485 batch loss 1.27085423 epoch total loss 1.29879713\n",
      "Trained batch 486 batch loss 1.28644514 epoch total loss 1.29877174\n",
      "Trained batch 487 batch loss 1.32058764 epoch total loss 1.29881656\n",
      "Trained batch 488 batch loss 1.22951853 epoch total loss 1.29867458\n",
      "Trained batch 489 batch loss 1.34179473 epoch total loss 1.29876268\n",
      "Trained batch 490 batch loss 1.32845545 epoch total loss 1.29882324\n",
      "Trained batch 491 batch loss 1.24697566 epoch total loss 1.29871762\n",
      "Trained batch 492 batch loss 1.32285964 epoch total loss 1.29876673\n",
      "Trained batch 493 batch loss 1.25462508 epoch total loss 1.29867721\n",
      "Trained batch 494 batch loss 1.37912261 epoch total loss 1.29884017\n",
      "Trained batch 495 batch loss 1.37672377 epoch total loss 1.2989974\n",
      "Trained batch 496 batch loss 1.43304563 epoch total loss 1.29926765\n",
      "Trained batch 497 batch loss 1.31936145 epoch total loss 1.29930806\n",
      "Trained batch 498 batch loss 1.18896747 epoch total loss 1.29908645\n",
      "Trained batch 499 batch loss 1.32168591 epoch total loss 1.29913187\n",
      "Trained batch 500 batch loss 1.26959813 epoch total loss 1.29907274\n",
      "Trained batch 501 batch loss 1.29323 epoch total loss 1.29906106\n",
      "Trained batch 502 batch loss 1.25180006 epoch total loss 1.29896688\n",
      "Trained batch 503 batch loss 1.356107 epoch total loss 1.29908037\n",
      "Trained batch 504 batch loss 1.38664 epoch total loss 1.29925418\n",
      "Trained batch 505 batch loss 1.18162847 epoch total loss 1.29902124\n",
      "Trained batch 506 batch loss 1.33799982 epoch total loss 1.29909837\n",
      "Trained batch 507 batch loss 1.2218293 epoch total loss 1.2989459\n",
      "Trained batch 508 batch loss 1.20592844 epoch total loss 1.2987628\n",
      "Trained batch 509 batch loss 1.22242689 epoch total loss 1.29861271\n",
      "Trained batch 510 batch loss 1.18749499 epoch total loss 1.29839492\n",
      "Trained batch 511 batch loss 1.3384428 epoch total loss 1.29847324\n",
      "Trained batch 512 batch loss 1.37270927 epoch total loss 1.2986182\n",
      "Trained batch 513 batch loss 1.50294399 epoch total loss 1.29901648\n",
      "Trained batch 514 batch loss 1.51690447 epoch total loss 1.29944038\n",
      "Trained batch 515 batch loss 1.4218086 epoch total loss 1.29967797\n",
      "Trained batch 516 batch loss 1.33105302 epoch total loss 1.29973876\n",
      "Trained batch 517 batch loss 1.22420049 epoch total loss 1.29959261\n",
      "Trained batch 518 batch loss 1.08886552 epoch total loss 1.29918587\n",
      "Trained batch 519 batch loss 1.04962325 epoch total loss 1.29870498\n",
      "Trained batch 520 batch loss 1.15629292 epoch total loss 1.29843116\n",
      "Trained batch 521 batch loss 1.21758199 epoch total loss 1.29827595\n",
      "Trained batch 522 batch loss 1.41167283 epoch total loss 1.29849327\n",
      "Trained batch 523 batch loss 1.45649135 epoch total loss 1.29879534\n",
      "Trained batch 524 batch loss 1.3620708 epoch total loss 1.2989161\n",
      "Trained batch 525 batch loss 1.19685411 epoch total loss 1.29872167\n",
      "Trained batch 526 batch loss 1.3045013 epoch total loss 1.29873264\n",
      "Trained batch 527 batch loss 1.24454534 epoch total loss 1.29862988\n",
      "Trained batch 528 batch loss 1.27922213 epoch total loss 1.29859316\n",
      "Trained batch 529 batch loss 1.22847819 epoch total loss 1.29846048\n",
      "Trained batch 530 batch loss 1.37185121 epoch total loss 1.298599\n",
      "Trained batch 531 batch loss 1.24385011 epoch total loss 1.29849577\n",
      "Trained batch 532 batch loss 1.21362245 epoch total loss 1.29833627\n",
      "Trained batch 533 batch loss 1.38555396 epoch total loss 1.2985\n",
      "Trained batch 534 batch loss 1.33424091 epoch total loss 1.29856682\n",
      "Trained batch 535 batch loss 1.1087687 epoch total loss 1.29821205\n",
      "Trained batch 536 batch loss 1.24179959 epoch total loss 1.29810691\n",
      "Trained batch 537 batch loss 1.3133378 epoch total loss 1.29813528\n",
      "Trained batch 538 batch loss 1.12991047 epoch total loss 1.29782248\n",
      "Trained batch 539 batch loss 1.20017457 epoch total loss 1.2976414\n",
      "Trained batch 540 batch loss 1.14467943 epoch total loss 1.29735804\n",
      "Trained batch 541 batch loss 1.23315072 epoch total loss 1.29723942\n",
      "Trained batch 542 batch loss 1.39642501 epoch total loss 1.29742241\n",
      "Trained batch 543 batch loss 1.52244866 epoch total loss 1.29783678\n",
      "Trained batch 544 batch loss 1.34310877 epoch total loss 1.29792\n",
      "Trained batch 545 batch loss 1.17714715 epoch total loss 1.29769838\n",
      "Trained batch 546 batch loss 1.07009161 epoch total loss 1.2972815\n",
      "Trained batch 547 batch loss 1.32268155 epoch total loss 1.29732788\n",
      "Trained batch 548 batch loss 1.3613683 epoch total loss 1.29744482\n",
      "Trained batch 549 batch loss 1.34746957 epoch total loss 1.2975359\n",
      "Trained batch 550 batch loss 1.36058378 epoch total loss 1.29765058\n",
      "Trained batch 551 batch loss 1.32212281 epoch total loss 1.29769504\n",
      "Trained batch 552 batch loss 1.34158099 epoch total loss 1.29777455\n",
      "Trained batch 553 batch loss 1.30616891 epoch total loss 1.29778969\n",
      "Trained batch 554 batch loss 1.20234251 epoch total loss 1.29761732\n",
      "Trained batch 555 batch loss 1.42213547 epoch total loss 1.29784167\n",
      "Trained batch 556 batch loss 1.19011331 epoch total loss 1.29764795\n",
      "Trained batch 557 batch loss 1.32040358 epoch total loss 1.29768872\n",
      "Trained batch 558 batch loss 1.27840209 epoch total loss 1.29765415\n",
      "Trained batch 559 batch loss 1.30194938 epoch total loss 1.29766178\n",
      "Trained batch 560 batch loss 1.31907821 epoch total loss 1.2977\n",
      "Trained batch 561 batch loss 1.32497978 epoch total loss 1.29774868\n",
      "Trained batch 562 batch loss 1.32367957 epoch total loss 1.29779482\n",
      "Trained batch 563 batch loss 1.34118223 epoch total loss 1.29787183\n",
      "Trained batch 564 batch loss 1.2862649 epoch total loss 1.2978512\n",
      "Trained batch 565 batch loss 1.37799871 epoch total loss 1.29799306\n",
      "Trained batch 566 batch loss 1.27901769 epoch total loss 1.29795945\n",
      "Trained batch 567 batch loss 1.29650092 epoch total loss 1.29795694\n",
      "Trained batch 568 batch loss 1.35756779 epoch total loss 1.29806185\n",
      "Trained batch 569 batch loss 1.1407671 epoch total loss 1.2977854\n",
      "Trained batch 570 batch loss 1.22409427 epoch total loss 1.29765618\n",
      "Trained batch 571 batch loss 1.19198251 epoch total loss 1.29747105\n",
      "Trained batch 572 batch loss 1.28066969 epoch total loss 1.2974416\n",
      "Trained batch 573 batch loss 1.06789637 epoch total loss 1.29704094\n",
      "Trained batch 574 batch loss 1.14698219 epoch total loss 1.29677951\n",
      "Trained batch 575 batch loss 1.12577021 epoch total loss 1.29648221\n",
      "Trained batch 576 batch loss 1.17824554 epoch total loss 1.29627681\n",
      "Trained batch 577 batch loss 1.20505643 epoch total loss 1.29611874\n",
      "Trained batch 578 batch loss 1.15154862 epoch total loss 1.29586864\n",
      "Trained batch 579 batch loss 1.18355703 epoch total loss 1.29567468\n",
      "Trained batch 580 batch loss 1.17906666 epoch total loss 1.29547358\n",
      "Trained batch 581 batch loss 1.27412295 epoch total loss 1.29543686\n",
      "Trained batch 582 batch loss 1.45720911 epoch total loss 1.29571486\n",
      "Trained batch 583 batch loss 1.35684586 epoch total loss 1.29581976\n",
      "Trained batch 584 batch loss 1.30667806 epoch total loss 1.29583836\n",
      "Trained batch 585 batch loss 1.32284522 epoch total loss 1.29588449\n",
      "Trained batch 586 batch loss 1.35999835 epoch total loss 1.2959938\n",
      "Trained batch 587 batch loss 1.37512541 epoch total loss 1.29612863\n",
      "Trained batch 588 batch loss 1.25390911 epoch total loss 1.29605687\n",
      "Trained batch 589 batch loss 1.24367976 epoch total loss 1.29596782\n",
      "Trained batch 590 batch loss 1.37838125 epoch total loss 1.29610753\n",
      "Trained batch 591 batch loss 1.29279816 epoch total loss 1.29610193\n",
      "Trained batch 592 batch loss 1.34944749 epoch total loss 1.29619193\n",
      "Trained batch 593 batch loss 1.2805649 epoch total loss 1.29616559\n",
      "Trained batch 594 batch loss 1.3479054 epoch total loss 1.29625273\n",
      "Trained batch 595 batch loss 1.28636694 epoch total loss 1.29623616\n",
      "Trained batch 596 batch loss 1.23183906 epoch total loss 1.29612803\n",
      "Trained batch 597 batch loss 1.29627275 epoch total loss 1.29612827\n",
      "Trained batch 598 batch loss 1.27809429 epoch total loss 1.29609811\n",
      "Trained batch 599 batch loss 1.23065507 epoch total loss 1.2959888\n",
      "Trained batch 600 batch loss 1.30164504 epoch total loss 1.29599822\n",
      "Trained batch 601 batch loss 1.15009212 epoch total loss 1.29575551\n",
      "Trained batch 602 batch loss 1.23290801 epoch total loss 1.29565108\n",
      "Trained batch 603 batch loss 1.18984735 epoch total loss 1.2954756\n",
      "Trained batch 604 batch loss 1.3534174 epoch total loss 1.29557145\n",
      "Trained batch 605 batch loss 1.28425217 epoch total loss 1.29555273\n",
      "Trained batch 606 batch loss 1.30688763 epoch total loss 1.29557145\n",
      "Trained batch 607 batch loss 1.29611039 epoch total loss 1.29557228\n",
      "Trained batch 608 batch loss 1.34367955 epoch total loss 1.29565144\n",
      "Trained batch 609 batch loss 1.3221041 epoch total loss 1.29569483\n",
      "Trained batch 610 batch loss 1.2444104 epoch total loss 1.29561067\n",
      "Trained batch 611 batch loss 1.17378402 epoch total loss 1.29541123\n",
      "Trained batch 612 batch loss 1.11910963 epoch total loss 1.2951231\n",
      "Trained batch 613 batch loss 1.15230763 epoch total loss 1.29489017\n",
      "Trained batch 614 batch loss 1.35437918 epoch total loss 1.29498696\n",
      "Trained batch 615 batch loss 1.2898097 epoch total loss 1.2949785\n",
      "Trained batch 616 batch loss 1.42173994 epoch total loss 1.29518437\n",
      "Trained batch 617 batch loss 1.56265247 epoch total loss 1.29561782\n",
      "Trained batch 618 batch loss 1.38242245 epoch total loss 1.29575825\n",
      "Trained batch 619 batch loss 1.41098309 epoch total loss 1.29594445\n",
      "Trained batch 620 batch loss 1.30986822 epoch total loss 1.29596698\n",
      "Trained batch 621 batch loss 1.1735419 epoch total loss 1.29576981\n",
      "Trained batch 622 batch loss 1.21326816 epoch total loss 1.29563713\n",
      "Trained batch 623 batch loss 1.21432376 epoch total loss 1.2955066\n",
      "Trained batch 624 batch loss 1.31953228 epoch total loss 1.29554498\n",
      "Trained batch 625 batch loss 1.30221033 epoch total loss 1.29555571\n",
      "Trained batch 626 batch loss 1.35631585 epoch total loss 1.29565275\n",
      "Trained batch 627 batch loss 1.28590608 epoch total loss 1.29563713\n",
      "Trained batch 628 batch loss 1.2681489 epoch total loss 1.29559338\n",
      "Trained batch 629 batch loss 1.31682754 epoch total loss 1.29562712\n",
      "Trained batch 630 batch loss 1.34322762 epoch total loss 1.2957027\n",
      "Trained batch 631 batch loss 1.28513038 epoch total loss 1.29568589\n",
      "Trained batch 632 batch loss 1.21807909 epoch total loss 1.2955631\n",
      "Trained batch 633 batch loss 1.14539945 epoch total loss 1.29532588\n",
      "Trained batch 634 batch loss 1.23853254 epoch total loss 1.29523635\n",
      "Trained batch 635 batch loss 1.30149102 epoch total loss 1.29524612\n",
      "Trained batch 636 batch loss 1.3293066 epoch total loss 1.29529965\n",
      "Trained batch 637 batch loss 1.2821511 epoch total loss 1.29527903\n",
      "Trained batch 638 batch loss 1.31056738 epoch total loss 1.29530299\n",
      "Trained batch 639 batch loss 1.3308475 epoch total loss 1.29535866\n",
      "Trained batch 640 batch loss 1.29723775 epoch total loss 1.29536164\n",
      "Trained batch 641 batch loss 1.34839725 epoch total loss 1.29544437\n",
      "Trained batch 642 batch loss 1.39098525 epoch total loss 1.29559314\n",
      "Trained batch 643 batch loss 1.3905834 epoch total loss 1.29574084\n",
      "Trained batch 644 batch loss 1.32466507 epoch total loss 1.29578578\n",
      "Trained batch 645 batch loss 1.18139577 epoch total loss 1.2956084\n",
      "Trained batch 646 batch loss 1.25743532 epoch total loss 1.29554927\n",
      "Trained batch 647 batch loss 1.26746547 epoch total loss 1.29550588\n",
      "Trained batch 648 batch loss 1.11360669 epoch total loss 1.29522514\n",
      "Trained batch 649 batch loss 1.22921133 epoch total loss 1.29512346\n",
      "Trained batch 650 batch loss 1.31599283 epoch total loss 1.29515553\n",
      "Trained batch 651 batch loss 1.30323899 epoch total loss 1.29516792\n",
      "Trained batch 652 batch loss 1.35224414 epoch total loss 1.29525542\n",
      "Trained batch 653 batch loss 1.12228179 epoch total loss 1.29499054\n",
      "Trained batch 654 batch loss 1.05481577 epoch total loss 1.29462326\n",
      "Trained batch 655 batch loss 1.10710895 epoch total loss 1.29433692\n",
      "Trained batch 656 batch loss 1.15799236 epoch total loss 1.29412913\n",
      "Trained batch 657 batch loss 1.47132397 epoch total loss 1.2943989\n",
      "Trained batch 658 batch loss 1.27953494 epoch total loss 1.29437625\n",
      "Trained batch 659 batch loss 1.32163906 epoch total loss 1.29441762\n",
      "Trained batch 660 batch loss 1.22972155 epoch total loss 1.29431963\n",
      "Trained batch 661 batch loss 1.23804522 epoch total loss 1.29423451\n",
      "Trained batch 662 batch loss 1.19121647 epoch total loss 1.29407895\n",
      "Trained batch 663 batch loss 1.3078388 epoch total loss 1.29409969\n",
      "Trained batch 664 batch loss 1.19811785 epoch total loss 1.29395521\n",
      "Trained batch 665 batch loss 1.28554451 epoch total loss 1.29394245\n",
      "Trained batch 666 batch loss 1.27401793 epoch total loss 1.29391265\n",
      "Trained batch 667 batch loss 1.26457858 epoch total loss 1.29386866\n",
      "Trained batch 668 batch loss 1.30367088 epoch total loss 1.29388332\n",
      "Trained batch 669 batch loss 1.30512452 epoch total loss 1.2939\n",
      "Trained batch 670 batch loss 1.19545555 epoch total loss 1.29375315\n",
      "Trained batch 671 batch loss 1.23623133 epoch total loss 1.29366732\n",
      "Trained batch 672 batch loss 1.27880716 epoch total loss 1.29364526\n",
      "Trained batch 673 batch loss 1.27555013 epoch total loss 1.29361832\n",
      "Trained batch 674 batch loss 1.27159381 epoch total loss 1.29358566\n",
      "Trained batch 675 batch loss 1.3109529 epoch total loss 1.29361153\n",
      "Trained batch 676 batch loss 1.27918327 epoch total loss 1.29359007\n",
      "Trained batch 677 batch loss 1.48403764 epoch total loss 1.2938714\n",
      "Trained batch 678 batch loss 1.38759899 epoch total loss 1.29400957\n",
      "Trained batch 679 batch loss 1.26740849 epoch total loss 1.29397035\n",
      "Trained batch 680 batch loss 1.40992665 epoch total loss 1.29414093\n",
      "Trained batch 681 batch loss 1.37171078 epoch total loss 1.29425478\n",
      "Trained batch 682 batch loss 1.28257394 epoch total loss 1.29423773\n",
      "Trained batch 683 batch loss 1.2796911 epoch total loss 1.29421639\n",
      "Trained batch 684 batch loss 1.25932026 epoch total loss 1.29416537\n",
      "Trained batch 685 batch loss 1.22707236 epoch total loss 1.29406738\n",
      "Trained batch 686 batch loss 1.2723732 epoch total loss 1.29403579\n",
      "Trained batch 687 batch loss 1.29971814 epoch total loss 1.29404414\n",
      "Trained batch 688 batch loss 1.25830948 epoch total loss 1.29399216\n",
      "Trained batch 689 batch loss 1.22445059 epoch total loss 1.29389119\n",
      "Trained batch 690 batch loss 1.26758039 epoch total loss 1.29385304\n",
      "Trained batch 691 batch loss 1.27538419 epoch total loss 1.29382634\n",
      "Trained batch 692 batch loss 1.28321326 epoch total loss 1.29381096\n",
      "Trained batch 693 batch loss 1.18952656 epoch total loss 1.29366052\n",
      "Trained batch 694 batch loss 1.20367134 epoch total loss 1.29353082\n",
      "Trained batch 695 batch loss 1.22241533 epoch total loss 1.29342854\n",
      "Trained batch 696 batch loss 1.15513659 epoch total loss 1.29322982\n",
      "Trained batch 697 batch loss 1.15565848 epoch total loss 1.29303241\n",
      "Trained batch 698 batch loss 1.26587784 epoch total loss 1.29299355\n",
      "Trained batch 699 batch loss 1.33641398 epoch total loss 1.29305565\n",
      "Trained batch 700 batch loss 1.35566008 epoch total loss 1.29314506\n",
      "Trained batch 701 batch loss 1.34578443 epoch total loss 1.29322016\n",
      "Trained batch 702 batch loss 1.23749864 epoch total loss 1.29314077\n",
      "Trained batch 703 batch loss 1.25804555 epoch total loss 1.29309082\n",
      "Trained batch 704 batch loss 1.39551508 epoch total loss 1.29323626\n",
      "Trained batch 705 batch loss 1.29262924 epoch total loss 1.29323542\n",
      "Trained batch 706 batch loss 1.23763514 epoch total loss 1.29315662\n",
      "Trained batch 707 batch loss 1.2354275 epoch total loss 1.29307497\n",
      "Trained batch 708 batch loss 1.19367838 epoch total loss 1.29293454\n",
      "Trained batch 709 batch loss 1.19254088 epoch total loss 1.29279292\n",
      "Trained batch 710 batch loss 1.26767826 epoch total loss 1.29275763\n",
      "Trained batch 711 batch loss 1.37526977 epoch total loss 1.29287362\n",
      "Trained batch 712 batch loss 1.17477012 epoch total loss 1.29270768\n",
      "Trained batch 713 batch loss 1.19743478 epoch total loss 1.29257417\n",
      "Trained batch 714 batch loss 1.10648131 epoch total loss 1.29231358\n",
      "Trained batch 715 batch loss 1.21423435 epoch total loss 1.29220426\n",
      "Trained batch 716 batch loss 1.06575859 epoch total loss 1.291888\n",
      "Trained batch 717 batch loss 0.971276045 epoch total loss 1.29144084\n",
      "Trained batch 718 batch loss 1.1438303 epoch total loss 1.29123533\n",
      "Trained batch 719 batch loss 1.21889281 epoch total loss 1.2911346\n",
      "Trained batch 720 batch loss 1.50734782 epoch total loss 1.29143488\n",
      "Trained batch 721 batch loss 1.50498688 epoch total loss 1.29173112\n",
      "Trained batch 722 batch loss 1.43539071 epoch total loss 1.29193008\n",
      "Trained batch 723 batch loss 1.32686305 epoch total loss 1.29197836\n",
      "Trained batch 724 batch loss 1.31892395 epoch total loss 1.29201555\n",
      "Trained batch 725 batch loss 1.24060678 epoch total loss 1.29194462\n",
      "Trained batch 726 batch loss 1.36320615 epoch total loss 1.29204285\n",
      "Trained batch 727 batch loss 1.29277682 epoch total loss 1.29204381\n",
      "Trained batch 728 batch loss 1.31237423 epoch total loss 1.2920717\n",
      "Trained batch 729 batch loss 1.28605795 epoch total loss 1.29206347\n",
      "Trained batch 730 batch loss 1.30240262 epoch total loss 1.29207778\n",
      "Trained batch 731 batch loss 1.29847932 epoch total loss 1.29208648\n",
      "Trained batch 732 batch loss 1.24799275 epoch total loss 1.29202616\n",
      "Trained batch 733 batch loss 1.18111396 epoch total loss 1.29187489\n",
      "Trained batch 734 batch loss 1.21686602 epoch total loss 1.2917726\n",
      "Trained batch 735 batch loss 1.34546161 epoch total loss 1.29184568\n",
      "Trained batch 736 batch loss 1.15786707 epoch total loss 1.29166365\n",
      "Trained batch 737 batch loss 1.20201826 epoch total loss 1.29154205\n",
      "Trained batch 738 batch loss 1.20627856 epoch total loss 1.29142654\n",
      "Trained batch 739 batch loss 1.10360479 epoch total loss 1.29117227\n",
      "Trained batch 740 batch loss 1.27156806 epoch total loss 1.2911458\n",
      "Trained batch 741 batch loss 1.23059034 epoch total loss 1.29106402\n",
      "Trained batch 742 batch loss 1.20439136 epoch total loss 1.29094732\n",
      "Trained batch 743 batch loss 1.22154927 epoch total loss 1.29085386\n",
      "Trained batch 744 batch loss 1.289083 epoch total loss 1.29085147\n",
      "Trained batch 745 batch loss 1.09495282 epoch total loss 1.2905885\n",
      "Trained batch 746 batch loss 1.23418128 epoch total loss 1.29051292\n",
      "Trained batch 747 batch loss 1.1263721 epoch total loss 1.29029322\n",
      "Trained batch 748 batch loss 1.17288172 epoch total loss 1.29013622\n",
      "Trained batch 749 batch loss 1.15280795 epoch total loss 1.28995287\n",
      "Trained batch 750 batch loss 1.13232696 epoch total loss 1.28974271\n",
      "Trained batch 751 batch loss 1.1643647 epoch total loss 1.2895757\n",
      "Trained batch 752 batch loss 1.34941435 epoch total loss 1.28965533\n",
      "Trained batch 753 batch loss 1.206393 epoch total loss 1.28954482\n",
      "Trained batch 754 batch loss 1.27452874 epoch total loss 1.28952491\n",
      "Trained batch 755 batch loss 1.34020114 epoch total loss 1.28959203\n",
      "Trained batch 756 batch loss 1.28335357 epoch total loss 1.28958368\n",
      "Trained batch 757 batch loss 1.318205 epoch total loss 1.28962147\n",
      "Trained batch 758 batch loss 1.35084355 epoch total loss 1.2897023\n",
      "Trained batch 759 batch loss 1.3179183 epoch total loss 1.28973949\n",
      "Trained batch 760 batch loss 1.27133203 epoch total loss 1.28971529\n",
      "Trained batch 761 batch loss 1.22293437 epoch total loss 1.28962755\n",
      "Trained batch 762 batch loss 1.29989386 epoch total loss 1.28964102\n",
      "Trained batch 763 batch loss 1.33286667 epoch total loss 1.28969765\n",
      "Trained batch 764 batch loss 1.31722879 epoch total loss 1.28973365\n",
      "Trained batch 765 batch loss 1.33634794 epoch total loss 1.28979456\n",
      "Trained batch 766 batch loss 1.30051661 epoch total loss 1.28980863\n",
      "Trained batch 767 batch loss 1.32967305 epoch total loss 1.28986061\n",
      "Trained batch 768 batch loss 1.33942068 epoch total loss 1.2899251\n",
      "Trained batch 769 batch loss 1.30609989 epoch total loss 1.28994608\n",
      "Trained batch 770 batch loss 1.39391208 epoch total loss 1.29008114\n",
      "Trained batch 771 batch loss 1.28467798 epoch total loss 1.29007411\n",
      "Trained batch 772 batch loss 1.35431051 epoch total loss 1.29015732\n",
      "Trained batch 773 batch loss 1.3827759 epoch total loss 1.29027712\n",
      "Trained batch 774 batch loss 1.24419904 epoch total loss 1.29021764\n",
      "Trained batch 775 batch loss 1.30142081 epoch total loss 1.29023206\n",
      "Trained batch 776 batch loss 1.26072645 epoch total loss 1.29019403\n",
      "Trained batch 777 batch loss 1.26330531 epoch total loss 1.29015946\n",
      "Trained batch 778 batch loss 1.34676981 epoch total loss 1.29023218\n",
      "Trained batch 779 batch loss 1.21586466 epoch total loss 1.29013669\n",
      "Trained batch 780 batch loss 1.17918563 epoch total loss 1.28999448\n",
      "Trained batch 781 batch loss 1.21338415 epoch total loss 1.28989637\n",
      "Trained batch 782 batch loss 1.24773526 epoch total loss 1.28984249\n",
      "Trained batch 783 batch loss 1.1859802 epoch total loss 1.28970981\n",
      "Trained batch 784 batch loss 1.21876299 epoch total loss 1.28961933\n",
      "Trained batch 785 batch loss 1.25691783 epoch total loss 1.2895776\n",
      "Trained batch 786 batch loss 1.29778051 epoch total loss 1.28958809\n",
      "Trained batch 787 batch loss 1.36182964 epoch total loss 1.28967988\n",
      "Trained batch 788 batch loss 1.53186679 epoch total loss 1.28998721\n",
      "Trained batch 789 batch loss 1.5622046 epoch total loss 1.2903322\n",
      "Trained batch 790 batch loss 1.38057268 epoch total loss 1.2904464\n",
      "Trained batch 791 batch loss 1.377244 epoch total loss 1.29055607\n",
      "Trained batch 792 batch loss 1.33422923 epoch total loss 1.29061127\n",
      "Trained batch 793 batch loss 1.24649775 epoch total loss 1.29055572\n",
      "Trained batch 794 batch loss 1.32748342 epoch total loss 1.29060221\n",
      "Trained batch 795 batch loss 1.24513841 epoch total loss 1.29054499\n",
      "Trained batch 796 batch loss 1.24697149 epoch total loss 1.29049027\n",
      "Trained batch 797 batch loss 1.3889488 epoch total loss 1.29061377\n",
      "Trained batch 798 batch loss 1.28685439 epoch total loss 1.290609\n",
      "Trained batch 799 batch loss 1.274472 epoch total loss 1.29058874\n",
      "Trained batch 800 batch loss 1.22840571 epoch total loss 1.29051101\n",
      "Trained batch 801 batch loss 1.22772586 epoch total loss 1.29043269\n",
      "Trained batch 802 batch loss 1.25646842 epoch total loss 1.29039037\n",
      "Trained batch 803 batch loss 1.14004064 epoch total loss 1.29020309\n",
      "Trained batch 804 batch loss 1.211236 epoch total loss 1.29010475\n",
      "Trained batch 805 batch loss 1.1906364 epoch total loss 1.28998125\n",
      "Trained batch 806 batch loss 1.21356761 epoch total loss 1.28988659\n",
      "Trained batch 807 batch loss 1.20483649 epoch total loss 1.28978121\n",
      "Trained batch 808 batch loss 1.25532937 epoch total loss 1.28973854\n",
      "Trained batch 809 batch loss 1.41781294 epoch total loss 1.28989697\n",
      "Trained batch 810 batch loss 1.22207797 epoch total loss 1.28981316\n",
      "Trained batch 811 batch loss 1.41627359 epoch total loss 1.28996909\n",
      "Trained batch 812 batch loss 1.28863323 epoch total loss 1.2899673\n",
      "Trained batch 813 batch loss 1.23589551 epoch total loss 1.28990078\n",
      "Trained batch 814 batch loss 1.09058142 epoch total loss 1.28965592\n",
      "Trained batch 815 batch loss 1.15008235 epoch total loss 1.28948462\n",
      "Trained batch 816 batch loss 1.26625383 epoch total loss 1.28945613\n",
      "Trained batch 817 batch loss 1.32802153 epoch total loss 1.28950322\n",
      "Trained batch 818 batch loss 1.43351674 epoch total loss 1.28967929\n",
      "Trained batch 819 batch loss 1.26256776 epoch total loss 1.28964615\n",
      "Trained batch 820 batch loss 1.27890718 epoch total loss 1.28963315\n",
      "Trained batch 821 batch loss 1.2075789 epoch total loss 1.28953314\n",
      "Trained batch 822 batch loss 1.29127812 epoch total loss 1.28953516\n",
      "Trained batch 823 batch loss 1.14549422 epoch total loss 1.28936017\n",
      "Trained batch 824 batch loss 1.15847445 epoch total loss 1.28920126\n",
      "Trained batch 825 batch loss 1.24543202 epoch total loss 1.28914833\n",
      "Trained batch 826 batch loss 1.26379597 epoch total loss 1.28911757\n",
      "Trained batch 827 batch loss 1.23929811 epoch total loss 1.28905737\n",
      "Trained batch 828 batch loss 1.1786797 epoch total loss 1.2889241\n",
      "Trained batch 829 batch loss 1.30793595 epoch total loss 1.28894711\n",
      "Trained batch 830 batch loss 1.16564679 epoch total loss 1.28879845\n",
      "Trained batch 831 batch loss 1.19897532 epoch total loss 1.28869045\n",
      "Trained batch 832 batch loss 1.14890993 epoch total loss 1.28852248\n",
      "Trained batch 833 batch loss 1.13712955 epoch total loss 1.28834069\n",
      "Trained batch 834 batch loss 1.20525849 epoch total loss 1.28824091\n",
      "Trained batch 835 batch loss 1.20137727 epoch total loss 1.28813696\n",
      "Trained batch 836 batch loss 1.10248029 epoch total loss 1.28791499\n",
      "Trained batch 837 batch loss 1.24608445 epoch total loss 1.28786504\n",
      "Trained batch 838 batch loss 1.35090959 epoch total loss 1.28794026\n",
      "Trained batch 839 batch loss 1.19532907 epoch total loss 1.28782988\n",
      "Trained batch 840 batch loss 1.41101408 epoch total loss 1.2879765\n",
      "Trained batch 841 batch loss 1.37732363 epoch total loss 1.28808272\n",
      "Trained batch 842 batch loss 1.23608971 epoch total loss 1.28802097\n",
      "Trained batch 843 batch loss 1.272892 epoch total loss 1.28800309\n",
      "Trained batch 844 batch loss 1.29078794 epoch total loss 1.28800642\n",
      "Trained batch 845 batch loss 1.16235566 epoch total loss 1.28785765\n",
      "Trained batch 846 batch loss 1.15540338 epoch total loss 1.28770113\n",
      "Trained batch 847 batch loss 1.01966953 epoch total loss 1.28738463\n",
      "Trained batch 848 batch loss 1.33724618 epoch total loss 1.28744352\n",
      "Trained batch 849 batch loss 1.32287252 epoch total loss 1.28748524\n",
      "Trained batch 850 batch loss 1.33021295 epoch total loss 1.28753543\n",
      "Trained batch 851 batch loss 1.3434695 epoch total loss 1.28760123\n",
      "Trained batch 852 batch loss 1.41250384 epoch total loss 1.28774786\n",
      "Trained batch 853 batch loss 1.18943787 epoch total loss 1.28763258\n",
      "Trained batch 854 batch loss 1.16008723 epoch total loss 1.28748322\n",
      "Trained batch 855 batch loss 1.16644931 epoch total loss 1.28734171\n",
      "Trained batch 856 batch loss 1.23204708 epoch total loss 1.2872771\n",
      "Trained batch 857 batch loss 1.28127122 epoch total loss 1.28727007\n",
      "Trained batch 858 batch loss 1.23874736 epoch total loss 1.28721356\n",
      "Trained batch 859 batch loss 1.40936351 epoch total loss 1.28735578\n",
      "Trained batch 860 batch loss 1.32171655 epoch total loss 1.28739583\n",
      "Trained batch 861 batch loss 1.30354571 epoch total loss 1.28741467\n",
      "Trained batch 862 batch loss 1.41170335 epoch total loss 1.28755891\n",
      "Trained batch 863 batch loss 1.44858778 epoch total loss 1.28774548\n",
      "Trained batch 864 batch loss 1.32095146 epoch total loss 1.28778386\n",
      "Trained batch 865 batch loss 1.34803224 epoch total loss 1.28785348\n",
      "Trained batch 866 batch loss 1.39944577 epoch total loss 1.28798234\n",
      "Trained batch 867 batch loss 1.31922817 epoch total loss 1.28801835\n",
      "Trained batch 868 batch loss 1.29180086 epoch total loss 1.28802264\n",
      "Trained batch 869 batch loss 1.21301639 epoch total loss 1.28793633\n",
      "Trained batch 870 batch loss 1.17932487 epoch total loss 1.28781152\n",
      "Trained batch 871 batch loss 1.13579571 epoch total loss 1.28763688\n",
      "Trained batch 872 batch loss 1.26949036 epoch total loss 1.28761613\n",
      "Trained batch 873 batch loss 1.31492472 epoch total loss 1.28764749\n",
      "Trained batch 874 batch loss 1.25110912 epoch total loss 1.28760564\n",
      "Trained batch 875 batch loss 1.25921953 epoch total loss 1.28757322\n",
      "Trained batch 876 batch loss 1.41063237 epoch total loss 1.28771377\n",
      "Trained batch 877 batch loss 1.24348521 epoch total loss 1.28766334\n",
      "Trained batch 878 batch loss 1.36061406 epoch total loss 1.28774643\n",
      "Trained batch 879 batch loss 1.26365101 epoch total loss 1.28771901\n",
      "Trained batch 880 batch loss 1.27652156 epoch total loss 1.28770626\n",
      "Trained batch 881 batch loss 1.20228541 epoch total loss 1.28760934\n",
      "Trained batch 882 batch loss 1.22177 epoch total loss 1.28753471\n",
      "Trained batch 883 batch loss 1.31398535 epoch total loss 1.28756464\n",
      "Trained batch 884 batch loss 1.29180145 epoch total loss 1.2875694\n",
      "Trained batch 885 batch loss 1.3160218 epoch total loss 1.28760147\n",
      "Trained batch 886 batch loss 1.39507234 epoch total loss 1.28772271\n",
      "Trained batch 887 batch loss 1.19830847 epoch total loss 1.28762197\n",
      "Trained batch 888 batch loss 1.22727299 epoch total loss 1.28755403\n",
      "Trained batch 889 batch loss 1.31678271 epoch total loss 1.28758693\n",
      "Trained batch 890 batch loss 1.36334527 epoch total loss 1.28767216\n",
      "Trained batch 891 batch loss 1.30619931 epoch total loss 1.2876929\n",
      "Trained batch 892 batch loss 1.37232375 epoch total loss 1.2877878\n",
      "Trained batch 893 batch loss 1.33005381 epoch total loss 1.28783512\n",
      "Trained batch 894 batch loss 1.32854533 epoch total loss 1.28788054\n",
      "Trained batch 895 batch loss 1.36509275 epoch total loss 1.28796685\n",
      "Trained batch 896 batch loss 1.32465601 epoch total loss 1.28800786\n",
      "Trained batch 897 batch loss 1.22302604 epoch total loss 1.28793538\n",
      "Trained batch 898 batch loss 1.25203729 epoch total loss 1.28789544\n",
      "Trained batch 899 batch loss 1.46167362 epoch total loss 1.2880888\n",
      "Trained batch 900 batch loss 1.41667628 epoch total loss 1.28823161\n",
      "Trained batch 901 batch loss 1.34281647 epoch total loss 1.28829217\n",
      "Trained batch 902 batch loss 1.2382648 epoch total loss 1.28823674\n",
      "Trained batch 903 batch loss 1.30301952 epoch total loss 1.28825307\n",
      "Trained batch 904 batch loss 1.21760809 epoch total loss 1.28817487\n",
      "Trained batch 905 batch loss 1.35184753 epoch total loss 1.2882452\n",
      "Trained batch 906 batch loss 1.36193573 epoch total loss 1.28832662\n",
      "Trained batch 907 batch loss 1.42220092 epoch total loss 1.2884742\n",
      "Trained batch 908 batch loss 1.36040807 epoch total loss 1.28855336\n",
      "Trained batch 909 batch loss 1.22363424 epoch total loss 1.28848195\n",
      "Trained batch 910 batch loss 1.18105006 epoch total loss 1.28836393\n",
      "Trained batch 911 batch loss 1.133412 epoch total loss 1.28819382\n",
      "Trained batch 912 batch loss 1.15173364 epoch total loss 1.28804421\n",
      "Trained batch 913 batch loss 1.12607193 epoch total loss 1.28786683\n",
      "Trained batch 914 batch loss 1.17707872 epoch total loss 1.28774559\n",
      "Trained batch 915 batch loss 1.22198319 epoch total loss 1.28767371\n",
      "Trained batch 916 batch loss 1.4110899 epoch total loss 1.28780854\n",
      "Trained batch 917 batch loss 1.31371188 epoch total loss 1.28783679\n",
      "Trained batch 918 batch loss 1.3062048 epoch total loss 1.2878567\n",
      "Trained batch 919 batch loss 1.06709468 epoch total loss 1.28761649\n",
      "Trained batch 920 batch loss 1.28120303 epoch total loss 1.28760958\n",
      "Trained batch 921 batch loss 1.12577927 epoch total loss 1.28743386\n",
      "Trained batch 922 batch loss 1.20365 epoch total loss 1.28734291\n",
      "Trained batch 923 batch loss 1.28955722 epoch total loss 1.28734529\n",
      "Trained batch 924 batch loss 1.25480616 epoch total loss 1.28731\n",
      "Trained batch 925 batch loss 1.38367498 epoch total loss 1.28741419\n",
      "Trained batch 926 batch loss 1.28261626 epoch total loss 1.28740907\n",
      "Trained batch 927 batch loss 1.22672665 epoch total loss 1.2873435\n",
      "Trained batch 928 batch loss 1.17148972 epoch total loss 1.28721869\n",
      "Trained batch 929 batch loss 1.3169018 epoch total loss 1.28725064\n",
      "Trained batch 930 batch loss 1.22773159 epoch total loss 1.28718674\n",
      "Trained batch 931 batch loss 1.15639472 epoch total loss 1.28704619\n",
      "Trained batch 932 batch loss 1.3965683 epoch total loss 1.28716373\n",
      "Trained batch 933 batch loss 1.25195849 epoch total loss 1.28712595\n",
      "Trained batch 934 batch loss 1.24992597 epoch total loss 1.28708613\n",
      "Trained batch 935 batch loss 1.1644119 epoch total loss 1.28695488\n",
      "Trained batch 936 batch loss 1.2910037 epoch total loss 1.28695929\n",
      "Trained batch 937 batch loss 1.29860592 epoch total loss 1.28697169\n",
      "Trained batch 938 batch loss 1.1936754 epoch total loss 1.28687227\n",
      "Trained batch 939 batch loss 1.14500916 epoch total loss 1.28672123\n",
      "Trained batch 940 batch loss 1.23888075 epoch total loss 1.28667033\n",
      "Trained batch 941 batch loss 1.28755951 epoch total loss 1.28667128\n",
      "Trained batch 942 batch loss 1.20963418 epoch total loss 1.2865895\n",
      "Trained batch 943 batch loss 1.31974208 epoch total loss 1.28662455\n",
      "Trained batch 944 batch loss 1.20485651 epoch total loss 1.286538\n",
      "Trained batch 945 batch loss 1.17298543 epoch total loss 1.28641772\n",
      "Trained batch 946 batch loss 1.25021267 epoch total loss 1.28637958\n",
      "Trained batch 947 batch loss 1.25685847 epoch total loss 1.28634834\n",
      "Trained batch 948 batch loss 1.27757978 epoch total loss 1.28633904\n",
      "Trained batch 949 batch loss 1.27816224 epoch total loss 1.28633046\n",
      "Trained batch 950 batch loss 1.25512898 epoch total loss 1.28629768\n",
      "Trained batch 951 batch loss 1.13410032 epoch total loss 1.2861377\n",
      "Trained batch 952 batch loss 1.13606918 epoch total loss 1.28598011\n",
      "Trained batch 953 batch loss 1.28290606 epoch total loss 1.28597689\n",
      "Trained batch 954 batch loss 1.25713837 epoch total loss 1.28594661\n",
      "Trained batch 955 batch loss 1.35121489 epoch total loss 1.28601491\n",
      "Trained batch 956 batch loss 1.49460268 epoch total loss 1.28623319\n",
      "Trained batch 957 batch loss 1.24698746 epoch total loss 1.28619218\n",
      "Trained batch 958 batch loss 1.22099698 epoch total loss 1.28612399\n",
      "Trained batch 959 batch loss 1.154814 epoch total loss 1.28598702\n",
      "Trained batch 960 batch loss 1.22199893 epoch total loss 1.2859205\n",
      "Trained batch 961 batch loss 1.2889297 epoch total loss 1.2859236\n",
      "Trained batch 962 batch loss 1.36579096 epoch total loss 1.28600669\n",
      "Trained batch 963 batch loss 1.26647639 epoch total loss 1.28598642\n",
      "Trained batch 964 batch loss 1.42877936 epoch total loss 1.2861346\n",
      "Trained batch 965 batch loss 1.30730748 epoch total loss 1.28615642\n",
      "Trained batch 966 batch loss 1.3819418 epoch total loss 1.2862556\n",
      "Trained batch 967 batch loss 1.29656482 epoch total loss 1.28626621\n",
      "Trained batch 968 batch loss 1.27972 epoch total loss 1.28625941\n",
      "Trained batch 969 batch loss 1.21172547 epoch total loss 1.2861824\n",
      "Trained batch 970 batch loss 1.31811857 epoch total loss 1.28621542\n",
      "Trained batch 971 batch loss 1.34296846 epoch total loss 1.28627384\n",
      "Trained batch 972 batch loss 1.18887389 epoch total loss 1.28617358\n",
      "Trained batch 973 batch loss 1.21776354 epoch total loss 1.28610337\n",
      "Trained batch 974 batch loss 1.19171572 epoch total loss 1.28600645\n",
      "Trained batch 975 batch loss 1.25139189 epoch total loss 1.28597093\n",
      "Trained batch 976 batch loss 1.10593843 epoch total loss 1.28578651\n",
      "Trained batch 977 batch loss 1.00338829 epoch total loss 1.28549743\n",
      "Trained batch 978 batch loss 1.12970054 epoch total loss 1.28533828\n",
      "Trained batch 979 batch loss 1.32911658 epoch total loss 1.28538299\n",
      "Trained batch 980 batch loss 1.35741687 epoch total loss 1.28545642\n",
      "Trained batch 981 batch loss 1.38781726 epoch total loss 1.28556085\n",
      "Trained batch 982 batch loss 1.34067774 epoch total loss 1.28561699\n",
      "Trained batch 983 batch loss 1.4488343 epoch total loss 1.28578305\n",
      "Trained batch 984 batch loss 1.30558729 epoch total loss 1.28580308\n",
      "Trained batch 985 batch loss 1.36833835 epoch total loss 1.28588676\n",
      "Trained batch 986 batch loss 1.35576916 epoch total loss 1.28595757\n",
      "Trained batch 987 batch loss 1.32829988 epoch total loss 1.28600049\n",
      "Trained batch 988 batch loss 1.33656943 epoch total loss 1.28605163\n",
      "Trained batch 989 batch loss 1.21751809 epoch total loss 1.28598237\n",
      "Trained batch 990 batch loss 1.11292279 epoch total loss 1.28580749\n",
      "Trained batch 991 batch loss 1.10365057 epoch total loss 1.28562367\n",
      "Trained batch 992 batch loss 1.20209432 epoch total loss 1.28553951\n",
      "Trained batch 993 batch loss 1.25934517 epoch total loss 1.28551328\n",
      "Trained batch 994 batch loss 1.291008 epoch total loss 1.28551877\n",
      "Trained batch 995 batch loss 1.39970803 epoch total loss 1.28563344\n",
      "Trained batch 996 batch loss 1.2668103 epoch total loss 1.28561461\n",
      "Trained batch 997 batch loss 1.35156274 epoch total loss 1.28568077\n",
      "Trained batch 998 batch loss 1.31419885 epoch total loss 1.28570938\n",
      "Trained batch 999 batch loss 1.22117507 epoch total loss 1.28564477\n",
      "Trained batch 1000 batch loss 1.27357841 epoch total loss 1.28563273\n",
      "Trained batch 1001 batch loss 1.40472519 epoch total loss 1.2857517\n",
      "Trained batch 1002 batch loss 1.33388984 epoch total loss 1.28579974\n",
      "Trained batch 1003 batch loss 1.26115406 epoch total loss 1.28577507\n",
      "Trained batch 1004 batch loss 1.41413713 epoch total loss 1.28590298\n",
      "Trained batch 1005 batch loss 1.26112747 epoch total loss 1.2858783\n",
      "Trained batch 1006 batch loss 1.28935635 epoch total loss 1.28588176\n",
      "Trained batch 1007 batch loss 1.30183387 epoch total loss 1.28589761\n",
      "Trained batch 1008 batch loss 1.18791032 epoch total loss 1.28580034\n",
      "Trained batch 1009 batch loss 1.37962031 epoch total loss 1.28589344\n",
      "Trained batch 1010 batch loss 1.44082952 epoch total loss 1.28604674\n",
      "Trained batch 1011 batch loss 1.30562472 epoch total loss 1.28606617\n",
      "Trained batch 1012 batch loss 1.32956994 epoch total loss 1.28610921\n",
      "Trained batch 1013 batch loss 1.2294749 epoch total loss 1.2860533\n",
      "Trained batch 1014 batch loss 1.05760241 epoch total loss 1.28582799\n",
      "Trained batch 1015 batch loss 1.18519378 epoch total loss 1.28572881\n",
      "Trained batch 1016 batch loss 1.2300086 epoch total loss 1.28567398\n",
      "Trained batch 1017 batch loss 1.23853707 epoch total loss 1.2856276\n",
      "Trained batch 1018 batch loss 1.35281289 epoch total loss 1.28569353\n",
      "Trained batch 1019 batch loss 1.31591654 epoch total loss 1.28572321\n",
      "Trained batch 1020 batch loss 1.28040802 epoch total loss 1.28571796\n",
      "Trained batch 1021 batch loss 1.35205388 epoch total loss 1.28578293\n",
      "Trained batch 1022 batch loss 1.56111228 epoch total loss 1.28605247\n",
      "Trained batch 1023 batch loss 1.43323755 epoch total loss 1.28619635\n",
      "Trained batch 1024 batch loss 1.39523101 epoch total loss 1.2863028\n",
      "Trained batch 1025 batch loss 1.32078922 epoch total loss 1.28633642\n",
      "Trained batch 1026 batch loss 1.26225 epoch total loss 1.28631294\n",
      "Trained batch 1027 batch loss 1.19118655 epoch total loss 1.28622031\n",
      "Trained batch 1028 batch loss 1.29270911 epoch total loss 1.28622663\n",
      "Trained batch 1029 batch loss 1.28246248 epoch total loss 1.28622293\n",
      "Trained batch 1030 batch loss 1.22958517 epoch total loss 1.28616798\n",
      "Trained batch 1031 batch loss 1.22695661 epoch total loss 1.28611052\n",
      "Trained batch 1032 batch loss 1.15895331 epoch total loss 1.28598738\n",
      "Trained batch 1033 batch loss 1.13239169 epoch total loss 1.28583872\n",
      "Trained batch 1034 batch loss 1.17604518 epoch total loss 1.28573251\n",
      "Trained batch 1035 batch loss 1.30563307 epoch total loss 1.2857517\n",
      "Trained batch 1036 batch loss 1.51287937 epoch total loss 1.28597105\n",
      "Trained batch 1037 batch loss 1.55190897 epoch total loss 1.28622746\n",
      "Trained batch 1038 batch loss 1.45751882 epoch total loss 1.28639245\n",
      "Trained batch 1039 batch loss 1.38145781 epoch total loss 1.286484\n",
      "Trained batch 1040 batch loss 1.4823159 epoch total loss 1.28667223\n",
      "Trained batch 1041 batch loss 1.4464643 epoch total loss 1.28682578\n",
      "Trained batch 1042 batch loss 1.38483047 epoch total loss 1.28691983\n",
      "Trained batch 1043 batch loss 1.28817844 epoch total loss 1.28692102\n",
      "Trained batch 1044 batch loss 1.31504691 epoch total loss 1.28694797\n",
      "Trained batch 1045 batch loss 1.32283866 epoch total loss 1.28698242\n",
      "Trained batch 1046 batch loss 1.31320441 epoch total loss 1.28700745\n",
      "Trained batch 1047 batch loss 1.33770084 epoch total loss 1.28705585\n",
      "Trained batch 1048 batch loss 1.21237159 epoch total loss 1.28698468\n",
      "Trained batch 1049 batch loss 1.40783095 epoch total loss 1.28709984\n",
      "Trained batch 1050 batch loss 1.30269551 epoch total loss 1.28711474\n",
      "Trained batch 1051 batch loss 1.25367594 epoch total loss 1.28708291\n",
      "Trained batch 1052 batch loss 1.3477962 epoch total loss 1.28714061\n",
      "Trained batch 1053 batch loss 1.25306511 epoch total loss 1.28710818\n",
      "Trained batch 1054 batch loss 1.34881353 epoch total loss 1.28716671\n",
      "Trained batch 1055 batch loss 1.26242387 epoch total loss 1.28714323\n",
      "Trained batch 1056 batch loss 1.17135525 epoch total loss 1.28703368\n",
      "Trained batch 1057 batch loss 1.07604432 epoch total loss 1.286834\n",
      "Trained batch 1058 batch loss 1.15406013 epoch total loss 1.28670859\n",
      "Trained batch 1059 batch loss 1.14663744 epoch total loss 1.28657627\n",
      "Trained batch 1060 batch loss 1.18959367 epoch total loss 1.28648472\n",
      "Trained batch 1061 batch loss 1.16183019 epoch total loss 1.2863673\n",
      "Trained batch 1062 batch loss 1.113276 epoch total loss 1.28620434\n",
      "Trained batch 1063 batch loss 1.24784172 epoch total loss 1.28616822\n",
      "Trained batch 1064 batch loss 1.12338328 epoch total loss 1.28601527\n",
      "Trained batch 1065 batch loss 1.26982009 epoch total loss 1.286\n",
      "Trained batch 1066 batch loss 1.31921649 epoch total loss 1.28603113\n",
      "Trained batch 1067 batch loss 1.22804236 epoch total loss 1.28597677\n",
      "Trained batch 1068 batch loss 1.29018021 epoch total loss 1.2859807\n",
      "Trained batch 1069 batch loss 1.23667967 epoch total loss 1.28593457\n",
      "Trained batch 1070 batch loss 1.2782191 epoch total loss 1.2859273\n",
      "Trained batch 1071 batch loss 1.42883921 epoch total loss 1.28606081\n",
      "Trained batch 1072 batch loss 1.15710735 epoch total loss 1.28594053\n",
      "Trained batch 1073 batch loss 1.04863632 epoch total loss 1.28571928\n",
      "Trained batch 1074 batch loss 1.08747578 epoch total loss 1.28553474\n",
      "Trained batch 1075 batch loss 1.149598 epoch total loss 1.28540838\n",
      "Trained batch 1076 batch loss 1.21973372 epoch total loss 1.28534734\n",
      "Trained batch 1077 batch loss 1.23748171 epoch total loss 1.28530276\n",
      "Trained batch 1078 batch loss 1.16891742 epoch total loss 1.28519487\n",
      "Trained batch 1079 batch loss 1.33382916 epoch total loss 1.28523993\n",
      "Trained batch 1080 batch loss 1.3697269 epoch total loss 1.28531826\n",
      "Trained batch 1081 batch loss 1.27287912 epoch total loss 1.28530669\n",
      "Trained batch 1082 batch loss 1.41138792 epoch total loss 1.28542316\n",
      "Trained batch 1083 batch loss 1.45621336 epoch total loss 1.28558087\n",
      "Trained batch 1084 batch loss 1.4077177 epoch total loss 1.28569353\n",
      "Trained batch 1085 batch loss 1.29824281 epoch total loss 1.28570509\n",
      "Trained batch 1086 batch loss 1.38501072 epoch total loss 1.28579652\n",
      "Trained batch 1087 batch loss 1.32108259 epoch total loss 1.28582895\n",
      "Trained batch 1088 batch loss 1.2878654 epoch total loss 1.28583074\n",
      "Trained batch 1089 batch loss 1.50769341 epoch total loss 1.28603446\n",
      "Trained batch 1090 batch loss 1.53298116 epoch total loss 1.28626108\n",
      "Trained batch 1091 batch loss 1.36763835 epoch total loss 1.28633571\n",
      "Trained batch 1092 batch loss 1.23577785 epoch total loss 1.28628933\n",
      "Trained batch 1093 batch loss 1.16313517 epoch total loss 1.28617656\n",
      "Trained batch 1094 batch loss 1.03626525 epoch total loss 1.28594816\n",
      "Trained batch 1095 batch loss 1.1124208 epoch total loss 1.28578973\n",
      "Trained batch 1096 batch loss 1.30366421 epoch total loss 1.28580606\n",
      "Trained batch 1097 batch loss 0.98518 epoch total loss 1.285532\n",
      "Trained batch 1098 batch loss 1.00296581 epoch total loss 1.28527462\n",
      "Trained batch 1099 batch loss 1.06324255 epoch total loss 1.28507257\n",
      "Trained batch 1100 batch loss 1.14571667 epoch total loss 1.28494596\n",
      "Trained batch 1101 batch loss 1.22513294 epoch total loss 1.28489161\n",
      "Trained batch 1102 batch loss 1.22389269 epoch total loss 1.28483617\n",
      "Trained batch 1103 batch loss 1.27180302 epoch total loss 1.28482449\n",
      "Trained batch 1104 batch loss 1.2480818 epoch total loss 1.28479111\n",
      "Trained batch 1105 batch loss 1.32125306 epoch total loss 1.28482413\n",
      "Trained batch 1106 batch loss 1.34603596 epoch total loss 1.28487957\n",
      "Trained batch 1107 batch loss 1.39120412 epoch total loss 1.28497565\n",
      "Trained batch 1108 batch loss 1.38628685 epoch total loss 1.28506696\n",
      "Trained batch 1109 batch loss 1.36261618 epoch total loss 1.28513694\n",
      "Trained batch 1110 batch loss 1.20286536 epoch total loss 1.28506291\n",
      "Trained batch 1111 batch loss 1.20587528 epoch total loss 1.28499162\n",
      "Trained batch 1112 batch loss 1.3046788 epoch total loss 1.28500938\n",
      "Trained batch 1113 batch loss 1.24221826 epoch total loss 1.28497088\n",
      "Trained batch 1114 batch loss 1.08288479 epoch total loss 1.28478944\n",
      "Trained batch 1115 batch loss 1.11416042 epoch total loss 1.2846365\n",
      "Trained batch 1116 batch loss 1.13463151 epoch total loss 1.28450203\n",
      "Trained batch 1117 batch loss 1.21863639 epoch total loss 1.28444302\n",
      "Trained batch 1118 batch loss 1.14092886 epoch total loss 1.28431463\n",
      "Trained batch 1119 batch loss 1.10652363 epoch total loss 1.28415585\n",
      "Trained batch 1120 batch loss 1.18953311 epoch total loss 1.28407133\n",
      "Trained batch 1121 batch loss 1.20809293 epoch total loss 1.28400362\n",
      "Trained batch 1122 batch loss 1.25103593 epoch total loss 1.28397417\n",
      "Trained batch 1123 batch loss 1.1916393 epoch total loss 1.28389192\n",
      "Trained batch 1124 batch loss 1.16863799 epoch total loss 1.2837894\n",
      "Trained batch 1125 batch loss 1.33897674 epoch total loss 1.28383839\n",
      "Trained batch 1126 batch loss 1.38590312 epoch total loss 1.28392899\n",
      "Trained batch 1127 batch loss 1.29081511 epoch total loss 1.28393507\n",
      "Trained batch 1128 batch loss 1.23096454 epoch total loss 1.2838881\n",
      "Trained batch 1129 batch loss 1.19611382 epoch total loss 1.2838105\n",
      "Trained batch 1130 batch loss 1.21297169 epoch total loss 1.28374779\n",
      "Trained batch 1131 batch loss 1.2262013 epoch total loss 1.28369689\n",
      "Trained batch 1132 batch loss 1.21813285 epoch total loss 1.28363895\n",
      "Trained batch 1133 batch loss 1.1911974 epoch total loss 1.28355742\n",
      "Trained batch 1134 batch loss 1.24888182 epoch total loss 1.28352678\n",
      "Trained batch 1135 batch loss 1.16417277 epoch total loss 1.28342164\n",
      "Trained batch 1136 batch loss 1.16633797 epoch total loss 1.28331864\n",
      "Trained batch 1137 batch loss 1.0700748 epoch total loss 1.28313112\n",
      "Trained batch 1138 batch loss 1.00768507 epoch total loss 1.28288901\n",
      "Trained batch 1139 batch loss 1.19364476 epoch total loss 1.28281069\n",
      "Trained batch 1140 batch loss 1.28497136 epoch total loss 1.28281248\n",
      "Trained batch 1141 batch loss 1.36276603 epoch total loss 1.28288257\n",
      "Trained batch 1142 batch loss 1.26735592 epoch total loss 1.28286898\n",
      "Trained batch 1143 batch loss 1.29991114 epoch total loss 1.28288388\n",
      "Trained batch 1144 batch loss 1.18305254 epoch total loss 1.28279674\n",
      "Trained batch 1145 batch loss 1.17517209 epoch total loss 1.28270268\n",
      "Trained batch 1146 batch loss 1.19484472 epoch total loss 1.28262603\n",
      "Trained batch 1147 batch loss 1.22279072 epoch total loss 1.28257382\n",
      "Trained batch 1148 batch loss 1.33999 epoch total loss 1.28262377\n",
      "Trained batch 1149 batch loss 1.22986746 epoch total loss 1.28257787\n",
      "Trained batch 1150 batch loss 1.12310135 epoch total loss 1.28243923\n",
      "Trained batch 1151 batch loss 1.2301389 epoch total loss 1.28239369\n",
      "Trained batch 1152 batch loss 1.20281911 epoch total loss 1.28232455\n",
      "Trained batch 1153 batch loss 1.30868185 epoch total loss 1.28234744\n",
      "Trained batch 1154 batch loss 1.27793205 epoch total loss 1.28234363\n",
      "Trained batch 1155 batch loss 1.37213397 epoch total loss 1.28242147\n",
      "Trained batch 1156 batch loss 1.1861496 epoch total loss 1.28233814\n",
      "Trained batch 1157 batch loss 1.09656537 epoch total loss 1.28217757\n",
      "Trained batch 1158 batch loss 1.35649776 epoch total loss 1.2822417\n",
      "Trained batch 1159 batch loss 1.28589547 epoch total loss 1.28224492\n",
      "Trained batch 1160 batch loss 1.19566333 epoch total loss 1.2821703\n",
      "Trained batch 1161 batch loss 1.10265112 epoch total loss 1.28201568\n",
      "Trained batch 1162 batch loss 1.14440441 epoch total loss 1.28189719\n",
      "Trained batch 1163 batch loss 1.2913022 epoch total loss 1.28190529\n",
      "Trained batch 1164 batch loss 1.2259984 epoch total loss 1.28185725\n",
      "Trained batch 1165 batch loss 1.42557967 epoch total loss 1.28198051\n",
      "Trained batch 1166 batch loss 1.28968501 epoch total loss 1.28198719\n",
      "Trained batch 1167 batch loss 1.05732715 epoch total loss 1.28179467\n",
      "Trained batch 1168 batch loss 1.06015813 epoch total loss 1.28160489\n",
      "Trained batch 1169 batch loss 1.01855552 epoch total loss 1.28137994\n",
      "Trained batch 1170 batch loss 1.11845624 epoch total loss 1.28124058\n",
      "Trained batch 1171 batch loss 1.12472951 epoch total loss 1.28110695\n",
      "Trained batch 1172 batch loss 1.22496152 epoch total loss 1.28105903\n",
      "Trained batch 1173 batch loss 1.17902458 epoch total loss 1.28097212\n",
      "Trained batch 1174 batch loss 1.23476934 epoch total loss 1.28093278\n",
      "Trained batch 1175 batch loss 1.26946402 epoch total loss 1.28092301\n",
      "Trained batch 1176 batch loss 1.40377533 epoch total loss 1.28102744\n",
      "Trained batch 1177 batch loss 1.41226721 epoch total loss 1.2811389\n",
      "Trained batch 1178 batch loss 1.31628704 epoch total loss 1.2811687\n",
      "Trained batch 1179 batch loss 1.27662826 epoch total loss 1.28116488\n",
      "Trained batch 1180 batch loss 1.23567653 epoch total loss 1.28112638\n",
      "Trained batch 1181 batch loss 1.23757291 epoch total loss 1.28108943\n",
      "Trained batch 1182 batch loss 1.29468215 epoch total loss 1.28110099\n",
      "Trained batch 1183 batch loss 1.36402702 epoch total loss 1.28117108\n",
      "Trained batch 1184 batch loss 1.54918289 epoch total loss 1.28139746\n",
      "Trained batch 1185 batch loss 1.45756018 epoch total loss 1.28154612\n",
      "Trained batch 1186 batch loss 1.30519843 epoch total loss 1.28156602\n",
      "Trained batch 1187 batch loss 1.29601312 epoch total loss 1.28157818\n",
      "Trained batch 1188 batch loss 1.43583655 epoch total loss 1.281708\n",
      "Trained batch 1189 batch loss 1.2922318 epoch total loss 1.28171682\n",
      "Trained batch 1190 batch loss 1.41916847 epoch total loss 1.28183234\n",
      "Trained batch 1191 batch loss 1.34336829 epoch total loss 1.28188407\n",
      "Trained batch 1192 batch loss 1.10329902 epoch total loss 1.28173423\n",
      "Trained batch 1193 batch loss 1.29677963 epoch total loss 1.28174675\n",
      "Trained batch 1194 batch loss 1.25483668 epoch total loss 1.28172433\n",
      "Trained batch 1195 batch loss 1.29247141 epoch total loss 1.28173327\n",
      "Trained batch 1196 batch loss 1.32015514 epoch total loss 1.28176546\n",
      "Trained batch 1197 batch loss 1.27535355 epoch total loss 1.2817601\n",
      "Trained batch 1198 batch loss 1.24918628 epoch total loss 1.28173292\n",
      "Trained batch 1199 batch loss 1.25931311 epoch total loss 1.2817142\n",
      "Trained batch 1200 batch loss 1.19223511 epoch total loss 1.28163958\n",
      "Trained batch 1201 batch loss 1.24271703 epoch total loss 1.28160715\n",
      "Trained batch 1202 batch loss 1.25048709 epoch total loss 1.28158128\n",
      "Trained batch 1203 batch loss 1.23545074 epoch total loss 1.2815429\n",
      "Trained batch 1204 batch loss 1.22471619 epoch total loss 1.28149581\n",
      "Trained batch 1205 batch loss 1.23720312 epoch total loss 1.28145897\n",
      "Trained batch 1206 batch loss 1.17217636 epoch total loss 1.28136837\n",
      "Trained batch 1207 batch loss 1.192976 epoch total loss 1.28129506\n",
      "Trained batch 1208 batch loss 1.24194634 epoch total loss 1.28126252\n",
      "Trained batch 1209 batch loss 1.22892833 epoch total loss 1.28121924\n",
      "Trained batch 1210 batch loss 1.17048228 epoch total loss 1.28112769\n",
      "Trained batch 1211 batch loss 1.26372147 epoch total loss 1.28111327\n",
      "Trained batch 1212 batch loss 1.17756462 epoch total loss 1.28102791\n",
      "Trained batch 1213 batch loss 1.27420306 epoch total loss 1.28102231\n",
      "Trained batch 1214 batch loss 1.37476444 epoch total loss 1.28109944\n",
      "Trained batch 1215 batch loss 1.23155212 epoch total loss 1.28105867\n",
      "Trained batch 1216 batch loss 1.19998658 epoch total loss 1.28099203\n",
      "Trained batch 1217 batch loss 1.28873193 epoch total loss 1.28099835\n",
      "Trained batch 1218 batch loss 1.43358517 epoch total loss 1.28112364\n",
      "Trained batch 1219 batch loss 1.35795319 epoch total loss 1.28118658\n",
      "Trained batch 1220 batch loss 1.18086934 epoch total loss 1.28110445\n",
      "Trained batch 1221 batch loss 1.19827569 epoch total loss 1.2810365\n",
      "Trained batch 1222 batch loss 1.0816617 epoch total loss 1.28087342\n",
      "Trained batch 1223 batch loss 1.11558366 epoch total loss 1.28073823\n",
      "Trained batch 1224 batch loss 1.25784993 epoch total loss 1.28071952\n",
      "Trained batch 1225 batch loss 1.28034139 epoch total loss 1.28071928\n",
      "Trained batch 1226 batch loss 1.2754662 epoch total loss 1.28071499\n",
      "Trained batch 1227 batch loss 1.1022594 epoch total loss 1.28056955\n",
      "Trained batch 1228 batch loss 1.10556841 epoch total loss 1.2804271\n",
      "Trained batch 1229 batch loss 1.28178501 epoch total loss 1.28042817\n",
      "Trained batch 1230 batch loss 1.38729453 epoch total loss 1.28051507\n",
      "Trained batch 1231 batch loss 1.26209068 epoch total loss 1.28050017\n",
      "Trained batch 1232 batch loss 1.26051688 epoch total loss 1.28048384\n",
      "Trained batch 1233 batch loss 1.10492373 epoch total loss 1.28034151\n",
      "Trained batch 1234 batch loss 0.95496124 epoch total loss 1.28007782\n",
      "Trained batch 1235 batch loss 0.970861197 epoch total loss 1.27982748\n",
      "Trained batch 1236 batch loss 1.07703793 epoch total loss 1.27966332\n",
      "Trained batch 1237 batch loss 1.20726728 epoch total loss 1.27960491\n",
      "Trained batch 1238 batch loss 1.17259431 epoch total loss 1.27951849\n",
      "Trained batch 1239 batch loss 1.20797598 epoch total loss 1.27946067\n",
      "Trained batch 1240 batch loss 1.34631193 epoch total loss 1.27951467\n",
      "Trained batch 1241 batch loss 1.30837512 epoch total loss 1.27953792\n",
      "Trained batch 1242 batch loss 1.29906416 epoch total loss 1.27955365\n",
      "Trained batch 1243 batch loss 1.22227263 epoch total loss 1.27950752\n",
      "Trained batch 1244 batch loss 1.19055891 epoch total loss 1.27943599\n",
      "Trained batch 1245 batch loss 1.18397844 epoch total loss 1.27935934\n",
      "Trained batch 1246 batch loss 1.20225799 epoch total loss 1.27929747\n",
      "Trained batch 1247 batch loss 1.31125021 epoch total loss 1.2793231\n",
      "Trained batch 1248 batch loss 1.39108682 epoch total loss 1.27941263\n",
      "Trained batch 1249 batch loss 1.29341626 epoch total loss 1.27942395\n",
      "Trained batch 1250 batch loss 1.25871778 epoch total loss 1.27940738\n",
      "Trained batch 1251 batch loss 1.30825043 epoch total loss 1.27943039\n",
      "Trained batch 1252 batch loss 1.31236517 epoch total loss 1.27945673\n",
      "Trained batch 1253 batch loss 1.28767157 epoch total loss 1.27946329\n",
      "Trained batch 1254 batch loss 1.19746971 epoch total loss 1.27939796\n",
      "Trained batch 1255 batch loss 1.21396959 epoch total loss 1.27934575\n",
      "Trained batch 1256 batch loss 1.2446723 epoch total loss 1.27931821\n",
      "Trained batch 1257 batch loss 1.24732041 epoch total loss 1.2792927\n",
      "Trained batch 1258 batch loss 1.17646146 epoch total loss 1.27921104\n",
      "Trained batch 1259 batch loss 1.20755982 epoch total loss 1.27915406\n",
      "Trained batch 1260 batch loss 1.19773507 epoch total loss 1.27908945\n",
      "Trained batch 1261 batch loss 1.2452749 epoch total loss 1.27906263\n",
      "Trained batch 1262 batch loss 1.27852941 epoch total loss 1.27906227\n",
      "Trained batch 1263 batch loss 1.24545527 epoch total loss 1.27903557\n",
      "Trained batch 1264 batch loss 1.40724325 epoch total loss 1.27913702\n",
      "Trained batch 1265 batch loss 1.36493945 epoch total loss 1.27920496\n",
      "Trained batch 1266 batch loss 1.41904712 epoch total loss 1.27931535\n",
      "Trained batch 1267 batch loss 1.37234604 epoch total loss 1.27938879\n",
      "Trained batch 1268 batch loss 1.37981629 epoch total loss 1.27946794\n",
      "Trained batch 1269 batch loss 1.22875094 epoch total loss 1.27942801\n",
      "Trained batch 1270 batch loss 1.23323 epoch total loss 1.27939165\n",
      "Trained batch 1271 batch loss 1.29182124 epoch total loss 1.27940142\n",
      "Trained batch 1272 batch loss 1.44317293 epoch total loss 1.27953017\n",
      "Trained batch 1273 batch loss 1.18431294 epoch total loss 1.27945542\n",
      "Trained batch 1274 batch loss 1.21088934 epoch total loss 1.27940166\n",
      "Trained batch 1275 batch loss 1.16844022 epoch total loss 1.27931464\n",
      "Trained batch 1276 batch loss 1.20882666 epoch total loss 1.27925932\n",
      "Trained batch 1277 batch loss 1.15556216 epoch total loss 1.27916253\n",
      "Trained batch 1278 batch loss 1.21928954 epoch total loss 1.27911556\n",
      "Trained batch 1279 batch loss 1.30272233 epoch total loss 1.27913404\n",
      "Trained batch 1280 batch loss 1.29063201 epoch total loss 1.2791431\n",
      "Trained batch 1281 batch loss 1.36591816 epoch total loss 1.27921081\n",
      "Trained batch 1282 batch loss 1.35676038 epoch total loss 1.27927136\n",
      "Trained batch 1283 batch loss 1.24914479 epoch total loss 1.27924788\n",
      "Trained batch 1284 batch loss 1.2128458 epoch total loss 1.27919614\n",
      "Trained batch 1285 batch loss 1.22636688 epoch total loss 1.27915502\n",
      "Trained batch 1286 batch loss 1.17304993 epoch total loss 1.27907252\n",
      "Trained batch 1287 batch loss 1.21298969 epoch total loss 1.27902126\n",
      "Trained batch 1288 batch loss 1.29277384 epoch total loss 1.27903187\n",
      "Trained batch 1289 batch loss 1.26128447 epoch total loss 1.27901804\n",
      "Trained batch 1290 batch loss 1.2746346 epoch total loss 1.27901471\n",
      "Trained batch 1291 batch loss 1.24384189 epoch total loss 1.27898753\n",
      "Trained batch 1292 batch loss 1.23122239 epoch total loss 1.27895045\n",
      "Trained batch 1293 batch loss 1.16338134 epoch total loss 1.27886105\n",
      "Trained batch 1294 batch loss 1.23427546 epoch total loss 1.27882659\n",
      "Trained batch 1295 batch loss 1.23772395 epoch total loss 1.27879488\n",
      "Trained batch 1296 batch loss 1.27092433 epoch total loss 1.27878869\n",
      "Trained batch 1297 batch loss 1.25821102 epoch total loss 1.27877283\n",
      "Trained batch 1298 batch loss 1.32767022 epoch total loss 1.2788105\n",
      "Trained batch 1299 batch loss 1.20490634 epoch total loss 1.27875364\n",
      "Trained batch 1300 batch loss 1.18687391 epoch total loss 1.27868295\n",
      "Trained batch 1301 batch loss 1.18962801 epoch total loss 1.27861452\n",
      "Trained batch 1302 batch loss 1.17158639 epoch total loss 1.27853227\n",
      "Trained batch 1303 batch loss 1.219841 epoch total loss 1.27848721\n",
      "Trained batch 1304 batch loss 1.06519246 epoch total loss 1.27832365\n",
      "Trained batch 1305 batch loss 1.04758501 epoch total loss 1.27814686\n",
      "Trained batch 1306 batch loss 1.18345487 epoch total loss 1.27807438\n",
      "Trained batch 1307 batch loss 1.24348211 epoch total loss 1.27804792\n",
      "Trained batch 1308 batch loss 1.30215096 epoch total loss 1.2780664\n",
      "Trained batch 1309 batch loss 1.27835381 epoch total loss 1.27806652\n",
      "Trained batch 1310 batch loss 1.34065485 epoch total loss 1.27811432\n",
      "Trained batch 1311 batch loss 1.39393771 epoch total loss 1.27820265\n",
      "Trained batch 1312 batch loss 1.13058853 epoch total loss 1.27809024\n",
      "Trained batch 1313 batch loss 1.24436975 epoch total loss 1.27806461\n",
      "Trained batch 1314 batch loss 1.19258475 epoch total loss 1.27799952\n",
      "Trained batch 1315 batch loss 1.31865764 epoch total loss 1.2780304\n",
      "Trained batch 1316 batch loss 1.15524888 epoch total loss 1.27793717\n",
      "Trained batch 1317 batch loss 1.0054934 epoch total loss 1.27773023\n",
      "Trained batch 1318 batch loss 0.997376204 epoch total loss 1.27751756\n",
      "Trained batch 1319 batch loss 1.13477468 epoch total loss 1.27740932\n",
      "Trained batch 1320 batch loss 1.43555 epoch total loss 1.27752912\n",
      "Trained batch 1321 batch loss 1.5473268 epoch total loss 1.27773345\n",
      "Trained batch 1322 batch loss 1.41930103 epoch total loss 1.2778405\n",
      "Trained batch 1323 batch loss 1.33760524 epoch total loss 1.27788568\n",
      "Trained batch 1324 batch loss 1.30449128 epoch total loss 1.27790582\n",
      "Trained batch 1325 batch loss 1.23677123 epoch total loss 1.27787483\n",
      "Trained batch 1326 batch loss 1.27599895 epoch total loss 1.2778734\n",
      "Trained batch 1327 batch loss 1.33708215 epoch total loss 1.27791798\n",
      "Trained batch 1328 batch loss 1.32397866 epoch total loss 1.27795267\n",
      "Trained batch 1329 batch loss 1.28115582 epoch total loss 1.27795506\n",
      "Trained batch 1330 batch loss 1.15721464 epoch total loss 1.27786422\n",
      "Trained batch 1331 batch loss 1.23156285 epoch total loss 1.27782941\n",
      "Trained batch 1332 batch loss 1.19110143 epoch total loss 1.27776444\n",
      "Trained batch 1333 batch loss 1.17868972 epoch total loss 1.27769\n",
      "Trained batch 1334 batch loss 1.16723633 epoch total loss 1.27760732\n",
      "Trained batch 1335 batch loss 1.21598268 epoch total loss 1.27756107\n",
      "Trained batch 1336 batch loss 1.24445415 epoch total loss 1.27753639\n",
      "Trained batch 1337 batch loss 1.21291673 epoch total loss 1.27748799\n",
      "Trained batch 1338 batch loss 1.20690966 epoch total loss 1.2774353\n",
      "Trained batch 1339 batch loss 1.21274257 epoch total loss 1.2773869\n",
      "Trained batch 1340 batch loss 1.08626485 epoch total loss 1.27724433\n",
      "Trained batch 1341 batch loss 1.34788275 epoch total loss 1.27729702\n",
      "Trained batch 1342 batch loss 1.35985363 epoch total loss 1.27735853\n",
      "Trained batch 1343 batch loss 1.29435742 epoch total loss 1.27737117\n",
      "Trained batch 1344 batch loss 1.21707439 epoch total loss 1.27732635\n",
      "Trained batch 1345 batch loss 1.07848096 epoch total loss 1.27717853\n",
      "Trained batch 1346 batch loss 1.15940404 epoch total loss 1.27709103\n",
      "Trained batch 1347 batch loss 1.07218289 epoch total loss 1.2769388\n",
      "Trained batch 1348 batch loss 1.22494411 epoch total loss 1.27690029\n",
      "Trained batch 1349 batch loss 1.1400708 epoch total loss 1.27679884\n",
      "Trained batch 1350 batch loss 1.15205622 epoch total loss 1.27670646\n",
      "Trained batch 1351 batch loss 1.13145018 epoch total loss 1.27659893\n",
      "Trained batch 1352 batch loss 1.18194973 epoch total loss 1.27652895\n",
      "Trained batch 1353 batch loss 1.191625 epoch total loss 1.27646625\n",
      "Trained batch 1354 batch loss 1.15491164 epoch total loss 1.27637649\n",
      "Trained batch 1355 batch loss 1.3493464 epoch total loss 1.27643037\n",
      "Trained batch 1356 batch loss 1.03958559 epoch total loss 1.27625561\n",
      "Trained batch 1357 batch loss 1.12462783 epoch total loss 1.27614391\n",
      "Trained batch 1358 batch loss 1.20991898 epoch total loss 1.27609515\n",
      "Trained batch 1359 batch loss 1.23692632 epoch total loss 1.27606642\n",
      "Trained batch 1360 batch loss 1.17930377 epoch total loss 1.27599525\n",
      "Trained batch 1361 batch loss 1.42488158 epoch total loss 1.27610469\n",
      "Trained batch 1362 batch loss 1.54878712 epoch total loss 1.27630484\n",
      "Trained batch 1363 batch loss 1.39544165 epoch total loss 1.27639222\n",
      "Trained batch 1364 batch loss 1.13359165 epoch total loss 1.27628756\n",
      "Trained batch 1365 batch loss 1.24671102 epoch total loss 1.27626586\n",
      "Trained batch 1366 batch loss 1.16326261 epoch total loss 1.27618313\n",
      "Trained batch 1367 batch loss 1.20040309 epoch total loss 1.2761277\n",
      "Trained batch 1368 batch loss 1.43128097 epoch total loss 1.27624106\n",
      "Trained batch 1369 batch loss 1.38319242 epoch total loss 1.27631927\n",
      "Trained batch 1370 batch loss 1.35139585 epoch total loss 1.2763741\n",
      "Trained batch 1371 batch loss 1.394701 epoch total loss 1.27646029\n",
      "Trained batch 1372 batch loss 1.31354284 epoch total loss 1.27648735\n",
      "Trained batch 1373 batch loss 1.27951717 epoch total loss 1.27648962\n",
      "Trained batch 1374 batch loss 1.31023359 epoch total loss 1.27651417\n",
      "Trained batch 1375 batch loss 1.31461072 epoch total loss 1.27654183\n",
      "Trained batch 1376 batch loss 1.23927259 epoch total loss 1.27651477\n",
      "Trained batch 1377 batch loss 1.25110006 epoch total loss 1.27649629\n",
      "Trained batch 1378 batch loss 1.25678408 epoch total loss 1.27648199\n",
      "Trained batch 1379 batch loss 1.29503417 epoch total loss 1.27649546\n",
      "Trained batch 1380 batch loss 1.14316988 epoch total loss 1.2763989\n",
      "Trained batch 1381 batch loss 1.13908577 epoch total loss 1.27629936\n",
      "Trained batch 1382 batch loss 1.18919754 epoch total loss 1.27623641\n",
      "Trained batch 1383 batch loss 1.05793333 epoch total loss 1.27607858\n",
      "Trained batch 1384 batch loss 1.13308382 epoch total loss 1.27597523\n",
      "Trained batch 1385 batch loss 1.37997389 epoch total loss 1.27605033\n",
      "Trained batch 1386 batch loss 1.37502992 epoch total loss 1.27612174\n",
      "Trained batch 1387 batch loss 1.43338811 epoch total loss 1.2762351\n",
      "Trained batch 1388 batch loss 1.43161416 epoch total loss 1.27634704\n",
      "Epoch 3 train loss 1.276347041130066\n",
      "Validated batch 1 batch loss 1.17416859\n",
      "Validated batch 2 batch loss 1.27296531\n",
      "Validated batch 3 batch loss 1.28738105\n",
      "Validated batch 4 batch loss 1.21658373\n",
      "Validated batch 5 batch loss 1.35174227\n",
      "Validated batch 6 batch loss 1.36509705\n",
      "Validated batch 7 batch loss 1.13225198\n",
      "Validated batch 8 batch loss 1.28362858\n",
      "Validated batch 9 batch loss 1.23204625\n",
      "Validated batch 10 batch loss 1.31005907\n",
      "Validated batch 11 batch loss 1.2642194\n",
      "Validated batch 12 batch loss 1.09376729\n",
      "Validated batch 13 batch loss 1.14210463\n",
      "Validated batch 14 batch loss 1.21002221\n",
      "Validated batch 15 batch loss 1.20780039\n",
      "Validated batch 16 batch loss 1.25005841\n",
      "Validated batch 17 batch loss 1.23985183\n",
      "Validated batch 18 batch loss 1.20984209\n",
      "Validated batch 19 batch loss 1.2810905\n",
      "Validated batch 20 batch loss 1.34599066\n",
      "Validated batch 21 batch loss 1.31896484\n",
      "Validated batch 22 batch loss 1.26602685\n",
      "Validated batch 23 batch loss 1.12329102\n",
      "Validated batch 24 batch loss 1.33010793\n",
      "Validated batch 25 batch loss 1.29223812\n",
      "Validated batch 26 batch loss 1.22099757\n",
      "Validated batch 27 batch loss 1.21593499\n",
      "Validated batch 28 batch loss 1.19262922\n",
      "Validated batch 29 batch loss 1.26553297\n",
      "Validated batch 30 batch loss 1.33894181\n",
      "Validated batch 31 batch loss 1.138304\n",
      "Validated batch 32 batch loss 1.27897561\n",
      "Validated batch 33 batch loss 1.22648776\n",
      "Validated batch 34 batch loss 1.29849756\n",
      "Validated batch 35 batch loss 1.27057683\n",
      "Validated batch 36 batch loss 1.28068101\n",
      "Validated batch 37 batch loss 1.22786736\n",
      "Validated batch 38 batch loss 1.22233582\n",
      "Validated batch 39 batch loss 1.26101923\n",
      "Validated batch 40 batch loss 1.25679278\n",
      "Validated batch 41 batch loss 1.29493308\n",
      "Validated batch 42 batch loss 1.34219015\n",
      "Validated batch 43 batch loss 1.49615467\n",
      "Validated batch 44 batch loss 1.34136355\n",
      "Validated batch 45 batch loss 1.26024318\n",
      "Validated batch 46 batch loss 1.15527737\n",
      "Validated batch 47 batch loss 1.25687075\n",
      "Validated batch 48 batch loss 1.22124457\n",
      "Validated batch 49 batch loss 1.20261896\n",
      "Validated batch 50 batch loss 1.2609278\n",
      "Validated batch 51 batch loss 1.29526544\n",
      "Validated batch 52 batch loss 1.3651377\n",
      "Validated batch 53 batch loss 1.13097537\n",
      "Validated batch 54 batch loss 1.2709372\n",
      "Validated batch 55 batch loss 1.2275126\n",
      "Validated batch 56 batch loss 1.28960681\n",
      "Validated batch 57 batch loss 1.25536036\n",
      "Validated batch 58 batch loss 1.14517105\n",
      "Validated batch 59 batch loss 1.40370679\n",
      "Validated batch 60 batch loss 1.20091581\n",
      "Validated batch 61 batch loss 1.33098662\n",
      "Validated batch 62 batch loss 1.22368\n",
      "Validated batch 63 batch loss 1.34792197\n",
      "Validated batch 64 batch loss 1.14635587\n",
      "Validated batch 65 batch loss 1.25761259\n",
      "Validated batch 66 batch loss 1.1973412\n",
      "Validated batch 67 batch loss 1.21710229\n",
      "Validated batch 68 batch loss 1.27955854\n",
      "Validated batch 69 batch loss 1.23573828\n",
      "Validated batch 70 batch loss 1.29575241\n",
      "Validated batch 71 batch loss 1.18766379\n",
      "Validated batch 72 batch loss 1.25892198\n",
      "Validated batch 73 batch loss 1.16725278\n",
      "Validated batch 74 batch loss 1.22399282\n",
      "Validated batch 75 batch loss 1.30921245\n",
      "Validated batch 76 batch loss 1.27176893\n",
      "Validated batch 77 batch loss 1.33990848\n",
      "Validated batch 78 batch loss 1.32130098\n",
      "Validated batch 79 batch loss 1.28086782\n",
      "Validated batch 80 batch loss 1.2723316\n",
      "Validated batch 81 batch loss 1.39558744\n",
      "Validated batch 82 batch loss 1.30883646\n",
      "Validated batch 83 batch loss 1.36400211\n",
      "Validated batch 84 batch loss 1.35048914\n",
      "Validated batch 85 batch loss 1.35192275\n",
      "Validated batch 86 batch loss 1.33746862\n",
      "Validated batch 87 batch loss 1.17808044\n",
      "Validated batch 88 batch loss 1.27134991\n",
      "Validated batch 89 batch loss 1.35529709\n",
      "Validated batch 90 batch loss 1.30157506\n",
      "Validated batch 91 batch loss 1.22366905\n",
      "Validated batch 92 batch loss 1.24167466\n",
      "Validated batch 93 batch loss 1.23171055\n",
      "Validated batch 94 batch loss 1.10031676\n",
      "Validated batch 95 batch loss 1.25323844\n",
      "Validated batch 96 batch loss 1.20364726\n",
      "Validated batch 97 batch loss 1.22154391\n",
      "Validated batch 98 batch loss 1.26043797\n",
      "Validated batch 99 batch loss 1.27614713\n",
      "Validated batch 100 batch loss 1.25439119\n",
      "Validated batch 101 batch loss 1.28437972\n",
      "Validated batch 102 batch loss 1.19321811\n",
      "Validated batch 103 batch loss 1.34745669\n",
      "Validated batch 104 batch loss 1.28128135\n",
      "Validated batch 105 batch loss 1.1964401\n",
      "Validated batch 106 batch loss 1.22674346\n",
      "Validated batch 107 batch loss 1.29937291\n",
      "Validated batch 108 batch loss 1.19341791\n",
      "Validated batch 109 batch loss 1.37963843\n",
      "Validated batch 110 batch loss 1.27953482\n",
      "Validated batch 111 batch loss 1.20222795\n",
      "Validated batch 112 batch loss 1.30093682\n",
      "Validated batch 113 batch loss 1.22888839\n",
      "Validated batch 114 batch loss 1.19191325\n",
      "Validated batch 115 batch loss 1.23456693\n",
      "Validated batch 116 batch loss 1.2347188\n",
      "Validated batch 117 batch loss 1.35069549\n",
      "Validated batch 118 batch loss 1.16629267\n",
      "Validated batch 119 batch loss 1.22834074\n",
      "Validated batch 120 batch loss 1.27336097\n",
      "Validated batch 121 batch loss 1.29325199\n",
      "Validated batch 122 batch loss 1.31063294\n",
      "Validated batch 123 batch loss 1.33618736\n",
      "Validated batch 124 batch loss 1.34486651\n",
      "Validated batch 125 batch loss 1.22157347\n",
      "Validated batch 126 batch loss 1.33384919\n",
      "Validated batch 127 batch loss 1.29912269\n",
      "Validated batch 128 batch loss 1.23611689\n",
      "Validated batch 129 batch loss 1.37239218\n",
      "Validated batch 130 batch loss 1.31203067\n",
      "Validated batch 131 batch loss 1.3168745\n",
      "Validated batch 132 batch loss 1.36669862\n",
      "Validated batch 133 batch loss 1.17345989\n",
      "Validated batch 134 batch loss 1.20903814\n",
      "Validated batch 135 batch loss 1.23796046\n",
      "Validated batch 136 batch loss 1.15566909\n",
      "Validated batch 137 batch loss 1.37309909\n",
      "Validated batch 138 batch loss 1.21345055\n",
      "Validated batch 139 batch loss 1.23490524\n",
      "Validated batch 140 batch loss 1.29514015\n",
      "Validated batch 141 batch loss 1.23969603\n",
      "Validated batch 142 batch loss 1.11842775\n",
      "Validated batch 143 batch loss 1.17332733\n",
      "Validated batch 144 batch loss 1.30962443\n",
      "Validated batch 145 batch loss 1.1554755\n",
      "Validated batch 146 batch loss 1.15236497\n",
      "Validated batch 147 batch loss 1.21855283\n",
      "Validated batch 148 batch loss 1.29086447\n",
      "Validated batch 149 batch loss 1.1446439\n",
      "Validated batch 150 batch loss 1.26460469\n",
      "Validated batch 151 batch loss 1.13190401\n",
      "Validated batch 152 batch loss 1.25784695\n",
      "Validated batch 153 batch loss 1.30866981\n",
      "Validated batch 154 batch loss 1.37834096\n",
      "Validated batch 155 batch loss 1.18587554\n",
      "Validated batch 156 batch loss 1.3846004\n",
      "Validated batch 157 batch loss 1.0944674\n",
      "Validated batch 158 batch loss 1.13796127\n",
      "Validated batch 159 batch loss 1.21790433\n",
      "Validated batch 160 batch loss 1.23706281\n",
      "Validated batch 161 batch loss 1.34062159\n",
      "Validated batch 162 batch loss 1.32704902\n",
      "Validated batch 163 batch loss 1.28336358\n",
      "Validated batch 164 batch loss 1.16995513\n",
      "Validated batch 165 batch loss 1.20177722\n",
      "Validated batch 166 batch loss 1.28019166\n",
      "Validated batch 167 batch loss 1.24744713\n",
      "Validated batch 168 batch loss 1.28873634\n",
      "Validated batch 169 batch loss 1.2231791\n",
      "Validated batch 170 batch loss 1.14606476\n",
      "Validated batch 171 batch loss 1.37076128\n",
      "Validated batch 172 batch loss 1.23406327\n",
      "Validated batch 173 batch loss 1.10381234\n",
      "Validated batch 174 batch loss 1.21761811\n",
      "Validated batch 175 batch loss 1.34995604\n",
      "Validated batch 176 batch loss 1.33212459\n",
      "Validated batch 177 batch loss 1.42820036\n",
      "Validated batch 178 batch loss 1.21934438\n",
      "Validated batch 179 batch loss 1.41449547\n",
      "Validated batch 180 batch loss 1.23127306\n",
      "Validated batch 181 batch loss 1.32470179\n",
      "Validated batch 182 batch loss 1.32084131\n",
      "Validated batch 183 batch loss 1.06957138\n",
      "Validated batch 184 batch loss 1.15741682\n",
      "Validated batch 185 batch loss 1.37590146\n",
      "Epoch 3 val loss 1.2581100463867188\n",
      "Model /aiffel/aiffel/mpii/mine/model-epoch-3-loss-1.2581.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73d0e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'models', 'model-v0.0.1-epoch-2-loss-1.3072.h5')\n",
    "\n",
    "model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "model.load_weights(best_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05565e7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38/1796686902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# learning curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# learning curve\n",
    "plt.plot(model.history['train_loss'], label='Training Loss')\n",
    "plt.plot(model.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ffed2",
   "metadata": {},
   "source": [
    "# Simple Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74a7f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1507e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet 선언\n",
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')\n",
    "# 3개의 deconv+bn+relu layer\n",
    "upconv = _make_deconv_layer(3)\n",
    "# 마지막 layer\n",
    "final_layer = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a15d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# 모델 함수\n",
    "def SimpleBaseline(input_shape=(256, 256, 3)):\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = resnet(inputs)\n",
    "    x = upconv(x)\n",
    "    out = final_layer(x)\n",
    "    model = keras.Model(inputs, out)\n",
    "    \n",
    "    return tf.keras.Model(inputs, out, name='simple_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "509492d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: in user code:\n",
      "\n",
      "    /tmp/ipykernel_472/3568804479.py:59 train_step  *\n",
      "        self.optimizer.apply_gradients(\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:628 apply_gradients  **\n",
      "        self._create_all_weights(var_list)\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:815 _create_all_weights\n",
      "        self._create_slots(var_list)\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:117 _create_slots\n",
      "        self.add_slot(var, 'm')\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:892 add_slot\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x798c03a58f70>), which is different from the scope used for the original variable (<tf.Variable 'conv1_conv/kernel:0' shape=(7, 7, 3, 64) dtype=float32, numpy=\n",
      "    array([[[[ 2.82526277e-02, -1.18737184e-02,  1.51488732e-03, ...,\n",
      "              -1.07003953e-02, -5.27982824e-02, -1.36667420e-03],\n",
      "             [ 5.86827798e-03,  5.04415408e-02,  3.46324709e-03, ...,\n",
      "               1.01423981e-02,  1.39493728e-02,  1.67549420e-02],\n",
      "             [-2.44090753e-03, -4.86173332e-02,  2.69966386e-03, ...,\n",
      "              -3.44439060e-04,  3.48098315e-02,  6.28910400e-03]],\n",
      "    \n",
      "            [[ 1.81872323e-02, -7.20698107e-03,  4.80302610e-03, ...,\n",
      "              -7.43396254e-03, -8.56800564e-03,  1.16849300e-02],\n",
      "             [ 1.87554304e-02,  5.12730293e-02,  4.50406177e-03, ...,\n",
      "               1.39413681e-02,  1.26296384e-02, -1.73004344e-02],\n",
      "             [ 1.90453827e-02, -3.87909152e-02,  4.25842637e-03, ...,\n",
      "               2.75742816e-04, -1.27962548e-02, -8.35626759e-03]],\n",
      "    \n",
      "            [[ 1.58849321e-02, -1.06073255e-02,  1.30999666e-02, ...,\n",
      "              -2.26797583e-03, -3.98984266e-04,  3.39989027e-04],\n",
      "             [ 3.61421369e-02,  5.02430499e-02,  1.22699486e-02, ...,\n",
      "               1.19910473e-02,  2.02837810e-02, -1.96981970e-02],\n",
      "             [ 2.17959806e-02, -3.86004597e-02,  1.12379901e-02, ...,\n",
      "              -2.07756506e-03, -3.40645364e-03, -3.78638096e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-5.30153252e-02, -8.60502943e-03,  6.38643000e-03, ...,\n",
      "              -4.49256925e-03,  3.48024699e-03, -1.40979560e-02],\n",
      "             [-9.35578942e-02,  4.61557060e-02,  1.53722311e-03, ...,\n",
      "               1.21013075e-02,  5.05337631e-03,  3.30474339e-02],\n",
      "             [-7.69589692e-02, -3.51354294e-02,  2.22769519e-03, ...,\n",
      "               9.18304977e-06, -1.15465783e-02,  2.29630154e-02]],\n",
      "    \n",
      "            [[-4.73558307e-02, -4.07940615e-03,  4.76515992e-03, ...,\n",
      "              -9.73805040e-03, -1.03890402e-02,  1.62366014e-02],\n",
      "             [-1.24100089e-01,  4.78516519e-02, -9.90210217e-04, ...,\n",
      "               1.10340826e-02, -6.77202828e-03,  5.49102016e-02],\n",
      "             [-7.13113099e-02, -2.86470409e-02,  6.20829698e-04, ...,\n",
      "              -2.17762636e-03, -1.58942658e-02,  3.44766974e-02]],\n",
      "    \n",
      "            [[ 1.85429510e-02, -1.12518407e-02,  1.12506151e-02, ...,\n",
      "              -1.51338596e-02, -5.66656142e-03, -1.30050071e-02],\n",
      "             [-2.68079005e-02,  3.64737920e-02,  4.55197273e-03, ...,\n",
      "               5.53486776e-03,  1.12653999e-02,  2.46754289e-03],\n",
      "             [ 1.43940765e-02, -3.56382579e-02,  5.08728763e-03, ...,\n",
      "              -7.46753719e-03,  1.61169283e-02,  1.12382937e-02]]],\n",
      "    \n",
      "    \n",
      "           [[[ 7.99009297e-03, -9.49061289e-03, -4.21846565e-03, ...,\n",
      "              -1.23715792e-02, -3.82804796e-02, -5.90979494e-03],\n",
      "             [-7.68794632e-03,  5.46954982e-02, -1.03303632e-02, ...,\n",
      "               1.40626412e-02,  1.99436247e-02,  2.51518637e-02],\n",
      "             [ 3.70471564e-04, -3.70203964e-02, -9.80611611e-03, ...,\n",
      "              -4.95379185e-03,  2.27415562e-02,  1.38941938e-02]],\n",
      "    \n",
      "            [[ 2.48856675e-02, -9.57963988e-03, -2.37837038e-03, ...,\n",
      "              -1.08526833e-02,  2.24138368e-02, -2.40965877e-02],\n",
      "             [ 2.42966190e-02,  4.93442900e-02, -1.32921906e-02, ...,\n",
      "               1.47738317e-02,  2.67323572e-02,  1.14357602e-02],\n",
      "             [ 2.91274227e-02, -3.05654686e-02, -1.42364930e-02, ...,\n",
      "              -8.36174563e-03, -3.00847553e-02, -2.51545687e-03]],\n",
      "    \n",
      "            [[ 7.67260045e-02, -1.19650066e-02, -2.10191216e-03, ...,\n",
      "               1.79589365e-03,  2.02653632e-02, -1.33340694e-02],\n",
      "             [ 1.49444759e-01,  5.00719361e-02, -1.52172269e-02, ...,\n",
      "               1.83409695e-02,  1.56401172e-02,  8.53796005e-02],\n",
      "             [ 1.17180273e-01, -2.56576538e-02, -1.85890812e-02, ...,\n",
      "              -2.50462536e-03, -5.22738546e-02,  1.17943510e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-1.89151186e-02, -1.06457584e-02, -1.19606184e-03, ...,\n",
      "              -7.13960640e-03,  7.56816342e-02,  8.62411484e-02],\n",
      "             [ 1.33888470e-02,  4.24321182e-02, -1.93305630e-02, ...,\n",
      "               8.93499516e-03,  3.26688178e-02,  1.71118364e-01],\n",
      "             [-9.38678440e-03, -2.88689751e-02, -1.87061988e-02, ...,\n",
      "              -1.06920488e-02, -4.56195511e-02,  1.51734307e-01]],\n",
      "    \n",
      "            [[-7.93561861e-02, -8.69292021e-03,  1.06180850e-02, ...,\n",
      "              -8.22936464e-03,  5.34521677e-02,  2.43676770e-02],\n",
      "             [-1.76872283e-01,  4.03351039e-02, -6.91946782e-03, ...,\n",
      "               1.14902109e-02,  2.45164465e-02,  1.30252065e-02],\n",
      "             [-1.30214587e-01, -2.94868350e-02, -1.32359739e-03, ...,\n",
      "              -8.08166154e-03, -3.32693383e-02,  1.78283844e-02]],\n",
      "    \n",
      "            [[-1.53617216e-02, -1.02823023e-02,  1.44553250e-02, ...,\n",
      "              -1.23689836e-02,  2.81683691e-02, -1.52645903e-02],\n",
      "             [-1.22947149e-01,  3.72432098e-02, -2.82740779e-03, ...,\n",
      "               1.07275983e-02,  1.61965452e-02, -4.08420824e-02],\n",
      "             [-7.92325959e-02, -3.09139602e-02,  1.91061670e-04, ...,\n",
      "              -1.06926244e-02, -1.36199640e-02, -2.90216487e-02]]],\n",
      "    \n",
      "    \n",
      "           [[[-2.74732877e-02, -1.59629062e-02,  5.87167032e-03, ...,\n",
      "              -1.18064405e-02, -5.19699305e-02, -1.52737210e-02],\n",
      "             [-7.46604949e-02,  5.22083789e-02, -1.98963331e-03, ...,\n",
      "               1.27452025e-02,  7.53643783e-03, -1.96208209e-02],\n",
      "             [-3.34048420e-02, -3.39833461e-02, -1.99538236e-03, ...,\n",
      "              -9.30251833e-03,  3.30174603e-02, -1.65446047e-02]],\n",
      "    \n",
      "            [[-6.57535121e-02, -1.23513499e-02, -4.16519074e-03, ...,\n",
      "              -1.22041989e-03,  2.09396798e-02,  3.62350084e-02],\n",
      "             [-1.52494013e-01,  4.94739972e-02, -1.83443855e-02, ...,\n",
      "               2.37025358e-02,  2.67230812e-02,  8.47681686e-02],\n",
      "             [-8.80744159e-02, -2.57136654e-02, -2.17252262e-02, ...,\n",
      "              -3.12197860e-03, -2.06513535e-02,  6.63726628e-02]],\n",
      "    \n",
      "            [[ 1.99921392e-02, -1.76080931e-02,  1.81755237e-03, ...,\n",
      "               3.69562432e-02,  3.51557694e-02,  1.03931516e-01],\n",
      "             [ 6.10242449e-02,  4.46803048e-02, -1.41719123e-02, ...,\n",
      "               5.15808910e-02,  2.07974892e-02,  1.46060020e-01],\n",
      "             [ 8.05315524e-02, -2.88072433e-02, -1.85981095e-02, ...,\n",
      "               2.20173039e-02, -5.11762947e-02,  1.40093669e-01]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[ 1.15528561e-01, -1.67486407e-02,  8.49904679e-03, ...,\n",
      "               4.99674492e-03,  7.98972845e-02, -1.11083500e-01],\n",
      "             [ 3.32334489e-01,  4.24566194e-02, -9.70878359e-03, ...,\n",
      "               1.92873720e-02,  1.25060824e-03, -3.40990961e-01],\n",
      "             [ 2.16480315e-01, -2.68480480e-02, -8.96557700e-03, ...,\n",
      "              -6.44540135e-03, -7.85448179e-02, -2.04899684e-01]],\n",
      "    \n",
      "            [[-8.99803787e-02, -8.51823762e-03,  2.25046948e-02, ...,\n",
      "              -8.74274992e-04,  6.35959804e-02, -9.58404392e-02],\n",
      "             [-8.15074593e-02,  4.37885672e-02,  3.69152403e-03, ...,\n",
      "               1.71142723e-02,  6.33937493e-03, -2.73919165e-01],\n",
      "             [-9.73245725e-02, -2.61962153e-02,  8.95403326e-03, ...,\n",
      "              -7.23934872e-03, -5.64266555e-02, -1.84837982e-01]],\n",
      "    \n",
      "            [[-9.46454927e-02, -1.17739988e-02,  2.49665454e-02, ...,\n",
      "              -7.38179125e-03,  3.05740479e-02, -1.17530329e-02],\n",
      "             [-2.11111471e-01,  3.85808311e-02,  5.31885307e-03, ...,\n",
      "               1.61544569e-02,  3.10361455e-03, -8.36645439e-02],\n",
      "             [-1.75075874e-01, -3.21811885e-02,  9.45197884e-03, ...,\n",
      "              -1.05473688e-02, -2.80730613e-02, -6.67640790e-02]]],\n",
      "    \n",
      "    \n",
      "           ...,\n",
      "    \n",
      "    \n",
      "           [[[ 2.31804699e-02, -1.62718501e-02,  1.22078890e-02, ...,\n",
      "              -1.22131845e-02, -2.02786643e-02, -2.14508991e-03],\n",
      "             [ 2.30488200e-02,  4.41800952e-02,  3.59291583e-03, ...,\n",
      "               1.27932075e-02,  6.47032401e-03, -5.39429188e-02],\n",
      "             [ 2.03978457e-02, -2.67958529e-02,  5.69844292e-03, ...,\n",
      "              -8.20858125e-03,  2.51460597e-02, -3.12512405e-02]],\n",
      "    \n",
      "            [[-4.64516319e-02, -1.34653188e-02,  1.61393601e-02, ...,\n",
      "              -2.20572166e-02,  5.05596139e-02,  1.47165358e-03],\n",
      "             [-1.77852944e-01,  4.04180661e-02,  4.32515051e-03, ...,\n",
      "               7.27979047e-03,  1.37663782e-02, -5.00506982e-02],\n",
      "             [-1.09063022e-01, -2.11244933e-02,  6.98045455e-03, ...,\n",
      "              -2.00869981e-02, -6.30094185e-02, -4.20499854e-02]],\n",
      "    \n",
      "            [[-1.83006614e-01, -1.79655701e-02,  1.82811301e-02, ...,\n",
      "               1.56401389e-03,  9.29453745e-02,  4.12672907e-02],\n",
      "             [-4.11783189e-01,  3.40776965e-02,  8.74394365e-03, ...,\n",
      "               2.33494844e-02,  1.98237225e-02,  8.06325078e-02],\n",
      "             [-2.76736170e-01, -2.83147153e-02,  1.31541817e-02, ...,\n",
      "              -5.05925808e-03, -8.54580775e-02,  4.26753834e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[ 5.36167026e-02, -1.07590063e-02,  2.19804980e-02, ...,\n",
      "              -8.83348845e-03,  1.40453711e-01,  3.20528477e-01],\n",
      "             [ 1.85792699e-01,  3.76442447e-02,  1.02089429e-02, ...,\n",
      "               1.29263047e-02, -3.70457745e-03,  6.66479290e-01],\n",
      "             [ 1.32038444e-01, -2.75047179e-02,  2.28339490e-02, ...,\n",
      "              -1.19996015e-02, -1.22367747e-01,  4.83815670e-01]],\n",
      "    \n",
      "            [[ 8.34956467e-02, -9.09057911e-03,  2.50242520e-02, ...,\n",
      "              -1.67011786e-02,  1.20522320e-01,  1.36462688e-01],\n",
      "             [ 2.50555605e-01,  4.07686047e-02,  1.08884834e-02, ...,\n",
      "               7.53540406e-03, -7.55708572e-03,  3.96415204e-01],\n",
      "             [ 1.49690762e-01, -3.11034787e-02,  2.43526250e-02, ...,\n",
      "              -1.65321939e-02, -1.09688722e-01,  2.64446586e-01]],\n",
      "    \n",
      "            [[ 3.69576029e-02, -1.27014471e-02,  3.19833457e-02, ...,\n",
      "              -1.48784053e-02,  9.22970548e-02,  6.54868260e-02],\n",
      "             [ 9.63706747e-02,  4.39107306e-02,  1.59802549e-02, ...,\n",
      "               1.22494521e-02,  8.10312852e-03,  1.78935930e-01],\n",
      "             [ 2.95156911e-02, -2.96487771e-02,  2.69996542e-02, ...,\n",
      "              -1.38547905e-02, -7.72434175e-02,  1.32773802e-01]]],\n",
      "    \n",
      "    \n",
      "           [[[ 4.22548056e-02, -8.30464344e-03,  5.34065207e-03, ...,\n",
      "              -8.06468353e-03, -4.70053628e-02,  4.45614867e-02],\n",
      "             [ 9.77012664e-02,  3.83502319e-02, -5.37837343e-03, ...,\n",
      "               1.17106764e-02, -4.59602941e-03,  6.98771998e-02],\n",
      "             [ 6.38262108e-02, -2.08319575e-02, -1.72756368e-03, ...,\n",
      "              -8.19445588e-03,  4.25621867e-02,  4.83920909e-02]],\n",
      "    \n",
      "            [[ 4.59470600e-02, -4.77699284e-03,  7.04339007e-03, ...,\n",
      "              -1.82104297e-02,  3.14848162e-02,  4.64068204e-02],\n",
      "             [ 3.89483608e-02,  3.78783308e-02, -6.85291924e-03, ...,\n",
      "               7.33014196e-03,  3.90656322e-04,  1.52848229e-01],\n",
      "             [ 4.57218140e-02, -1.34090437e-02, -8.30697361e-04, ...,\n",
      "              -1.85202472e-02, -3.45353335e-02,  9.25581828e-02]],\n",
      "    \n",
      "            [[-4.66161780e-02, -1.22223441e-02,  9.35023464e-03, ...,\n",
      "              -1.31351836e-02,  6.08736612e-02,  9.18865502e-02],\n",
      "             [-1.92336142e-01,  3.18407975e-02, -1.01881009e-03, ...,\n",
      "               7.55425170e-03, -8.62357323e-04,  2.88297594e-01],\n",
      "             [-1.15666650e-01, -2.35320851e-02,  6.74636895e-03, ...,\n",
      "              -1.94703583e-02, -5.66169359e-02,  1.95824102e-01]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-2.10239179e-02, -9.81471874e-03,  9.81596112e-03, ...,\n",
      "              -1.36731779e-02,  1.20193027e-01, -1.26708716e-01],\n",
      "             [-3.72992679e-02,  3.05935629e-02, -3.00194928e-03, ...,\n",
      "               8.85152724e-03, -5.07611316e-03, -6.25461042e-02],\n",
      "             [ 7.84674310e-04, -2.91344281e-02,  1.12569630e-02, ...,\n",
      "              -1.38232643e-02, -9.49400812e-02, -8.74437019e-02]],\n",
      "    \n",
      "            [[ 3.32221799e-02, -4.22911346e-03,  1.13633750e-02, ...,\n",
      "              -1.41841583e-02,  9.59840789e-02, -1.23203963e-01],\n",
      "             [ 9.95653942e-02,  4.03233357e-02, -4.36036801e-03, ...,\n",
      "               8.42505507e-03, -1.50266392e-02, -1.58158958e-01],\n",
      "             [ 6.55353814e-02, -2.76978761e-02,  1.06595978e-02, ...,\n",
      "              -1.31017175e-02, -9.93799716e-02, -1.52014121e-01]],\n",
      "    \n",
      "            [[ 2.50522885e-02, -1.08845932e-02,  1.29567981e-02, ...,\n",
      "              -1.67823900e-02,  6.55406937e-02, -3.34061496e-02],\n",
      "             [ 1.00219429e-01,  4.24924381e-02, -4.06364352e-03, ...,\n",
      "               8.98410939e-03, -1.98677508e-03, -9.19047296e-02],\n",
      "             [ 6.97101504e-02, -3.41515057e-02,  8.97936709e-03, ...,\n",
      "              -1.51484888e-02, -8.06454644e-02, -8.53376985e-02]]],\n",
      "    \n",
      "    \n",
      "           [[[ 1.46303158e-02, -9.15218703e-03,  5.24803856e-03, ...,\n",
      "              -3.63799883e-03, -5.51798902e-02, -7.19531113e-03],\n",
      "             [ 6.12211153e-02,  2.67034862e-02, -4.38000960e-03, ...,\n",
      "               1.38858845e-02,  1.62421225e-03,  6.91889692e-03],\n",
      "             [ 1.86353922e-02, -2.39325576e-02,  5.56383107e-04, ...,\n",
      "              -6.68733614e-03,  7.36468807e-02,  3.71867418e-02]],\n",
      "    \n",
      "            [[ 3.52302976e-02, -3.27857491e-03,  7.14091491e-03, ...,\n",
      "              -9.93822515e-03,  2.38756705e-02, -2.10771449e-02],\n",
      "             [ 6.34438619e-02,  3.12160589e-02, -7.72275496e-03, ...,\n",
      "               1.49217555e-02,  3.86624038e-03, -1.16395289e-02],\n",
      "             [ 3.35849188e-02, -1.63664240e-02, -1.32562651e-03, ...,\n",
      "              -1.30512416e-02, -7.29435496e-03, -1.24825155e-02]],\n",
      "    \n",
      "            [[ 4.10873676e-03, -4.66612726e-03,  1.21031692e-02, ...,\n",
      "              -7.87103828e-03,  5.80726229e-02, -4.19587009e-02],\n",
      "             [-2.23153979e-02,  2.99241953e-02,  8.01213668e-04, ...,\n",
      "               1.82199273e-02,  9.57238674e-03, -8.57376456e-02],\n",
      "             [-2.01183017e-02, -1.96383689e-02,  7.32050464e-03, ...,\n",
      "              -1.07293837e-02, -2.17854325e-02, -7.95444921e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-1.71692297e-02, -3.16392444e-03,  2.40169745e-03, ...,\n",
      "              -9.67177004e-03,  9.26117748e-02, -1.16062798e-02],\n",
      "             [-8.63026828e-02,  3.55335064e-02, -1.06153013e-02, ...,\n",
      "               1.85809545e-02, -2.19932254e-02, -1.47949710e-01],\n",
      "             [-6.07556999e-02, -2.66596545e-02,  1.74473948e-03, ...,\n",
      "              -4.85855900e-03, -8.82942155e-02, -8.43590796e-02]],\n",
      "    \n",
      "            [[ 1.15142548e-02,  2.20947526e-03,  5.08834422e-03, ...,\n",
      "              -1.04352133e-02,  6.78158402e-02,  4.14623357e-02],\n",
      "             [ 7.41827395e-03,  4.52373996e-02, -1.10873608e-02, ...,\n",
      "               1.56368576e-02, -2.37460397e-02, -3.25448737e-02],\n",
      "             [ 7.84576032e-03, -2.45320965e-02,  5.84031455e-04, ...,\n",
      "              -8.31448287e-03, -8.92601907e-02, -3.36888898e-03]],\n",
      "    \n",
      "            [[ 4.79146978e-03, -4.22942545e-03,  1.15078716e-02, ...,\n",
      "              -2.12721284e-02,  4.96782959e-02,  2.05268860e-02],\n",
      "             [ 2.75192987e-02,  4.36737053e-02, -5.71439136e-03, ...,\n",
      "               9.46100149e-03, -8.58635467e-04, -1.79863740e-02],\n",
      "             [ 2.71184333e-02, -3.31169143e-02,  3.97488568e-03, ...,\n",
      "              -1.41424611e-02, -6.35233149e-02,  1.29984575e-03]]]],\n",
      "          dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 346, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 695, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "ValueError: in user code:\n",
      "\n",
      "    /tmp/ipykernel_472/3568804479.py:59 train_step  *\n",
      "        self.optimizer.apply_gradients(\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:628 apply_gradients  **\n",
      "        self._create_all_weights(var_list)\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:815 _create_all_weights\n",
      "        self._create_slots(var_list)\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:117 _create_slots\n",
      "        self.add_slot(var, 'm')\n",
      "    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:892 add_slot\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x798c03a58f70>), which is different from the scope used for the original variable (<tf.Variable 'conv1_conv/kernel:0' shape=(7, 7, 3, 64) dtype=float32, numpy=\n",
      "    array([[[[ 2.82526277e-02, -1.18737184e-02,  1.51488732e-03, ...,\n",
      "              -1.07003953e-02, -5.27982824e-02, -1.36667420e-03],\n",
      "             [ 5.86827798e-03,  5.04415408e-02,  3.46324709e-03, ...,\n",
      "               1.01423981e-02,  1.39493728e-02,  1.67549420e-02],\n",
      "             [-2.44090753e-03, -4.86173332e-02,  2.69966386e-03, ...,\n",
      "              -3.44439060e-04,  3.48098315e-02,  6.28910400e-03]],\n",
      "    \n",
      "            [[ 1.81872323e-02, -7.20698107e-03,  4.80302610e-03, ...,\n",
      "              -7.43396254e-03, -8.56800564e-03,  1.16849300e-02],\n",
      "             [ 1.87554304e-02,  5.12730293e-02,  4.50406177e-03, ...,\n",
      "               1.39413681e-02,  1.26296384e-02, -1.73004344e-02],\n",
      "             [ 1.90453827e-02, -3.87909152e-02,  4.25842637e-03, ...,\n",
      "               2.75742816e-04, -1.27962548e-02, -8.35626759e-03]],\n",
      "    \n",
      "            [[ 1.58849321e-02, -1.06073255e-02,  1.30999666e-02, ...,\n",
      "              -2.26797583e-03, -3.98984266e-04,  3.39989027e-04],\n",
      "             [ 3.61421369e-02,  5.02430499e-02,  1.22699486e-02, ...,\n",
      "               1.19910473e-02,  2.02837810e-02, -1.96981970e-02],\n",
      "             [ 2.17959806e-02, -3.86004597e-02,  1.12379901e-02, ...,\n",
      "              -2.07756506e-03, -3.40645364e-03, -3.78638096e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-5.30153252e-02, -8.60502943e-03,  6.38643000e-03, ...,\n",
      "              -4.49256925e-03,  3.48024699e-03, -1.40979560e-02],\n",
      "             [-9.35578942e-02,  4.61557060e-02,  1.53722311e-03, ...,\n",
      "               1.21013075e-02,  5.05337631e-03,  3.30474339e-02],\n",
      "             [-7.69589692e-02, -3.51354294e-02,  2.22769519e-03, ...,\n",
      "               9.18304977e-06, -1.15465783e-02,  2.29630154e-02]],\n",
      "    \n",
      "            [[-4.73558307e-02, -4.07940615e-03,  4.76515992e-03, ...,\n",
      "              -9.73805040e-03, -1.03890402e-02,  1.62366014e-02],\n",
      "             [-1.24100089e-01,  4.78516519e-02, -9.90210217e-04, ...,\n",
      "               1.10340826e-02, -6.77202828e-03,  5.49102016e-02],\n",
      "             [-7.13113099e-02, -2.86470409e-02,  6.20829698e-04, ...,\n",
      "              -2.17762636e-03, -1.58942658e-02,  3.44766974e-02]],\n",
      "    \n",
      "            [[ 1.85429510e-02, -1.12518407e-02,  1.12506151e-02, ...,\n",
      "              -1.51338596e-02, -5.66656142e-03, -1.30050071e-02],\n",
      "             [-2.68079005e-02,  3.64737920e-02,  4.55197273e-03, ...,\n",
      "               5.53486776e-03,  1.12653999e-02,  2.46754289e-03],\n",
      "             [ 1.43940765e-02, -3.56382579e-02,  5.08728763e-03, ...,\n",
      "              -7.46753719e-03,  1.61169283e-02,  1.12382937e-02]]],\n",
      "    \n",
      "    \n",
      "           [[[ 7.99009297e-03, -9.49061289e-03, -4.21846565e-03, ...,\n",
      "              -1.23715792e-02, -3.82804796e-02, -5.90979494e-03],\n",
      "             [-7.68794632e-03,  5.46954982e-02, -1.03303632e-02, ...,\n",
      "               1.40626412e-02,  1.99436247e-02,  2.51518637e-02],\n",
      "             [ 3.70471564e-04, -3.70203964e-02, -9.80611611e-03, ...,\n",
      "              -4.95379185e-03,  2.27415562e-02,  1.38941938e-02]],\n",
      "    \n",
      "            [[ 2.48856675e-02, -9.57963988e-03, -2.37837038e-03, ...,\n",
      "              -1.08526833e-02,  2.24138368e-02, -2.40965877e-02],\n",
      "             [ 2.42966190e-02,  4.93442900e-02, -1.32921906e-02, ...,\n",
      "               1.47738317e-02,  2.67323572e-02,  1.14357602e-02],\n",
      "             [ 2.91274227e-02, -3.05654686e-02, -1.42364930e-02, ...,\n",
      "              -8.36174563e-03, -3.00847553e-02, -2.51545687e-03]],\n",
      "    \n",
      "            [[ 7.67260045e-02, -1.19650066e-02, -2.10191216e-03, ...,\n",
      "               1.79589365e-03,  2.02653632e-02, -1.33340694e-02],\n",
      "             [ 1.49444759e-01,  5.00719361e-02, -1.52172269e-02, ...,\n",
      "               1.83409695e-02,  1.56401172e-02,  8.53796005e-02],\n",
      "             [ 1.17180273e-01, -2.56576538e-02, -1.85890812e-02, ...,\n",
      "              -2.50462536e-03, -5.22738546e-02,  1.17943510e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-1.89151186e-02, -1.06457584e-02, -1.19606184e-03, ...,\n",
      "              -7.13960640e-03,  7.56816342e-02,  8.62411484e-02],\n",
      "             [ 1.33888470e-02,  4.24321182e-02, -1.93305630e-02, ...,\n",
      "               8.93499516e-03,  3.26688178e-02,  1.71118364e-01],\n",
      "             [-9.38678440e-03, -2.88689751e-02, -1.87061988e-02, ...,\n",
      "              -1.06920488e-02, -4.56195511e-02,  1.51734307e-01]],\n",
      "    \n",
      "            [[-7.93561861e-02, -8.69292021e-03,  1.06180850e-02, ...,\n",
      "              -8.22936464e-03,  5.34521677e-02,  2.43676770e-02],\n",
      "             [-1.76872283e-01,  4.03351039e-02, -6.91946782e-03, ...,\n",
      "               1.14902109e-02,  2.45164465e-02,  1.30252065e-02],\n",
      "             [-1.30214587e-01, -2.94868350e-02, -1.32359739e-03, ...,\n",
      "              -8.08166154e-03, -3.32693383e-02,  1.78283844e-02]],\n",
      "    \n",
      "            [[-1.53617216e-02, -1.02823023e-02,  1.44553250e-02, ...,\n",
      "              -1.23689836e-02,  2.81683691e-02, -1.52645903e-02],\n",
      "             [-1.22947149e-01,  3.72432098e-02, -2.82740779e-03, ...,\n",
      "               1.07275983e-02,  1.61965452e-02, -4.08420824e-02],\n",
      "             [-7.92325959e-02, -3.09139602e-02,  1.91061670e-04, ...,\n",
      "              -1.06926244e-02, -1.36199640e-02, -2.90216487e-02]]],\n",
      "    \n",
      "    \n",
      "           [[[-2.74732877e-02, -1.59629062e-02,  5.87167032e-03, ...,\n",
      "              -1.18064405e-02, -5.19699305e-02, -1.52737210e-02],\n",
      "             [-7.46604949e-02,  5.22083789e-02, -1.98963331e-03, ...,\n",
      "               1.27452025e-02,  7.53643783e-03, -1.96208209e-02],\n",
      "             [-3.34048420e-02, -3.39833461e-02, -1.99538236e-03, ...,\n",
      "              -9.30251833e-03,  3.30174603e-02, -1.65446047e-02]],\n",
      "    \n",
      "            [[-6.57535121e-02, -1.23513499e-02, -4.16519074e-03, ...,\n",
      "              -1.22041989e-03,  2.09396798e-02,  3.62350084e-02],\n",
      "             [-1.52494013e-01,  4.94739972e-02, -1.83443855e-02, ...,\n",
      "               2.37025358e-02,  2.67230812e-02,  8.47681686e-02],\n",
      "             [-8.80744159e-02, -2.57136654e-02, -2.17252262e-02, ...,\n",
      "              -3.12197860e-03, -2.06513535e-02,  6.63726628e-02]],\n",
      "    \n",
      "            [[ 1.99921392e-02, -1.76080931e-02,  1.81755237e-03, ...,\n",
      "               3.69562432e-02,  3.51557694e-02,  1.03931516e-01],\n",
      "             [ 6.10242449e-02,  4.46803048e-02, -1.41719123e-02, ...,\n",
      "               5.15808910e-02,  2.07974892e-02,  1.46060020e-01],\n",
      "             [ 8.05315524e-02, -2.88072433e-02, -1.85981095e-02, ...,\n",
      "               2.20173039e-02, -5.11762947e-02,  1.40093669e-01]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[ 1.15528561e-01, -1.67486407e-02,  8.49904679e-03, ...,\n",
      "               4.99674492e-03,  7.98972845e-02, -1.11083500e-01],\n",
      "             [ 3.32334489e-01,  4.24566194e-02, -9.70878359e-03, ...,\n",
      "               1.92873720e-02,  1.25060824e-03, -3.40990961e-01],\n",
      "             [ 2.16480315e-01, -2.68480480e-02, -8.96557700e-03, ...,\n",
      "              -6.44540135e-03, -7.85448179e-02, -2.04899684e-01]],\n",
      "    \n",
      "            [[-8.99803787e-02, -8.51823762e-03,  2.25046948e-02, ...,\n",
      "              -8.74274992e-04,  6.35959804e-02, -9.58404392e-02],\n",
      "             [-8.15074593e-02,  4.37885672e-02,  3.69152403e-03, ...,\n",
      "               1.71142723e-02,  6.33937493e-03, -2.73919165e-01],\n",
      "             [-9.73245725e-02, -2.61962153e-02,  8.95403326e-03, ...,\n",
      "              -7.23934872e-03, -5.64266555e-02, -1.84837982e-01]],\n",
      "    \n",
      "            [[-9.46454927e-02, -1.17739988e-02,  2.49665454e-02, ...,\n",
      "              -7.38179125e-03,  3.05740479e-02, -1.17530329e-02],\n",
      "             [-2.11111471e-01,  3.85808311e-02,  5.31885307e-03, ...,\n",
      "               1.61544569e-02,  3.10361455e-03, -8.36645439e-02],\n",
      "             [-1.75075874e-01, -3.21811885e-02,  9.45197884e-03, ...,\n",
      "              -1.05473688e-02, -2.80730613e-02, -6.67640790e-02]]],\n",
      "    \n",
      "    \n",
      "           ...,\n",
      "    \n",
      "    \n",
      "           [[[ 2.31804699e-02, -1.62718501e-02,  1.22078890e-02, ...,\n",
      "              -1.22131845e-02, -2.02786643e-02, -2.14508991e-03],\n",
      "             [ 2.30488200e-02,  4.41800952e-02,  3.59291583e-03, ...,\n",
      "               1.27932075e-02,  6.47032401e-03, -5.39429188e-02],\n",
      "             [ 2.03978457e-02, -2.67958529e-02,  5.69844292e-03, ...,\n",
      "              -8.20858125e-03,  2.51460597e-02, -3.12512405e-02]],\n",
      "    \n",
      "            [[-4.64516319e-02, -1.34653188e-02,  1.61393601e-02, ...,\n",
      "              -2.20572166e-02,  5.05596139e-02,  1.47165358e-03],\n",
      "             [-1.77852944e-01,  4.04180661e-02,  4.32515051e-03, ...,\n",
      "               7.27979047e-03,  1.37663782e-02, -5.00506982e-02],\n",
      "             [-1.09063022e-01, -2.11244933e-02,  6.98045455e-03, ...,\n",
      "              -2.00869981e-02, -6.30094185e-02, -4.20499854e-02]],\n",
      "    \n",
      "            [[-1.83006614e-01, -1.79655701e-02,  1.82811301e-02, ...,\n",
      "               1.56401389e-03,  9.29453745e-02,  4.12672907e-02],\n",
      "             [-4.11783189e-01,  3.40776965e-02,  8.74394365e-03, ...,\n",
      "               2.33494844e-02,  1.98237225e-02,  8.06325078e-02],\n",
      "             [-2.76736170e-01, -2.83147153e-02,  1.31541817e-02, ...,\n",
      "              -5.05925808e-03, -8.54580775e-02,  4.26753834e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[ 5.36167026e-02, -1.07590063e-02,  2.19804980e-02, ...,\n",
      "              -8.83348845e-03,  1.40453711e-01,  3.20528477e-01],\n",
      "             [ 1.85792699e-01,  3.76442447e-02,  1.02089429e-02, ...,\n",
      "               1.29263047e-02, -3.70457745e-03,  6.66479290e-01],\n",
      "             [ 1.32038444e-01, -2.75047179e-02,  2.28339490e-02, ...,\n",
      "              -1.19996015e-02, -1.22367747e-01,  4.83815670e-01]],\n",
      "    \n",
      "            [[ 8.34956467e-02, -9.09057911e-03,  2.50242520e-02, ...,\n",
      "              -1.67011786e-02,  1.20522320e-01,  1.36462688e-01],\n",
      "             [ 2.50555605e-01,  4.07686047e-02,  1.08884834e-02, ...,\n",
      "               7.53540406e-03, -7.55708572e-03,  3.96415204e-01],\n",
      "             [ 1.49690762e-01, -3.11034787e-02,  2.43526250e-02, ...,\n",
      "              -1.65321939e-02, -1.09688722e-01,  2.64446586e-01]],\n",
      "    \n",
      "            [[ 3.69576029e-02, -1.27014471e-02,  3.19833457e-02, ...,\n",
      "              -1.48784053e-02,  9.22970548e-02,  6.54868260e-02],\n",
      "             [ 9.63706747e-02,  4.39107306e-02,  1.59802549e-02, ...,\n",
      "               1.22494521e-02,  8.10312852e-03,  1.78935930e-01],\n",
      "             [ 2.95156911e-02, -2.96487771e-02,  2.69996542e-02, ...,\n",
      "              -1.38547905e-02, -7.72434175e-02,  1.32773802e-01]]],\n",
      "    \n",
      "    \n",
      "           [[[ 4.22548056e-02, -8.30464344e-03,  5.34065207e-03, ...,\n",
      "              -8.06468353e-03, -4.70053628e-02,  4.45614867e-02],\n",
      "             [ 9.77012664e-02,  3.83502319e-02, -5.37837343e-03, ...,\n",
      "               1.17106764e-02, -4.59602941e-03,  6.98771998e-02],\n",
      "             [ 6.38262108e-02, -2.08319575e-02, -1.72756368e-03, ...,\n",
      "              -8.19445588e-03,  4.25621867e-02,  4.83920909e-02]],\n",
      "    \n",
      "            [[ 4.59470600e-02, -4.77699284e-03,  7.04339007e-03, ...,\n",
      "              -1.82104297e-02,  3.14848162e-02,  4.64068204e-02],\n",
      "             [ 3.89483608e-02,  3.78783308e-02, -6.85291924e-03, ...,\n",
      "               7.33014196e-03,  3.90656322e-04,  1.52848229e-01],\n",
      "             [ 4.57218140e-02, -1.34090437e-02, -8.30697361e-04, ...,\n",
      "              -1.85202472e-02, -3.45353335e-02,  9.25581828e-02]],\n",
      "    \n",
      "            [[-4.66161780e-02, -1.22223441e-02,  9.35023464e-03, ...,\n",
      "              -1.31351836e-02,  6.08736612e-02,  9.18865502e-02],\n",
      "             [-1.92336142e-01,  3.18407975e-02, -1.01881009e-03, ...,\n",
      "               7.55425170e-03, -8.62357323e-04,  2.88297594e-01],\n",
      "             [-1.15666650e-01, -2.35320851e-02,  6.74636895e-03, ...,\n",
      "              -1.94703583e-02, -5.66169359e-02,  1.95824102e-01]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-2.10239179e-02, -9.81471874e-03,  9.81596112e-03, ...,\n",
      "              -1.36731779e-02,  1.20193027e-01, -1.26708716e-01],\n",
      "             [-3.72992679e-02,  3.05935629e-02, -3.00194928e-03, ...,\n",
      "               8.85152724e-03, -5.07611316e-03, -6.25461042e-02],\n",
      "             [ 7.84674310e-04, -2.91344281e-02,  1.12569630e-02, ...,\n",
      "              -1.38232643e-02, -9.49400812e-02, -8.74437019e-02]],\n",
      "    \n",
      "            [[ 3.32221799e-02, -4.22911346e-03,  1.13633750e-02, ...,\n",
      "              -1.41841583e-02,  9.59840789e-02, -1.23203963e-01],\n",
      "             [ 9.95653942e-02,  4.03233357e-02, -4.36036801e-03, ...,\n",
      "               8.42505507e-03, -1.50266392e-02, -1.58158958e-01],\n",
      "             [ 6.55353814e-02, -2.76978761e-02,  1.06595978e-02, ...,\n",
      "              -1.31017175e-02, -9.93799716e-02, -1.52014121e-01]],\n",
      "    \n",
      "            [[ 2.50522885e-02, -1.08845932e-02,  1.29567981e-02, ...,\n",
      "              -1.67823900e-02,  6.55406937e-02, -3.34061496e-02],\n",
      "             [ 1.00219429e-01,  4.24924381e-02, -4.06364352e-03, ...,\n",
      "               8.98410939e-03, -1.98677508e-03, -9.19047296e-02],\n",
      "             [ 6.97101504e-02, -3.41515057e-02,  8.97936709e-03, ...,\n",
      "              -1.51484888e-02, -8.06454644e-02, -8.53376985e-02]]],\n",
      "    \n",
      "    \n",
      "           [[[ 1.46303158e-02, -9.15218703e-03,  5.24803856e-03, ...,\n",
      "              -3.63799883e-03, -5.51798902e-02, -7.19531113e-03],\n",
      "             [ 6.12211153e-02,  2.67034862e-02, -4.38000960e-03, ...,\n",
      "               1.38858845e-02,  1.62421225e-03,  6.91889692e-03],\n",
      "             [ 1.86353922e-02, -2.39325576e-02,  5.56383107e-04, ...,\n",
      "              -6.68733614e-03,  7.36468807e-02,  3.71867418e-02]],\n",
      "    \n",
      "            [[ 3.52302976e-02, -3.27857491e-03,  7.14091491e-03, ...,\n",
      "              -9.93822515e-03,  2.38756705e-02, -2.10771449e-02],\n",
      "             [ 6.34438619e-02,  3.12160589e-02, -7.72275496e-03, ...,\n",
      "               1.49217555e-02,  3.86624038e-03, -1.16395289e-02],\n",
      "             [ 3.35849188e-02, -1.63664240e-02, -1.32562651e-03, ...,\n",
      "              -1.30512416e-02, -7.29435496e-03, -1.24825155e-02]],\n",
      "    \n",
      "            [[ 4.10873676e-03, -4.66612726e-03,  1.21031692e-02, ...,\n",
      "              -7.87103828e-03,  5.80726229e-02, -4.19587009e-02],\n",
      "             [-2.23153979e-02,  2.99241953e-02,  8.01213668e-04, ...,\n",
      "               1.82199273e-02,  9.57238674e-03, -8.57376456e-02],\n",
      "             [-2.01183017e-02, -1.96383689e-02,  7.32050464e-03, ...,\n",
      "              -1.07293837e-02, -2.17854325e-02, -7.95444921e-02]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[-1.71692297e-02, -3.16392444e-03,  2.40169745e-03, ...,\n",
      "              -9.67177004e-03,  9.26117748e-02, -1.16062798e-02],\n",
      "             [-8.63026828e-02,  3.55335064e-02, -1.06153013e-02, ...,\n",
      "               1.85809545e-02, -2.19932254e-02, -1.47949710e-01],\n",
      "             [-6.07556999e-02, -2.66596545e-02,  1.74473948e-03, ...,\n",
      "              -4.85855900e-03, -8.82942155e-02, -8.43590796e-02]],\n",
      "    \n",
      "            [[ 1.15142548e-02,  2.20947526e-03,  5.08834422e-03, ...,\n",
      "              -1.04352133e-02,  6.78158402e-02,  4.14623357e-02],\n",
      "             [ 7.41827395e-03,  4.52373996e-02, -1.10873608e-02, ...,\n",
      "               1.56368576e-02, -2.37460397e-02, -3.25448737e-02],\n",
      "             [ 7.84576032e-03, -2.45320965e-02,  5.84031455e-04, ...,\n",
      "              -8.31448287e-03, -8.92601907e-02, -3.36888898e-03]],\n",
      "    \n",
      "            [[ 4.79146978e-03, -4.22942545e-03,  1.15078716e-02, ...,\n",
      "              -2.12721284e-02,  4.96782959e-02,  2.05268860e-02],\n",
      "             [ 2.75192987e-02,  4.36737053e-02, -5.71439136e-03, ...,\n",
      "               9.46100149e-03, -8.58635467e-04, -1.79863740e-02],\n",
      "             [ 2.71184333e-02, -3.31169143e-02,  3.97488568e-03, ...,\n",
      "              -1.41424611e-02, -6.35233149e-02,  1.29984575e-03]]]],\n",
      "          dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_472/3568804479.py:77 distributed_train_epoch  *\n        per_replica_loss = self.strategy.run(\n    /tmp/ipykernel_472/3568804479.py:59 train_step  *\n        self.optimizer.apply_gradients(\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:628 apply_gradients  **\n        self._create_all_weights(var_list)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:815 _create_all_weights\n        self._create_slots(var_list)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:117 _create_slots\n        self.add_slot(var, 'm')\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:892 add_slot\n        raise ValueError(\n\n    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x798c03a58f70>), which is different from the scope used for the original variable (<tf.Variable 'conv1_conv/kernel:0' shape=(7, 7, 3, 64) dtype=float32, numpy=\n    array([[[[ 2.82526277e-02, -1.18737184e-02,  1.51488732e-03, ...,\n              -1.07003953e-02, -5.27982824e-02, -1.36667420e-03],\n             [ 5.86827798e-03,  5.04415408e-02,  3.46324709e-03, ...,\n               1.01423981e-02,  1.39493728e-02,  1.67549420e-02],\n             [-2.44090753e-03, -4.86173332e-02,  2.69966386e-03, ...,\n              -3.44439060e-04,  3.48098315e-02,  6.28910400e-03]],\n    \n            [[ 1.81872323e-02, -7.20698107e-03,  4.80302610e-03, ...,\n              -7.43396254e-03, -8.56800564e-03,  1.16849300e-02],\n             [ 1.87554304e-02,  5.12730293e-02,  4.50406177e-03, ...,\n               1.39413681e-02,  1.26296384e-02, -1.73004344e-02],\n             [ 1.90453827e-02, -3.87909152e-02,  4.25842637e-03, ...,\n               2.75742816e-04, -1.27962548e-02, -8.35626759e-03]],\n    \n            [[ 1.58849321e-02, -1.06073255e-02,  1.30999666e-02, ...,\n              -2.26797583e-03, -3.98984266e-04,  3.39989027e-04],\n             [ 3.61421369e-02,  5.02430499e-02,  1.22699486e-02, ...,\n               1.19910473e-02,  2.02837810e-02, -1.96981970e-02],\n             [ 2.17959806e-02, -3.86004597e-02,  1.12379901e-02, ...,\n              -2.07756506e-03, -3.40645364e-03, -3.78638096e-02]],\n    \n            ...,\n    \n            [[-5.30153252e-02, -8.60502943e-03,  6.38643000e-03, ...,\n              -4.49256925e-03,  3.48024699e-03, -1.40979560e-02],\n             [-9.35578942e-02,  4.61557060e-02,  1.53722311e-03, ...,\n               1.21013075e-02,  5.05337631e-03,  3.30474339e-02],\n             [-7.69589692e-02, -3.51354294e-02,  2.22769519e-03, ...,\n               9.18304977e-06, -1.15465783e-02,  2.29630154e-02]],\n    \n            [[-4.73558307e-02, -4.07940615e-03,  4.76515992e-03, ...,\n              -9.73805040e-03, -1.03890402e-02,  1.62366014e-02],\n             [-1.24100089e-01,  4.78516519e-02, -9.90210217e-04, ...,\n               1.10340826e-02, -6.77202828e-03,  5.49102016e-02],\n             [-7.13113099e-02, -2.86470409e-02,  6.20829698e-04, ...,\n              -2.17762636e-03, -1.58942658e-02,  3.44766974e-02]],\n    \n            [[ 1.85429510e-02, -1.12518407e-02,  1.12506151e-02, ...,\n              -1.51338596e-02, -5.66656142e-03, -1.30050071e-02],\n             [-2.68079005e-02,  3.64737920e-02,  4.55197273e-03, ...,\n               5.53486776e-03,  1.12653999e-02,  2.46754289e-03],\n             [ 1.43940765e-02, -3.56382579e-02,  5.08728763e-03, ...,\n              -7.46753719e-03,  1.61169283e-02,  1.12382937e-02]]],\n    \n    \n           [[[ 7.99009297e-03, -9.49061289e-03, -4.21846565e-03, ...,\n              -1.23715792e-02, -3.82804796e-02, -5.90979494e-03],\n             [-7.68794632e-03,  5.46954982e-02, -1.03303632e-02, ...,\n               1.40626412e-02,  1.99436247e-02,  2.51518637e-02],\n             [ 3.70471564e-04, -3.70203964e-02, -9.80611611e-03, ...,\n              -4.95379185e-03,  2.27415562e-02,  1.38941938e-02]],\n    \n            [[ 2.48856675e-02, -9.57963988e-03, -2.37837038e-03, ...,\n              -1.08526833e-02,  2.24138368e-02, -2.40965877e-02],\n             [ 2.42966190e-02,  4.93442900e-02, -1.32921906e-02, ...,\n               1.47738317e-02,  2.67323572e-02,  1.14357602e-02],\n             [ 2.91274227e-02, -3.05654686e-02, -1.42364930e-02, ...,\n              -8.36174563e-03, -3.00847553e-02, -2.51545687e-03]],\n    \n            [[ 7.67260045e-02, -1.19650066e-02, -2.10191216e-03, ...,\n               1.79589365e-03,  2.02653632e-02, -1.33340694e-02],\n             [ 1.49444759e-01,  5.00719361e-02, -1.52172269e-02, ...,\n               1.83409695e-02,  1.56401172e-02,  8.53796005e-02],\n             [ 1.17180273e-01, -2.56576538e-02, -1.85890812e-02, ...,\n              -2.50462536e-03, -5.22738546e-02,  1.17943510e-02]],\n    \n            ...,\n    \n            [[-1.89151186e-02, -1.06457584e-02, -1.19606184e-03, ...,\n              -7.13960640e-03,  7.56816342e-02,  8.62411484e-02],\n             [ 1.33888470e-02,  4.24321182e-02, -1.93305630e-02, ...,\n               8.93499516e-03,  3.26688178e-02,  1.71118364e-01],\n             [-9.38678440e-03, -2.88689751e-02, -1.87061988e-02, ...,\n              -1.06920488e-02, -4.56195511e-02,  1.51734307e-01]],\n    \n            [[-7.93561861e-02, -8.69292021e-03,  1.06180850e-02, ...,\n              -8.22936464e-03,  5.34521677e-02,  2.43676770e-02],\n             [-1.76872283e-01,  4.03351039e-02, -6.91946782e-03, ...,\n               1.14902109e-02,  2.45164465e-02,  1.30252065e-02],\n             [-1.30214587e-01, -2.94868350e-02, -1.32359739e-03, ...,\n              -8.08166154e-03, -3.32693383e-02,  1.78283844e-02]],\n    \n            [[-1.53617216e-02, -1.02823023e-02,  1.44553250e-02, ...,\n              -1.23689836e-02,  2.81683691e-02, -1.52645903e-02],\n             [-1.22947149e-01,  3.72432098e-02, -2.82740779e-03, ...,\n               1.07275983e-02,  1.61965452e-02, -4.08420824e-02],\n             [-7.92325959e-02, -3.09139602e-02,  1.91061670e-04, ...,\n              -1.06926244e-02, -1.36199640e-02, -2.90216487e-02]]],\n    \n    \n           [[[-2.74732877e-02, -1.59629062e-02,  5.87167032e-03, ...,\n              -1.18064405e-02, -5.19699305e-02, -1.52737210e-02],\n             [-7.46604949e-02,  5.22083789e-02, -1.98963331e-03, ...,\n               1.27452025e-02,  7.53643783e-03, -1.96208209e-02],\n             [-3.34048420e-02, -3.39833461e-02, -1.99538236e-03, ...,\n              -9.30251833e-03,  3.30174603e-02, -1.65446047e-02]],\n    \n            [[-6.57535121e-02, -1.23513499e-02, -4.16519074e-03, ...,\n              -1.22041989e-03,  2.09396798e-02,  3.62350084e-02],\n             [-1.52494013e-01,  4.94739972e-02, -1.83443855e-02, ...,\n               2.37025358e-02,  2.67230812e-02,  8.47681686e-02],\n             [-8.80744159e-02, -2.57136654e-02, -2.17252262e-02, ...,\n              -3.12197860e-03, -2.06513535e-02,  6.63726628e-02]],\n    \n            [[ 1.99921392e-02, -1.76080931e-02,  1.81755237e-03, ...,\n               3.69562432e-02,  3.51557694e-02,  1.03931516e-01],\n             [ 6.10242449e-02,  4.46803048e-02, -1.41719123e-02, ...,\n               5.15808910e-02,  2.07974892e-02,  1.46060020e-01],\n             [ 8.05315524e-02, -2.88072433e-02, -1.85981095e-02, ...,\n               2.20173039e-02, -5.11762947e-02,  1.40093669e-01]],\n    \n            ...,\n    \n            [[ 1.15528561e-01, -1.67486407e-02,  8.49904679e-03, ...,\n               4.99674492e-03,  7.98972845e-02, -1.11083500e-01],\n             [ 3.32334489e-01,  4.24566194e-02, -9.70878359e-03, ...,\n               1.92873720e-02,  1.25060824e-03, -3.40990961e-01],\n             [ 2.16480315e-01, -2.68480480e-02, -8.96557700e-03, ...,\n              -6.44540135e-03, -7.85448179e-02, -2.04899684e-01]],\n    \n            [[-8.99803787e-02, -8.51823762e-03,  2.25046948e-02, ...,\n              -8.74274992e-04,  6.35959804e-02, -9.58404392e-02],\n             [-8.15074593e-02,  4.37885672e-02,  3.69152403e-03, ...,\n               1.71142723e-02,  6.33937493e-03, -2.73919165e-01],\n             [-9.73245725e-02, -2.61962153e-02,  8.95403326e-03, ...,\n              -7.23934872e-03, -5.64266555e-02, -1.84837982e-01]],\n    \n            [[-9.46454927e-02, -1.17739988e-02,  2.49665454e-02, ...,\n              -7.38179125e-03,  3.05740479e-02, -1.17530329e-02],\n             [-2.11111471e-01,  3.85808311e-02,  5.31885307e-03, ...,\n               1.61544569e-02,  3.10361455e-03, -8.36645439e-02],\n             [-1.75075874e-01, -3.21811885e-02,  9.45197884e-03, ...,\n              -1.05473688e-02, -2.80730613e-02, -6.67640790e-02]]],\n    \n    \n           ...,\n    \n    \n           [[[ 2.31804699e-02, -1.62718501e-02,  1.22078890e-02, ...,\n              -1.22131845e-02, -2.02786643e-02, -2.14508991e-03],\n             [ 2.30488200e-02,  4.41800952e-02,  3.59291583e-03, ...,\n               1.27932075e-02,  6.47032401e-03, -5.39429188e-02],\n             [ 2.03978457e-02, -2.67958529e-02,  5.69844292e-03, ...,\n              -8.20858125e-03,  2.51460597e-02, -3.12512405e-02]],\n    \n            [[-4.64516319e-02, -1.34653188e-02,  1.61393601e-02, ...,\n              -2.20572166e-02,  5.05596139e-02,  1.47165358e-03],\n             [-1.77852944e-01,  4.04180661e-02,  4.32515051e-03, ...,\n               7.27979047e-03,  1.37663782e-02, -5.00506982e-02],\n             [-1.09063022e-01, -2.11244933e-02,  6.98045455e-03, ...,\n              -2.00869981e-02, -6.30094185e-02, -4.20499854e-02]],\n    \n            [[-1.83006614e-01, -1.79655701e-02,  1.82811301e-02, ...,\n               1.56401389e-03,  9.29453745e-02,  4.12672907e-02],\n             [-4.11783189e-01,  3.40776965e-02,  8.74394365e-03, ...,\n               2.33494844e-02,  1.98237225e-02,  8.06325078e-02],\n             [-2.76736170e-01, -2.83147153e-02,  1.31541817e-02, ...,\n              -5.05925808e-03, -8.54580775e-02,  4.26753834e-02]],\n    \n            ...,\n    \n            [[ 5.36167026e-02, -1.07590063e-02,  2.19804980e-02, ...,\n              -8.83348845e-03,  1.40453711e-01,  3.20528477e-01],\n             [ 1.85792699e-01,  3.76442447e-02,  1.02089429e-02, ...,\n               1.29263047e-02, -3.70457745e-03,  6.66479290e-01],\n             [ 1.32038444e-01, -2.75047179e-02,  2.28339490e-02, ...,\n              -1.19996015e-02, -1.22367747e-01,  4.83815670e-01]],\n    \n            [[ 8.34956467e-02, -9.09057911e-03,  2.50242520e-02, ...,\n              -1.67011786e-02,  1.20522320e-01,  1.36462688e-01],\n             [ 2.50555605e-01,  4.07686047e-02,  1.08884834e-02, ...,\n               7.53540406e-03, -7.55708572e-03,  3.96415204e-01],\n             [ 1.49690762e-01, -3.11034787e-02,  2.43526250e-02, ...,\n              -1.65321939e-02, -1.09688722e-01,  2.64446586e-01]],\n    \n            [[ 3.69576029e-02, -1.27014471e-02,  3.19833457e-02, ...,\n              -1.48784053e-02,  9.22970548e-02,  6.54868260e-02],\n             [ 9.63706747e-02,  4.39107306e-02,  1.59802549e-02, ...,\n               1.22494521e-02,  8.10312852e-03,  1.78935930e-01],\n             [ 2.95156911e-02, -2.96487771e-02,  2.69996542e-02, ...,\n              -1.38547905e-02, -7.72434175e-02,  1.32773802e-01]]],\n    \n    \n           [[[ 4.22548056e-02, -8.30464344e-03,  5.34065207e-03, ...,\n              -8.06468353e-03, -4.70053628e-02,  4.45614867e-02],\n             [ 9.77012664e-02,  3.83502319e-02, -5.37837343e-03, ...,\n               1.17106764e-02, -4.59602941e-03,  6.98771998e-02],\n             [ 6.38262108e-02, -2.08319575e-02, -1.72756368e-03, ...,\n              -8.19445588e-03,  4.25621867e-02,  4.83920909e-02]],\n    \n            [[ 4.59470600e-02, -4.77699284e-03,  7.04339007e-03, ...,\n              -1.82104297e-02,  3.14848162e-02,  4.64068204e-02],\n             [ 3.89483608e-02,  3.78783308e-02, -6.85291924e-03, ...,\n               7.33014196e-03,  3.90656322e-04,  1.52848229e-01],\n             [ 4.57218140e-02, -1.34090437e-02, -8.30697361e-04, ...,\n              -1.85202472e-02, -3.45353335e-02,  9.25581828e-02]],\n    \n            [[-4.66161780e-02, -1.22223441e-02,  9.35023464e-03, ...,\n              -1.31351836e-02,  6.08736612e-02,  9.18865502e-02],\n             [-1.92336142e-01,  3.18407975e-02, -1.01881009e-03, ...,\n               7.55425170e-03, -8.62357323e-04,  2.88297594e-01],\n             [-1.15666650e-01, -2.35320851e-02,  6.74636895e-03, ...,\n              -1.94703583e-02, -5.66169359e-02,  1.95824102e-01]],\n    \n            ...,\n    \n            [[-2.10239179e-02, -9.81471874e-03,  9.81596112e-03, ...,\n              -1.36731779e-02,  1.20193027e-01, -1.26708716e-01],\n             [-3.72992679e-02,  3.05935629e-02, -3.00194928e-03, ...,\n               8.85152724e-03, -5.07611316e-03, -6.25461042e-02],\n             [ 7.84674310e-04, -2.91344281e-02,  1.12569630e-02, ...,\n              -1.38232643e-02, -9.49400812e-02, -8.74437019e-02]],\n    \n            [[ 3.32221799e-02, -4.22911346e-03,  1.13633750e-02, ...,\n              -1.41841583e-02,  9.59840789e-02, -1.23203963e-01],\n             [ 9.95653942e-02,  4.03233357e-02, -4.36036801e-03, ...,\n               8.42505507e-03, -1.50266392e-02, -1.58158958e-01],\n             [ 6.55353814e-02, -2.76978761e-02,  1.06595978e-02, ...,\n              -1.31017175e-02, -9.93799716e-02, -1.52014121e-01]],\n    \n            [[ 2.50522885e-02, -1.08845932e-02,  1.29567981e-02, ...,\n              -1.67823900e-02,  6.55406937e-02, -3.34061496e-02],\n             [ 1.00219429e-01,  4.24924381e-02, -4.06364352e-03, ...,\n               8.98410939e-03, -1.98677508e-03, -9.19047296e-02],\n             [ 6.97101504e-02, -3.41515057e-02,  8.97936709e-03, ...,\n              -1.51484888e-02, -8.06454644e-02, -8.53376985e-02]]],\n    \n    \n           [[[ 1.46303158e-02, -9.15218703e-03,  5.24803856e-03, ...,\n              -3.63799883e-03, -5.51798902e-02, -7.19531113e-03],\n             [ 6.12211153e-02,  2.67034862e-02, -4.38000960e-03, ...,\n               1.38858845e-02,  1.62421225e-03,  6.91889692e-03],\n             [ 1.86353922e-02, -2.39325576e-02,  5.56383107e-04, ...,\n              -6.68733614e-03,  7.36468807e-02,  3.71867418e-02]],\n    \n            [[ 3.52302976e-02, -3.27857491e-03,  7.14091491e-03, ...,\n              -9.93822515e-03,  2.38756705e-02, -2.10771449e-02],\n             [ 6.34438619e-02,  3.12160589e-02, -7.72275496e-03, ...,\n               1.49217555e-02,  3.86624038e-03, -1.16395289e-02],\n             [ 3.35849188e-02, -1.63664240e-02, -1.32562651e-03, ...,\n              -1.30512416e-02, -7.29435496e-03, -1.24825155e-02]],\n    \n            [[ 4.10873676e-03, -4.66612726e-03,  1.21031692e-02, ...,\n              -7.87103828e-03,  5.80726229e-02, -4.19587009e-02],\n             [-2.23153979e-02,  2.99241953e-02,  8.01213668e-04, ...,\n               1.82199273e-02,  9.57238674e-03, -8.57376456e-02],\n             [-2.01183017e-02, -1.96383689e-02,  7.32050464e-03, ...,\n              -1.07293837e-02, -2.17854325e-02, -7.95444921e-02]],\n    \n            ...,\n    \n            [[-1.71692297e-02, -3.16392444e-03,  2.40169745e-03, ...,\n              -9.67177004e-03,  9.26117748e-02, -1.16062798e-02],\n             [-8.63026828e-02,  3.55335064e-02, -1.06153013e-02, ...,\n               1.85809545e-02, -2.19932254e-02, -1.47949710e-01],\n             [-6.07556999e-02, -2.66596545e-02,  1.74473948e-03, ...,\n              -4.85855900e-03, -8.82942155e-02, -8.43590796e-02]],\n    \n            [[ 1.15142548e-02,  2.20947526e-03,  5.08834422e-03, ...,\n              -1.04352133e-02,  6.78158402e-02,  4.14623357e-02],\n             [ 7.41827395e-03,  4.52373996e-02, -1.10873608e-02, ...,\n               1.56368576e-02, -2.37460397e-02, -3.25448737e-02],\n             [ 7.84576032e-03, -2.45320965e-02,  5.84031455e-04, ...,\n              -8.31448287e-03, -8.92601907e-02, -3.36888898e-03]],\n    \n            [[ 4.79146978e-03, -4.22942545e-03,  1.15078716e-02, ...,\n              -2.12721284e-02,  4.96782959e-02,  2.05268860e-02],\n             [ 2.75192987e-02,  4.36737053e-02, -5.71439136e-03, ...,\n               9.46100149e-03, -8.58635467e-04, -1.79863740e-02],\n             [ 2.71184333e-02, -3.31169143e-02,  3.97488568e-03, ...,\n              -1.41424611e-02, -6.35233149e-02,  1.29984575e-03]]]],\n          dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_472/808791913.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0007\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mbest_model_file_simple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tfrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_tfrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_472/1662617086.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dist_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dist_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_472/3568804479.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, train_dist_dataset, val_dist_dataset)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 epoch, self.current_learning_rate))\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             train_total_loss, num_train_batches = distributed_train_epoch(\n\u001b[0m\u001b[1;32m    113\u001b[0m                 train_dist_dataset)\n\u001b[1;32m    114\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_train_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /tmp/ipykernel_472/3568804479.py:77 distributed_train_epoch  *\n        per_replica_loss = self.strategy.run(\n    /tmp/ipykernel_472/3568804479.py:59 train_step  *\n        self.optimizer.apply_gradients(\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:628 apply_gradients  **\n        self._create_all_weights(var_list)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:815 _create_all_weights\n        self._create_slots(var_list)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:117 _create_slots\n        self.add_slot(var, 'm')\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:892 add_slot\n        raise ValueError(\n\n    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x798c03a58f70>), which is different from the scope used for the original variable (<tf.Variable 'conv1_conv/kernel:0' shape=(7, 7, 3, 64) dtype=float32, numpy=\n    array([[[[ 2.82526277e-02, -1.18737184e-02,  1.51488732e-03, ...,\n              -1.07003953e-02, -5.27982824e-02, -1.36667420e-03],\n             [ 5.86827798e-03,  5.04415408e-02,  3.46324709e-03, ...,\n               1.01423981e-02,  1.39493728e-02,  1.67549420e-02],\n             [-2.44090753e-03, -4.86173332e-02,  2.69966386e-03, ...,\n              -3.44439060e-04,  3.48098315e-02,  6.28910400e-03]],\n    \n            [[ 1.81872323e-02, -7.20698107e-03,  4.80302610e-03, ...,\n              -7.43396254e-03, -8.56800564e-03,  1.16849300e-02],\n             [ 1.87554304e-02,  5.12730293e-02,  4.50406177e-03, ...,\n               1.39413681e-02,  1.26296384e-02, -1.73004344e-02],\n             [ 1.90453827e-02, -3.87909152e-02,  4.25842637e-03, ...,\n               2.75742816e-04, -1.27962548e-02, -8.35626759e-03]],\n    \n            [[ 1.58849321e-02, -1.06073255e-02,  1.30999666e-02, ...,\n              -2.26797583e-03, -3.98984266e-04,  3.39989027e-04],\n             [ 3.61421369e-02,  5.02430499e-02,  1.22699486e-02, ...,\n               1.19910473e-02,  2.02837810e-02, -1.96981970e-02],\n             [ 2.17959806e-02, -3.86004597e-02,  1.12379901e-02, ...,\n              -2.07756506e-03, -3.40645364e-03, -3.78638096e-02]],\n    \n            ...,\n    \n            [[-5.30153252e-02, -8.60502943e-03,  6.38643000e-03, ...,\n              -4.49256925e-03,  3.48024699e-03, -1.40979560e-02],\n             [-9.35578942e-02,  4.61557060e-02,  1.53722311e-03, ...,\n               1.21013075e-02,  5.05337631e-03,  3.30474339e-02],\n             [-7.69589692e-02, -3.51354294e-02,  2.22769519e-03, ...,\n               9.18304977e-06, -1.15465783e-02,  2.29630154e-02]],\n    \n            [[-4.73558307e-02, -4.07940615e-03,  4.76515992e-03, ...,\n              -9.73805040e-03, -1.03890402e-02,  1.62366014e-02],\n             [-1.24100089e-01,  4.78516519e-02, -9.90210217e-04, ...,\n               1.10340826e-02, -6.77202828e-03,  5.49102016e-02],\n             [-7.13113099e-02, -2.86470409e-02,  6.20829698e-04, ...,\n              -2.17762636e-03, -1.58942658e-02,  3.44766974e-02]],\n    \n            [[ 1.85429510e-02, -1.12518407e-02,  1.12506151e-02, ...,\n              -1.51338596e-02, -5.66656142e-03, -1.30050071e-02],\n             [-2.68079005e-02,  3.64737920e-02,  4.55197273e-03, ...,\n               5.53486776e-03,  1.12653999e-02,  2.46754289e-03],\n             [ 1.43940765e-02, -3.56382579e-02,  5.08728763e-03, ...,\n              -7.46753719e-03,  1.61169283e-02,  1.12382937e-02]]],\n    \n    \n           [[[ 7.99009297e-03, -9.49061289e-03, -4.21846565e-03, ...,\n              -1.23715792e-02, -3.82804796e-02, -5.90979494e-03],\n             [-7.68794632e-03,  5.46954982e-02, -1.03303632e-02, ...,\n               1.40626412e-02,  1.99436247e-02,  2.51518637e-02],\n             [ 3.70471564e-04, -3.70203964e-02, -9.80611611e-03, ...,\n              -4.95379185e-03,  2.27415562e-02,  1.38941938e-02]],\n    \n            [[ 2.48856675e-02, -9.57963988e-03, -2.37837038e-03, ...,\n              -1.08526833e-02,  2.24138368e-02, -2.40965877e-02],\n             [ 2.42966190e-02,  4.93442900e-02, -1.32921906e-02, ...,\n               1.47738317e-02,  2.67323572e-02,  1.14357602e-02],\n             [ 2.91274227e-02, -3.05654686e-02, -1.42364930e-02, ...,\n              -8.36174563e-03, -3.00847553e-02, -2.51545687e-03]],\n    \n            [[ 7.67260045e-02, -1.19650066e-02, -2.10191216e-03, ...,\n               1.79589365e-03,  2.02653632e-02, -1.33340694e-02],\n             [ 1.49444759e-01,  5.00719361e-02, -1.52172269e-02, ...,\n               1.83409695e-02,  1.56401172e-02,  8.53796005e-02],\n             [ 1.17180273e-01, -2.56576538e-02, -1.85890812e-02, ...,\n              -2.50462536e-03, -5.22738546e-02,  1.17943510e-02]],\n    \n            ...,\n    \n            [[-1.89151186e-02, -1.06457584e-02, -1.19606184e-03, ...,\n              -7.13960640e-03,  7.56816342e-02,  8.62411484e-02],\n             [ 1.33888470e-02,  4.24321182e-02, -1.93305630e-02, ...,\n               8.93499516e-03,  3.26688178e-02,  1.71118364e-01],\n             [-9.38678440e-03, -2.88689751e-02, -1.87061988e-02, ...,\n              -1.06920488e-02, -4.56195511e-02,  1.51734307e-01]],\n    \n            [[-7.93561861e-02, -8.69292021e-03,  1.06180850e-02, ...,\n              -8.22936464e-03,  5.34521677e-02,  2.43676770e-02],\n             [-1.76872283e-01,  4.03351039e-02, -6.91946782e-03, ...,\n               1.14902109e-02,  2.45164465e-02,  1.30252065e-02],\n             [-1.30214587e-01, -2.94868350e-02, -1.32359739e-03, ...,\n              -8.08166154e-03, -3.32693383e-02,  1.78283844e-02]],\n    \n            [[-1.53617216e-02, -1.02823023e-02,  1.44553250e-02, ...,\n              -1.23689836e-02,  2.81683691e-02, -1.52645903e-02],\n             [-1.22947149e-01,  3.72432098e-02, -2.82740779e-03, ...,\n               1.07275983e-02,  1.61965452e-02, -4.08420824e-02],\n             [-7.92325959e-02, -3.09139602e-02,  1.91061670e-04, ...,\n              -1.06926244e-02, -1.36199640e-02, -2.90216487e-02]]],\n    \n    \n           [[[-2.74732877e-02, -1.59629062e-02,  5.87167032e-03, ...,\n              -1.18064405e-02, -5.19699305e-02, -1.52737210e-02],\n             [-7.46604949e-02,  5.22083789e-02, -1.98963331e-03, ...,\n               1.27452025e-02,  7.53643783e-03, -1.96208209e-02],\n             [-3.34048420e-02, -3.39833461e-02, -1.99538236e-03, ...,\n              -9.30251833e-03,  3.30174603e-02, -1.65446047e-02]],\n    \n            [[-6.57535121e-02, -1.23513499e-02, -4.16519074e-03, ...,\n              -1.22041989e-03,  2.09396798e-02,  3.62350084e-02],\n             [-1.52494013e-01,  4.94739972e-02, -1.83443855e-02, ...,\n               2.37025358e-02,  2.67230812e-02,  8.47681686e-02],\n             [-8.80744159e-02, -2.57136654e-02, -2.17252262e-02, ...,\n              -3.12197860e-03, -2.06513535e-02,  6.63726628e-02]],\n    \n            [[ 1.99921392e-02, -1.76080931e-02,  1.81755237e-03, ...,\n               3.69562432e-02,  3.51557694e-02,  1.03931516e-01],\n             [ 6.10242449e-02,  4.46803048e-02, -1.41719123e-02, ...,\n               5.15808910e-02,  2.07974892e-02,  1.46060020e-01],\n             [ 8.05315524e-02, -2.88072433e-02, -1.85981095e-02, ...,\n               2.20173039e-02, -5.11762947e-02,  1.40093669e-01]],\n    \n            ...,\n    \n            [[ 1.15528561e-01, -1.67486407e-02,  8.49904679e-03, ...,\n               4.99674492e-03,  7.98972845e-02, -1.11083500e-01],\n             [ 3.32334489e-01,  4.24566194e-02, -9.70878359e-03, ...,\n               1.92873720e-02,  1.25060824e-03, -3.40990961e-01],\n             [ 2.16480315e-01, -2.68480480e-02, -8.96557700e-03, ...,\n              -6.44540135e-03, -7.85448179e-02, -2.04899684e-01]],\n    \n            [[-8.99803787e-02, -8.51823762e-03,  2.25046948e-02, ...,\n              -8.74274992e-04,  6.35959804e-02, -9.58404392e-02],\n             [-8.15074593e-02,  4.37885672e-02,  3.69152403e-03, ...,\n               1.71142723e-02,  6.33937493e-03, -2.73919165e-01],\n             [-9.73245725e-02, -2.61962153e-02,  8.95403326e-03, ...,\n              -7.23934872e-03, -5.64266555e-02, -1.84837982e-01]],\n    \n            [[-9.46454927e-02, -1.17739988e-02,  2.49665454e-02, ...,\n              -7.38179125e-03,  3.05740479e-02, -1.17530329e-02],\n             [-2.11111471e-01,  3.85808311e-02,  5.31885307e-03, ...,\n               1.61544569e-02,  3.10361455e-03, -8.36645439e-02],\n             [-1.75075874e-01, -3.21811885e-02,  9.45197884e-03, ...,\n              -1.05473688e-02, -2.80730613e-02, -6.67640790e-02]]],\n    \n    \n           ...,\n    \n    \n           [[[ 2.31804699e-02, -1.62718501e-02,  1.22078890e-02, ...,\n              -1.22131845e-02, -2.02786643e-02, -2.14508991e-03],\n             [ 2.30488200e-02,  4.41800952e-02,  3.59291583e-03, ...,\n               1.27932075e-02,  6.47032401e-03, -5.39429188e-02],\n             [ 2.03978457e-02, -2.67958529e-02,  5.69844292e-03, ...,\n              -8.20858125e-03,  2.51460597e-02, -3.12512405e-02]],\n    \n            [[-4.64516319e-02, -1.34653188e-02,  1.61393601e-02, ...,\n              -2.20572166e-02,  5.05596139e-02,  1.47165358e-03],\n             [-1.77852944e-01,  4.04180661e-02,  4.32515051e-03, ...,\n               7.27979047e-03,  1.37663782e-02, -5.00506982e-02],\n             [-1.09063022e-01, -2.11244933e-02,  6.98045455e-03, ...,\n              -2.00869981e-02, -6.30094185e-02, -4.20499854e-02]],\n    \n            [[-1.83006614e-01, -1.79655701e-02,  1.82811301e-02, ...,\n               1.56401389e-03,  9.29453745e-02,  4.12672907e-02],\n             [-4.11783189e-01,  3.40776965e-02,  8.74394365e-03, ...,\n               2.33494844e-02,  1.98237225e-02,  8.06325078e-02],\n             [-2.76736170e-01, -2.83147153e-02,  1.31541817e-02, ...,\n              -5.05925808e-03, -8.54580775e-02,  4.26753834e-02]],\n    \n            ...,\n    \n            [[ 5.36167026e-02, -1.07590063e-02,  2.19804980e-02, ...,\n              -8.83348845e-03,  1.40453711e-01,  3.20528477e-01],\n             [ 1.85792699e-01,  3.76442447e-02,  1.02089429e-02, ...,\n               1.29263047e-02, -3.70457745e-03,  6.66479290e-01],\n             [ 1.32038444e-01, -2.75047179e-02,  2.28339490e-02, ...,\n              -1.19996015e-02, -1.22367747e-01,  4.83815670e-01]],\n    \n            [[ 8.34956467e-02, -9.09057911e-03,  2.50242520e-02, ...,\n              -1.67011786e-02,  1.20522320e-01,  1.36462688e-01],\n             [ 2.50555605e-01,  4.07686047e-02,  1.08884834e-02, ...,\n               7.53540406e-03, -7.55708572e-03,  3.96415204e-01],\n             [ 1.49690762e-01, -3.11034787e-02,  2.43526250e-02, ...,\n              -1.65321939e-02, -1.09688722e-01,  2.64446586e-01]],\n    \n            [[ 3.69576029e-02, -1.27014471e-02,  3.19833457e-02, ...,\n              -1.48784053e-02,  9.22970548e-02,  6.54868260e-02],\n             [ 9.63706747e-02,  4.39107306e-02,  1.59802549e-02, ...,\n               1.22494521e-02,  8.10312852e-03,  1.78935930e-01],\n             [ 2.95156911e-02, -2.96487771e-02,  2.69996542e-02, ...,\n              -1.38547905e-02, -7.72434175e-02,  1.32773802e-01]]],\n    \n    \n           [[[ 4.22548056e-02, -8.30464344e-03,  5.34065207e-03, ...,\n              -8.06468353e-03, -4.70053628e-02,  4.45614867e-02],\n             [ 9.77012664e-02,  3.83502319e-02, -5.37837343e-03, ...,\n               1.17106764e-02, -4.59602941e-03,  6.98771998e-02],\n             [ 6.38262108e-02, -2.08319575e-02, -1.72756368e-03, ...,\n              -8.19445588e-03,  4.25621867e-02,  4.83920909e-02]],\n    \n            [[ 4.59470600e-02, -4.77699284e-03,  7.04339007e-03, ...,\n              -1.82104297e-02,  3.14848162e-02,  4.64068204e-02],\n             [ 3.89483608e-02,  3.78783308e-02, -6.85291924e-03, ...,\n               7.33014196e-03,  3.90656322e-04,  1.52848229e-01],\n             [ 4.57218140e-02, -1.34090437e-02, -8.30697361e-04, ...,\n              -1.85202472e-02, -3.45353335e-02,  9.25581828e-02]],\n    \n            [[-4.66161780e-02, -1.22223441e-02,  9.35023464e-03, ...,\n              -1.31351836e-02,  6.08736612e-02,  9.18865502e-02],\n             [-1.92336142e-01,  3.18407975e-02, -1.01881009e-03, ...,\n               7.55425170e-03, -8.62357323e-04,  2.88297594e-01],\n             [-1.15666650e-01, -2.35320851e-02,  6.74636895e-03, ...,\n              -1.94703583e-02, -5.66169359e-02,  1.95824102e-01]],\n    \n            ...,\n    \n            [[-2.10239179e-02, -9.81471874e-03,  9.81596112e-03, ...,\n              -1.36731779e-02,  1.20193027e-01, -1.26708716e-01],\n             [-3.72992679e-02,  3.05935629e-02, -3.00194928e-03, ...,\n               8.85152724e-03, -5.07611316e-03, -6.25461042e-02],\n             [ 7.84674310e-04, -2.91344281e-02,  1.12569630e-02, ...,\n              -1.38232643e-02, -9.49400812e-02, -8.74437019e-02]],\n    \n            [[ 3.32221799e-02, -4.22911346e-03,  1.13633750e-02, ...,\n              -1.41841583e-02,  9.59840789e-02, -1.23203963e-01],\n             [ 9.95653942e-02,  4.03233357e-02, -4.36036801e-03, ...,\n               8.42505507e-03, -1.50266392e-02, -1.58158958e-01],\n             [ 6.55353814e-02, -2.76978761e-02,  1.06595978e-02, ...,\n              -1.31017175e-02, -9.93799716e-02, -1.52014121e-01]],\n    \n            [[ 2.50522885e-02, -1.08845932e-02,  1.29567981e-02, ...,\n              -1.67823900e-02,  6.55406937e-02, -3.34061496e-02],\n             [ 1.00219429e-01,  4.24924381e-02, -4.06364352e-03, ...,\n               8.98410939e-03, -1.98677508e-03, -9.19047296e-02],\n             [ 6.97101504e-02, -3.41515057e-02,  8.97936709e-03, ...,\n              -1.51484888e-02, -8.06454644e-02, -8.53376985e-02]]],\n    \n    \n           [[[ 1.46303158e-02, -9.15218703e-03,  5.24803856e-03, ...,\n              -3.63799883e-03, -5.51798902e-02, -7.19531113e-03],\n             [ 6.12211153e-02,  2.67034862e-02, -4.38000960e-03, ...,\n               1.38858845e-02,  1.62421225e-03,  6.91889692e-03],\n             [ 1.86353922e-02, -2.39325576e-02,  5.56383107e-04, ...,\n              -6.68733614e-03,  7.36468807e-02,  3.71867418e-02]],\n    \n            [[ 3.52302976e-02, -3.27857491e-03,  7.14091491e-03, ...,\n              -9.93822515e-03,  2.38756705e-02, -2.10771449e-02],\n             [ 6.34438619e-02,  3.12160589e-02, -7.72275496e-03, ...,\n               1.49217555e-02,  3.86624038e-03, -1.16395289e-02],\n             [ 3.35849188e-02, -1.63664240e-02, -1.32562651e-03, ...,\n              -1.30512416e-02, -7.29435496e-03, -1.24825155e-02]],\n    \n            [[ 4.10873676e-03, -4.66612726e-03,  1.21031692e-02, ...,\n              -7.87103828e-03,  5.80726229e-02, -4.19587009e-02],\n             [-2.23153979e-02,  2.99241953e-02,  8.01213668e-04, ...,\n               1.82199273e-02,  9.57238674e-03, -8.57376456e-02],\n             [-2.01183017e-02, -1.96383689e-02,  7.32050464e-03, ...,\n              -1.07293837e-02, -2.17854325e-02, -7.95444921e-02]],\n    \n            ...,\n    \n            [[-1.71692297e-02, -3.16392444e-03,  2.40169745e-03, ...,\n              -9.67177004e-03,  9.26117748e-02, -1.16062798e-02],\n             [-8.63026828e-02,  3.55335064e-02, -1.06153013e-02, ...,\n               1.85809545e-02, -2.19932254e-02, -1.47949710e-01],\n             [-6.07556999e-02, -2.66596545e-02,  1.74473948e-03, ...,\n              -4.85855900e-03, -8.82942155e-02, -8.43590796e-02]],\n    \n            [[ 1.15142548e-02,  2.20947526e-03,  5.08834422e-03, ...,\n              -1.04352133e-02,  6.78158402e-02,  4.14623357e-02],\n             [ 7.41827395e-03,  4.52373996e-02, -1.10873608e-02, ...,\n               1.56368576e-02, -2.37460397e-02, -3.25448737e-02],\n             [ 7.84576032e-03, -2.45320965e-02,  5.84031455e-04, ...,\n              -8.31448287e-03, -8.92601907e-02, -3.36888898e-03]],\n    \n            [[ 4.79146978e-03, -4.22942545e-03,  1.15078716e-02, ...,\n              -2.12721284e-02,  4.96782959e-02,  2.05268860e-02],\n             [ 2.75192987e-02,  4.36737053e-02, -5.71439136e-03, ...,\n               9.46100149e-03, -8.58635467e-04, -1.79863740e-02],\n             [ 2.71184333e-02, -3.31169143e-02,  3.97488568e-03, ...,\n              -1.41424611e-02, -6.35233149e-02,  1.29984575e-03]]]],\n          dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'simple')\n",
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "best_model_file_simple = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0663259",
   "metadata": {},
   "source": [
    "# 예측 엔진"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 지정\n",
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap에서 최댓값을 찾는 함수\n",
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 이미지를 입력하면 이미지와 keypoint를 출력하는 함수\n",
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypoint 그리는 함수\n",
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "# skeleton 그리는 함수\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 이미지로 성능 확인\n",
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a44beb",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "- 오류 해결 : strategy를 모델 선언과 같은 곳에서 해야 한다.\n",
    "하지만 고쳐볼 시간은 없었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96e28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
