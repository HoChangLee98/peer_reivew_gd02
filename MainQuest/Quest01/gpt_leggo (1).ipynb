{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35be8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "795b0c2d",
   "metadata": {},
   "source": [
    "# GPT1 vs Transformer\n",
    "1. No encoder : encoder-decoder attention also deleted\n",
    "2. Positional Embedding : input token * token embedding matrix + position embedding matrix\n",
    "3. Loss : maximizing log likelihood of u_i over k previous tokens ![image.png](attachment:image.png)\n",
    "4. Input : question$answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d54d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e472ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46d1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "# 멀티헤드 어텐션 : n개의 head로 나누어 각 셀프 어텐션 처리\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query,batch_size)\n",
    "    key = self.split_heads(key,batch_size)\n",
    "    value = self.split_heads(value,batch_size)\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "837e5cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding mask : 어느 부분이 패딩인지 알려주는 행렬\n",
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0174d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look ahead mask : 디코더에서 예측할 부분 이후의 단어를 보지 못하도록 가림\n",
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af021033",
   "metadata": {},
   "source": [
    "# 인코더 레이어, 인코더 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30e29287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 인코더 하나의 레이어를 함수로 구현.\\n    # 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\\ndef encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\\n  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\\n\\n  # 패딩 마스크 사용\\n  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\\n\\n  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\\n  attention = MultiHeadAttention(\\n      d_model, num_heads, name=\"attention\")({\\n          \\'query\\': inputs,\\n          \\'key\\': inputs,\\n          \\'value\\': inputs,\\n          \\'mask\\': padding_mask\\n      })\\n\\n  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\\n  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\\n  attention = tf.keras.layers.LayerNormalization(\\n      epsilon=1e-6)(inputs + attention)\\n\\n  # 두 번째 서브 레이어 : 2개의 완전연결층\\n  outputs = tf.keras.layers.Dense(units=units, activation=\\'relu\\')(attention)\\n  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\\n\\n  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\\n  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\\n  outputs = tf.keras.layers.LayerNormalization(\\n      epsilon=1e-6)(attention + outputs)\\n\\n  return tf.keras.Model(\\n      inputs=[inputs, padding_mask], outputs=outputs, name=name)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# 인코더 하나의 레이어를 함수로 구현.\n",
    "    # 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acbd4565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def encoder(vocab_size,\\n            num_layers,\\n            units,\\n            d_model,\\n            num_heads,\\n            dropout,\\n            name=\"encoder\"):\\n  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\\n\\n  # 패딩 마스크 사용\\n  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\\n\\n  # 임베딩 레이어\\n  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\\n  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\\n\\n  # 포지셔널 인코딩\\n  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\\n\\n  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\\n\\n  # num_layers만큼 쌓아올린 인코더의 층.\\n  for i in range(num_layers):\\n    outputs = encoder_layer(\\n        units=units,\\n        d_model=d_model,\\n        num_heads=num_heads,\\n        dropout=dropout,\\n        name=\"encoder_layer_{}\".format(i),\\n    )([outputs, padding_mask])\\n\\n  return tf.keras.Model(\\n      inputs=[inputs, padding_mask], outputs=outputs, name=name)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b804d",
   "metadata": {},
   "source": [
    "# 디코더 레이어에서 인코더-디코더 어텐션 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3cd16ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재->두 개로 변경\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    # 인코더 아웃풋 변수 삭제\n",
    "    # enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")(inputs={\n",
    "              'query': inputs,\n",
    "              'key': inputs,\n",
    "              'value': inputs,\n",
    "              'mask': look_ahead_mask\n",
    "        })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과 LayerNormalization (+잔차 연결)\n",
    "    #attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + inputs)\n",
    "\n",
    "    \"\"\"\n",
    "      # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "      attention2 = MultiHeadAttention(\n",
    "          d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "              'query': attention1,\n",
    "              'key': enc_outputs,\n",
    "              'value': enc_outputs,\n",
    "              'mask': padding_mask\n",
    "          })\n",
    "      # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "      # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "      attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "      attention2 = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(attention2 + attention1)\n",
    "    \"\"\"\n",
    "\n",
    "    # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과 LayerNormalization (+잔차 연결)\n",
    "    #outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87e41a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    # 인코더 아웃풋 변수 삭제\n",
    "    # enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "          units=units,\n",
    "          d_model=d_model,\n",
    "          num_heads=num_heads,\n",
    "          dropout=dropout,\n",
    "          name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "          inputs=[inputs, look_ahead_mask, padding_mask],\n",
    "          outputs=outputs,\n",
    "          name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f246292",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# 데이터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edad9b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 path\n",
    "data_path = \"/aiffel/aiffel/transformer_chatbot/data/ChatbotData.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30f08495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 샘플의 최대 개수\n",
    "MAX_SAMPLES = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c046ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "  # 입력받은 sentence를 소문자로 변경하고 양쪽 공백을 제거\n",
    "  sentence = sentence.strip()\n",
    "\n",
    "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "  # student와 온점 사이에 거리를 만듭니다.\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a586d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n",
    "def load_conversations():\n",
    "    inputs,outputs = [],[]\n",
    "    with open(data_path, errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.split(',')\n",
    "        inputs.append(preprocess_sentence(parts[0]))\n",
    "        outputs.append(preprocess_sentence(parts[1])) # 전처리 함수를 질문에 해당되는 inputs와 답변에 해당되는 outputs에 적용\n",
    "    \n",
    "    if (len(inputs) >= MAX_SAMPLES) :\n",
    "        return inputs, outputs\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a285b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11824\n",
      "전체 샘플 수 : 11824\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "questions, answers = load_conversations()\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b875f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 0번째 질문 샘플: Q\n",
      "전처리 후의 0번째 답변 샘플: A\n",
      "전처리 후의 21번째 질문 샘플: 가스비 비싼데 감기 걸리겠어\n",
      "전처리 후의 21번째 답변 샘플: 따뜻하게 사세요 ! \n"
     ]
    }
   ],
   "source": [
    "print('전처리 후의 0번째 질문 샘플: {}'.format(questions[0]))\n",
    "print('전처리 후의 0번째 답변 샘플: {}'.format(answers[0]))\n",
    "print('전처리 후의 21번째 질문 샘플: {}'.format(questions[21]))\n",
    "print('전처리 후의 21번째 답변 샘플: {}'.format(answers[21]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ede94c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "# 0번째 열 짤라줌\n",
    "questions = questions[1:]\n",
    "answers = answers[1:]\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be82cf2",
   "metadata": {},
   "source": [
    "# Input 형태 변경\n",
    "- question과 answer를 구분자 $로 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c325afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d6f44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c29a7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2개 추가해준 단어장 크기\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47c61c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9250e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩, input과 output을 구분자 $ 로 합치기\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs = []\n",
    "    separator = tokenizer.encode('$')\n",
    "    \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "        # 최대 길이 10 이하인 경우에만 데이터셋으로 허용 & 구분자로 합치기\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1+separator+sentence2)\n",
    "\n",
    "    # 최대 길이 10으로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef1027bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8149\n",
      "필터링 후의 질문&답변 샘플 개수: 9132\n"
     ]
    }
   ],
   "source": [
    "tknzed_inputs= tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문&답변 샘플 개수: {}'.format(len(tknzed_inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df37a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# inputs, outputs 제거\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'dec_inputs': tknzed_inputs,\n",
    "    }\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da87b0",
   "metadata": {},
   "source": [
    "# Transformer -> GPT1\n",
    "- inputs 삭제\n",
    "- 인코더 관련 전부 삭제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a99c47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt1(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"gpt1\"):\n",
    "    # inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    \"\"\"# 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\"\"\"\n",
    "\n",
    "    # 디코더에서 미래의 토큰을 마스크\n",
    "    # 내부적으로 패딩 마스크도 포함\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "    # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(dec_inputs)\n",
    "\n",
    "    \"\"\"\n",
    "      # 인코더\n",
    "      enc_outputs = encoder(\n",
    "          vocab_size=vocab_size,\n",
    "          num_layers=num_layers,\n",
    "          units=units,\n",
    "          d_model=d_model,\n",
    "          num_heads=num_heads,\n",
    "          dropout=dropout,\n",
    "      )(inputs=[inputs, enc_padding_mask])\n",
    "    \"\"\"\n",
    "    # 디코더\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 완전연결층-\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=dec_inputs, outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8f6698ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gpt1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    8411392     dec_inputs[0][0]                 \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8149)   2094293     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,505,685\n",
      "Trainable params: 10,505,685\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 12 # 디코더의 층의 개수\n",
    "D_MODEL = 256 # 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = gpt1(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3def73",
   "metadata": {},
   "source": [
    "# 손실 함수 재정의\n",
    "- 이전 k개 토큰에 대한 i번째 토큰의 조건부 확률을 maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9dc169c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수\n",
    "def loss_function(logits, labels):\n",
    "    \"\"\"y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\"\"\"\n",
    "\n",
    "    # Shift labels to the right to align them with the prediction positions\n",
    "    shifted_labels = tf.roll(labels, shift=-1, axis=1)\n",
    "    \n",
    "    # Mask for padding or irrelevant tokens, typically using the token ID for padding (e.g., 0)\n",
    "    mask = tf.cast(tf.not_equal(labels, 0), dtype=tf.float32)\n",
    "\n",
    "    # Compute the cross-entropy loss, ignoring the padding tokens\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=shifted_labels)\n",
    "    loss = loss * mask  # Apply mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "245dd5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Learning rate Scheduling : 학습 초기에 lr을 급격히 높였다가 이후 천천히 수렴하게 함\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5aaf99ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습률 변화를 시각적으로 확인\n",
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b4e97",
   "metadata": {},
   "source": [
    "---------------\n",
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-supervised learning\n",
    "class gpt1(keras.Model):\n",
    "    def __init__(self, decoder, **kwargs):\n",
    "        self.decoder = decoder\n",
    "        self.loss_tracker = keras.metrics.sparse_categorical_accuracy(name='loss')\n",
    "        \n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return self.loss_tracker\n",
    "        \n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f77067eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "00df0a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:791 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:522 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:622 apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/utils.py:72 filter_empty_gradients\n        raise ValueError(\"No gradients provided for any variable: %s.\" %\n\n    ValueError: No gradients provided for any variable: ['embedding/embeddings:0', 'attention/dense/kernel:0', 'attention/dense/bias:0', 'attention/dense_1/kernel:0', 'attention/dense_1/bias:0', 'attention/dense_2/kernel:0', 'attention/dense_2/bias:0', 'attention/dense_3/kernel:0', 'attention/dense_3/bias:0', 'layer_normalization/gamma:0', 'layer_normalization/beta:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'layer_normalization_1/gamma:0', 'layer_normalization_1/beta:0', 'attention/dense_6/kernel:0', 'attention/dense_6/bias:0', 'attention/dense_7/kernel:0', 'attention/dense_7/bias:0', 'attention/dense_8/kernel:0', 'attention/dense_8/bias:0', 'attention/dense_9/kernel:0', 'attention/dense_9/bias:0', 'layer_normalization_2/gamma:0', 'layer_normalization_2/beta:0', 'dense_10/kernel:0', 'dense_10/bias:0', 'dense_11/kernel:0', 'dense_11/bias:0', 'layer_normalization_3/gamma:0', 'layer_normalization_3/beta:0', 'attention/dense_12/kernel:0', 'attention/dense_12/bias:0', 'attention/dense_13/kernel:0', 'attention/dense_13/bias:0', 'attention/dense_14/kernel:0', 'attention/dense_14/bias:0', 'attention/dense_15/kernel:0', 'attention/dense_15/bias:0', 'layer_normalization_4/gamma:0', 'layer_normalization_4/beta:0', 'dense_16/kernel:0', 'dense_16/bias:0', 'dense_17/kernel:0', 'dense_17/bias:0', 'layer_normalization_5/gamma:0', 'layer_normalization_5/beta:0', 'attention/dense_18/kernel:0', 'attention/dense_18/bias:0', 'attention/dense_19/kernel:0', 'attention/dense_19/bias:0', 'attention/dense_20/kernel:0', 'attention/dense_20/bias:0', 'attention/dense_21/kernel:0', 'attention/dense_21/bias:0', 'layer_normalization_6/gamma:0', 'layer_normalization_6/beta:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'layer_normalization_7/gamma:0', 'layer_normalization_7/beta:0', 'attention/dense_24/kernel:0', 'attention/dense_24/bias:0', 'attention/dense_25/kernel:0', 'attention/dense_25/bias:0', 'attention/dense_26/kernel:0', 'attention/dense_26/bias:0', 'attention/dense_27/kernel:0', 'attention/dense_27/bias:0', 'layer_normalization_8/gamma:0', 'layer_normalization_8/beta:0', 'dense_28/kernel:0', 'dense_28/bias:0', 'dense_29/kernel:0', 'dense_29/bias:0', 'layer_normalization_9/gamma:0', 'layer_normalization_9/beta:0', 'attention/dense_30/kernel:0', 'attention/dense_30/bias:0', 'attention/dense_31/kernel:0', 'attention/dense_31/bias:0', 'attention/dense_32/kernel:0', 'attention/dense_32/bias:0', 'attention/dense_33/kernel:0', 'attention/dense_33/bias:0', 'layer_normalization_10/gamma:0', 'layer_normalization_10/beta:0', 'dense_34/kernel:0', 'dense_34/bias:0', 'dense_35/kernel:0', 'dense_35/bias:0', 'layer_normalization_11/gamma:0', 'layer_normalization_11/beta:0', 'attention/dense_36/kernel:0', 'attention/dense_36/bias:0', 'attention/dense_37/kernel:0', 'attention/dense_37/bias:0', 'attention/dense_38/kernel:0', 'attention/dense_38/bias:0', 'attention/dense_39/kernel:0', 'attention/dense_39/bias:0', 'layer_normalization_12/gamma:0', 'layer_normalization_12/beta:0', 'dense_40/kernel:0', 'dense_40/bias:0', 'dense_41/kernel:0', 'dense_41/bias:0', 'layer_normalization_13/gamma:0', 'layer_normalization_13/beta:0', 'attention/dense_42/kernel:0', 'attention/dense_42/bias:0', 'attention/dense_43/kernel:0', 'attention/dense_43/bias:0', 'attention/dense_44/kernel:0', 'attention/dense_44/bias:0', 'attention/dense_45/kernel:0', 'attention/dense_45/bias:0', 'layer_normalization_14/gamma:0', 'layer_normalization_14/beta:0', 'dense_46/kernel:0', 'dense_46/bias:0', 'dense_47/kernel:0', 'dense_47/bias:0', 'layer_normalization_15/gamma:0', 'layer_normalization_15/beta:0', 'attention/dense_48/kernel:0', 'attention/dense_48/bias:0', 'attention/dense_49/kernel:0', 'attention/dense_49/bias:0', 'attention/dense_50/kernel:0', 'attention/dense_50/bias:0', 'attention/dense_51/kernel:0', 'attention/dense_51/bias:0', 'layer_normalization_16/gamma:0', 'layer_normalization_16/beta:0', 'dense_52/kernel:0', 'dense_52/bias:0', 'dense_53/kernel:0', 'dense_53/bias:0', 'layer_normalization_17/gamma:0', 'layer_normalization_17/beta:0', 'attention/dense_54/kernel:0', 'attention/dense_54/bias:0', 'attention/dense_55/kernel:0', 'attention/dense_55/bias:0', 'attention/dense_56/kernel:0', 'attention/dense_56/bias:0', 'attention/dense_57/kernel:0', 'attention/dense_57/bias:0', 'layer_normalization_18/gamma:0', 'layer_normalization_18/beta:0', 'dense_58/kernel:0', 'dense_58/bias:0', 'dense_59/kernel:0', 'dense_59/bias:0', 'layer_normalization_19/gamma:0', 'layer_normalization_19/beta:0', 'attention/dense_60/kernel:0', 'attention/dense_60/bias:0', 'attention/dense_61/kernel:0', 'attention/dense_61/bias:0', 'attention/dense_62/kernel:0', 'attention/dense_62/bias:0', 'attention/dense_63/kernel:0', 'attention/dense_63/bias:0', 'layer_normalization_20/gamma:0', 'layer_normalization_20/beta:0', 'dense_64/kernel:0', 'dense_64/bias:0', 'dense_65/kernel:0', 'dense_65/bias:0', 'layer_normalization_21/gamma:0', 'layer_normalization_21/beta:0', 'attention/dense_66/kernel:0', 'attention/dense_66/bias:0', 'attention/dense_67/kernel:0', 'attention/dense_67/bias:0', 'attention/dense_68/kernel:0', 'attention/dense_68/bias:0', 'attention/dense_69/kernel:0', 'attention/dense_69/bias:0', 'layer_normalization_22/gamma:0', 'layer_normalization_22/beta:0', 'dense_70/kernel:0', 'dense_70/bias:0', 'dense_71/kernel:0', 'dense_71/bias:0', 'layer_normalization_23/gamma:0', 'layer_normalization_23/beta:0', 'outputs/kernel:0', 'outputs/bias:0'].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/586136912.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 모델 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:791 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:522 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:622 apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    /opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/utils.py:72 filter_empty_gradients\n        raise ValueError(\"No gradients provided for any variable: %s.\" %\n\n    ValueError: No gradients provided for any variable: ['embedding/embeddings:0', 'attention/dense/kernel:0', 'attention/dense/bias:0', 'attention/dense_1/kernel:0', 'attention/dense_1/bias:0', 'attention/dense_2/kernel:0', 'attention/dense_2/bias:0', 'attention/dense_3/kernel:0', 'attention/dense_3/bias:0', 'layer_normalization/gamma:0', 'layer_normalization/beta:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'layer_normalization_1/gamma:0', 'layer_normalization_1/beta:0', 'attention/dense_6/kernel:0', 'attention/dense_6/bias:0', 'attention/dense_7/kernel:0', 'attention/dense_7/bias:0', 'attention/dense_8/kernel:0', 'attention/dense_8/bias:0', 'attention/dense_9/kernel:0', 'attention/dense_9/bias:0', 'layer_normalization_2/gamma:0', 'layer_normalization_2/beta:0', 'dense_10/kernel:0', 'dense_10/bias:0', 'dense_11/kernel:0', 'dense_11/bias:0', 'layer_normalization_3/gamma:0', 'layer_normalization_3/beta:0', 'attention/dense_12/kernel:0', 'attention/dense_12/bias:0', 'attention/dense_13/kernel:0', 'attention/dense_13/bias:0', 'attention/dense_14/kernel:0', 'attention/dense_14/bias:0', 'attention/dense_15/kernel:0', 'attention/dense_15/bias:0', 'layer_normalization_4/gamma:0', 'layer_normalization_4/beta:0', 'dense_16/kernel:0', 'dense_16/bias:0', 'dense_17/kernel:0', 'dense_17/bias:0', 'layer_normalization_5/gamma:0', 'layer_normalization_5/beta:0', 'attention/dense_18/kernel:0', 'attention/dense_18/bias:0', 'attention/dense_19/kernel:0', 'attention/dense_19/bias:0', 'attention/dense_20/kernel:0', 'attention/dense_20/bias:0', 'attention/dense_21/kernel:0', 'attention/dense_21/bias:0', 'layer_normalization_6/gamma:0', 'layer_normalization_6/beta:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'layer_normalization_7/gamma:0', 'layer_normalization_7/beta:0', 'attention/dense_24/kernel:0', 'attention/dense_24/bias:0', 'attention/dense_25/kernel:0', 'attention/dense_25/bias:0', 'attention/dense_26/kernel:0', 'attention/dense_26/bias:0', 'attention/dense_27/kernel:0', 'attention/dense_27/bias:0', 'layer_normalization_8/gamma:0', 'layer_normalization_8/beta:0', 'dense_28/kernel:0', 'dense_28/bias:0', 'dense_29/kernel:0', 'dense_29/bias:0', 'layer_normalization_9/gamma:0', 'layer_normalization_9/beta:0', 'attention/dense_30/kernel:0', 'attention/dense_30/bias:0', 'attention/dense_31/kernel:0', 'attention/dense_31/bias:0', 'attention/dense_32/kernel:0', 'attention/dense_32/bias:0', 'attention/dense_33/kernel:0', 'attention/dense_33/bias:0', 'layer_normalization_10/gamma:0', 'layer_normalization_10/beta:0', 'dense_34/kernel:0', 'dense_34/bias:0', 'dense_35/kernel:0', 'dense_35/bias:0', 'layer_normalization_11/gamma:0', 'layer_normalization_11/beta:0', 'attention/dense_36/kernel:0', 'attention/dense_36/bias:0', 'attention/dense_37/kernel:0', 'attention/dense_37/bias:0', 'attention/dense_38/kernel:0', 'attention/dense_38/bias:0', 'attention/dense_39/kernel:0', 'attention/dense_39/bias:0', 'layer_normalization_12/gamma:0', 'layer_normalization_12/beta:0', 'dense_40/kernel:0', 'dense_40/bias:0', 'dense_41/kernel:0', 'dense_41/bias:0', 'layer_normalization_13/gamma:0', 'layer_normalization_13/beta:0', 'attention/dense_42/kernel:0', 'attention/dense_42/bias:0', 'attention/dense_43/kernel:0', 'attention/dense_43/bias:0', 'attention/dense_44/kernel:0', 'attention/dense_44/bias:0', 'attention/dense_45/kernel:0', 'attention/dense_45/bias:0', 'layer_normalization_14/gamma:0', 'layer_normalization_14/beta:0', 'dense_46/kernel:0', 'dense_46/bias:0', 'dense_47/kernel:0', 'dense_47/bias:0', 'layer_normalization_15/gamma:0', 'layer_normalization_15/beta:0', 'attention/dense_48/kernel:0', 'attention/dense_48/bias:0', 'attention/dense_49/kernel:0', 'attention/dense_49/bias:0', 'attention/dense_50/kernel:0', 'attention/dense_50/bias:0', 'attention/dense_51/kernel:0', 'attention/dense_51/bias:0', 'layer_normalization_16/gamma:0', 'layer_normalization_16/beta:0', 'dense_52/kernel:0', 'dense_52/bias:0', 'dense_53/kernel:0', 'dense_53/bias:0', 'layer_normalization_17/gamma:0', 'layer_normalization_17/beta:0', 'attention/dense_54/kernel:0', 'attention/dense_54/bias:0', 'attention/dense_55/kernel:0', 'attention/dense_55/bias:0', 'attention/dense_56/kernel:0', 'attention/dense_56/bias:0', 'attention/dense_57/kernel:0', 'attention/dense_57/bias:0', 'layer_normalization_18/gamma:0', 'layer_normalization_18/beta:0', 'dense_58/kernel:0', 'dense_58/bias:0', 'dense_59/kernel:0', 'dense_59/bias:0', 'layer_normalization_19/gamma:0', 'layer_normalization_19/beta:0', 'attention/dense_60/kernel:0', 'attention/dense_60/bias:0', 'attention/dense_61/kernel:0', 'attention/dense_61/bias:0', 'attention/dense_62/kernel:0', 'attention/dense_62/bias:0', 'attention/dense_63/kernel:0', 'attention/dense_63/bias:0', 'layer_normalization_20/gamma:0', 'layer_normalization_20/beta:0', 'dense_64/kernel:0', 'dense_64/bias:0', 'dense_65/kernel:0', 'dense_65/bias:0', 'layer_normalization_21/gamma:0', 'layer_normalization_21/beta:0', 'attention/dense_66/kernel:0', 'attention/dense_66/bias:0', 'attention/dense_67/kernel:0', 'attention/dense_67/bias:0', 'attention/dense_68/kernel:0', 'attention/dense_68/bias:0', 'attention/dense_69/kernel:0', 'attention/dense_69/bias:0', 'layer_normalization_22/gamma:0', 'layer_normalization_22/beta:0', 'dense_70/kernel:0', 'dense_70/bias:0', 'dense_71/kernel:0', 'dense_71/bias:0', 'layer_normalization_23/gamma:0', 'layer_normalization_23/beta:0', 'outputs/kernel:0', 'outputs/bias:0'].\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "EPOCHS = 30\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
